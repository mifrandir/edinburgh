\documentclass{article}
\usepackage{homework-preamble}
\usepackage{enumitem}
\mkthms

\title{Introduction to Theoretical Computer Science: Coursework 2}
\author{}
\begin{document}
\maketitle
\noindent The encoding of the register machine program $P$ as described in the lectures shall be denoted $\enc P$.
We shall also say that an RM $M$ halts with value $y$ on input $x$ if, given $x$ in $R_0$, $M$ halts
with $y$ in $R_0$.
\section{Partial functions}
\begin{claim*}[a, 1]
	Let $\hat H:\N\times\N\rightharpoonup\N$ be the partial function given by
	\begin{align*}
		\hat H(m,n)=\begin{cases}
			0    & \text{if $m$ is not the code of any RM program $P$;}           \\
			1    & \text{if $m=\enc P$ for some $P$, and $P$ halts on input $n$;} \\
			\bot & \text{otherwise}.
		\end{cases}
	\end{align*}
	Then $\hat H$ is computable.
	\begin{proof}
		As mentioned in (b), it is easily decidable whether a number $n$ is the code $\enc P$ of a
		machine. Further, as used in the proof of \emph{Theorem I.26} of the notes, it is possible
		to use the encoding $\enc M$ of an RM $M$ to simulate a single step of the computation and decide whether
		$M$ has halted. Combining these, we construct an RM using the following algorithm to compute
		$\hat H(m,n)$:
		\begin{enumerate}
			\item Decide wether $m$ is an encoding of some $P$. If not, return $0$.
			\item Encode $\enc M=\lra{m, \lra{n, \lra{}}, 0}$ and store the result in $R_0$.
			\item While the encoding in $R_0$ is not in a halting state, simulate another step and store the result in $R_0$.
			\item Return $1$.
		\end{enumerate}
		Observe that, if $P$ halts on input $n$, the algorithm above will indeed return $1$. Trivially,
		if there is no $P$ with encoding $m$, then the algorithm returns $0$.
		Appealing to the \emph{Church-Turing thesis} and the computabilitiy of the functions involved,
		we conclude that there exists a machine that computes the partial function $\hat H$.
	\end{proof}
\end{claim*}

\begin{claim*}[a, 2]
	Let $f:\N\rightharpoonup\N$ be a computable partial function. Then it is undecidable whether $f$
	is total.
	\begin{proof}
		Let $\enc M$ be a register machine state encoding.  Note that the partial function $H_M:\N\to\N$
		given by $\hat H_M(n)=H(\enc M, n)$ is computable.

		Assume that there exists an RM $I$ that decides whether, given a suitable encoding, a partial
		function $f:\N\to\N$ is total. Then we may construct an RM $H$ which takes as input an encoding
		of a register machine state $\enc M$, encodes the function $\hat H_M$, and runs $I$ on the result.
		Clearly, $H$ halts in all cases and returns `1' iff $M$ halts on all inputs. In other words,
		$UH$ decides the uniform halting problem; contradiction.
	\end{proof}
\end{claim*}

\begin{claim*}[b]
	The partial function $d:\N\rightharpoonup\N$ given by
	\begin{align*}
		d(n)=\begin{cases}
			P_n(n)+1 & \text{if $P_n$ returns a result on input $n$;} \\
			\bot     & \text{otherwise,}
		\end{cases}
	\end{align*}
	where $(P_k)_{k\in\N}$ is the sequence of all register machines, is
	computable.
	\begin{proof}
		We construct the following machine $M$:
		\begin{enumerate}
			\item Given input $n$, compute $\hat H(P_n,n)$.
			\item Simulate $P_n$ until termination and store the result in $R_0$.
			\item Increment $R_0$ and halt.
		\end{enumerate}
		Clearly, all steps of this computation are possible. In particular, $M$ doesn't halt
		if $\hat H(P_n,n)=\bot$ which implies $d(n)=\bot$. This is consistent. Thus $M$
		computes the partial function $d$.

		\emph{It is worth mentioning that an implicit consequence of the above is $d(k)=\bot$
			whenever $P_k$ computes $d$.}
	\end{proof}
\end{claim*}

\begin{claim*}[c]
	Let $f:\N\to\N$ be a total function such that $f(n)=d(n)$ for all $n\in\N$ where
	$d(n)\not=\bot$. Then $f$ is not computable.
	\begin{proof}
		Assume there exists an RM $M$ that computes $f$. Then there exists $k\in\N$ such that
		$P_k$ computes $f$. Since $f$ is total, $P_k$ returns $f(k)$ on input $k$. However,
		then $d(k)=f(k)+1$. This contradicts the premise that $f(k)=d(k)$ whenever $P_k$ terminates
		on input $k$.
	\end{proof}
\end{claim*}

\section{P, NP}

\begin{claim*}[a]
	\begin{align}
		\label{on2}
		n^2    & \not=O(n\lg n) \\
		\label{onlgn}
		n\lg n & =O(n^2)        \\
		\label{o3n}
		3^n    & \not=O(2^n)
	\end{align}
	The statement $3^n=2^{O(n)}$ is nonsensical and may therefore not be considered true
	unless further definitions are given.
\end{claim*}

The following result shall serve as a more rigorous justificaiton for some of the less precise
arguments further below.

\begin{lemma*}
	\label{pbits}
	Let $R_i$ be a register containing a value $n$. Then all of the following operations
	have a runtime that is $O(p(d))$ where $p(d)$ is a polynomial in $d=\lg n$:
	\begin{enumerate}
		\item Multiplication by $2$, i.e. bit shifting left.
		\item Extracting the $k$th bit where $0\leq k\leq d$.
		\item Division by $2$, i.e. bit shifting right.
	\end{enumerate}
	\begin{proof}
		Bit shifting the value $n$ in register $R_i$ can be done using the $\texttt{ADD}(i,i)$
		instruction in $O(d)$ time.
		To extract the $k$th bit from $R_i$, do the following:
		\begin{enumerate}
			\item Use temporary registers to calculate the greatest $j$ such that $2^j\leq n$
			      where $n$ is the value in $R_i$.
			      For example, starting with $2$, keep doubling a value in $R_t$ until $R_i-R_t$
			      is zero. Count the number of steps it takes and infer $j$.
			\item If $j>k$, substract $2^j$ from the value in $R_i$ and repeat.
			\item If $j=k$ then the $k$th bit is not set.
			\item If $j<k$ then the $k$th bit is set.
		\end{enumerate}
		Clearly, the worst case is $k=0$. This requires $d$ steps, all of which only involve
		$O(d)$ operations which take $O(d)$ time. Therefore bit extraction overall can be done
		in polynomial time.
	\end{proof}
\end{lemma*}

\begin{claim*}[b]
	Let $(D,X)$ and $(D,Y)$ be decision problems in \ptime. Then the decision problems
	\begin{align*}
		(D,X\cup Y), (D,X\cap Y), (D,D\setminus X)
	\end{align*}
	are in \ptime.
	\begin{proof}
		Let $M_X,M_Y$ be the RMs that decide $(D,X),(D,Y)$ in time $T_X(n)=O(n^c),T_Y(n)=O(n^d)$ for some $c,d$,
		respectively. Then we may construct $U$ that decides $(D,X\cup Y)$ as follows:
		\begin{enumerate}[label=U\arabic*]
			\item Run $M_X$ on input $d$ and store the output in $R_1$.
			\item Run $M_Y$ on input $d$ and store the output in $R_2$.
			\item If $R_1$ or $R_2$ contains a non-zero value then return $1$, else return $0$.
		\end{enumerate}
		Note that the runtime is $T_U(n)=T_X(n)+T_Y(n)+\Theta(1)$ since there are only four possible scenarios
		for U3. Thus we obtain the very generous bound $T_U(n)=O(n^{c+d})$, showing that $X\cup Y$ is in \ptime.

		Similarly, we construct the RM $I$ that decides $X\cap Y$:
		\begin{enumerate}[label=I\arabic*]
			\item Run $M_X$ on input $d$ and store the output in $R_1$.
			\item Run $M_Y$ on input $d$ and store the output in $R_2$.
			\item If $R_1$ or $R_2$ contain zero then return $0$, else return $1$.
		\end{enumerate}
		Completely analogously to $U$, we obtain the runtime $T_I(n)=O(n^{c+d})$, again proving that
		$X\cap Y$ is in \ptime.

		Even simpler is the construction of $N$ that decides $(D, D\setminus X)$, i.e. `$\neg X$':
		\begin{enumerate}[label=N\arabic*]
			\item Run $M_X$ on input $d$ and store the output in $R_0$.
			\item If $R_0$ is $0$ then return $1$, otherwise return $0$.
		\end{enumerate}
		The runtime we obtain is $T_N(n)=O(n^c)$. Thus $\neg X$ is in \ptime.
	\end{proof}
\end{claim*}

\begin{claim*}[c]
	Let $L_1,L_2$ be languages of strings over some finite alphabet $\Sigma$ whose decision problems are in
	\nptime. Then the decision problem of $L=L_1L_2$ is in \nptime.
	\begin{proof}
		Let $M_1,M_2$ be the NRMs bounded by the polynomials $f_1(n),f_2(n)$ that decide $L_1,L_2$, respectively.
		We construct an NRM $M$ that decides $L$ as follows:
		\begin{enumerate}[label=M\arabic*]
			\item \label{split} Non-deterministically split the input into two parts. This could be done in the following way:
			      \begin{enumerate}
				      \item Write the empty sequence into $R_1$.
				      \item \label{emptycheck} If $R_0$ is the empty sequence, go to \ref{deterministic}.
				      \item Maybe go to \ref{deterministic}.
				      \item Take one character from the end of the sequence in $R_0$ and append it to $R_1$.
				      \item Go to \ref{emptycheck}.
			      \end{enumerate}
			\item \label{deterministic} Reverse $R_1$.
			\item \label{runm1} Run $M_1$ on $R_0$.
			\item \label{runm2} Run $M_2$ on $R_1$.
			\item \label{trivial} If both $R_0$ and $R_1$ returned `1', accept the string.
		\end{enumerate}
		Firstly, we shall show that $M$ behaves as intended. If there exist words $x_1\in L_1$ and $x_2\in L_2$ such that the input
		is the concatenation $w=x_1x_2$ then one of the runs will result in the correct
		split and $M_1$ and $M_2$ will accept each string individually.  Conversely,
		if $M$ accepts a word $w$ it means that there exists a split such that
		$w=x_1x_2$ and $x_1\in L_1$ and $x_2\in L_2$, i.e. $w\in L_1L_2$. Thus $M$
		decides $L$.

		Secondly, we show that $M$ is polynomially bounded. Consider \ref{split}. Since $\Sigma$ is finite, we may
		represent a single character using
		$d$ bits. Thus a word $w=w_1\cdots w_k \in\Sigma^*$ may be encoded using $n=kd$ bits in total.
		In the worst case, $M$ may move
		every character out of $R_0$ into $R_1$ individually, which takes $k$ steps. Each one of these steps
		may be solved by bit shifting right $d$ times, bit shifting right $2d$ times ($d$ times on $R_1$ for appending
		and $d$ times on a temporary register for the modulo operation) and then a constant number of additions and multiplications.
		Since all $k$ steps are polynomial in $n$, so is \ref{split} in general.

		The runtime of \ref{deterministic} is similar to the worst case of \ref{split}. One can reverse a list
		by removing them and appending them to a temporary register. This has been shown above to be possible in
		polynomial time.

		Finally, it is clear that \ref{runm1} and \ref{runm2} are bounded by $f_1(n)$, $f_2(n)$ up to some
		constant overhead. Similarly, \ref{trivial} only takes a constant number of instructions, thus it is polynomially bounded, too.
		We conclude that $M$ as a whole is polynomially bounded. Since $M$ decides $L$, the decision problem
		$(\Sigma^*, L)$ is in \nptime.
	\end{proof}
\end{claim*}

\section{Reductions for NP-completeness}

\begin{claim*}[a]
	\text{EXACT-3SAT} is NP-complete.
	\begin{proof}
		As a special case of 3SAT, which itself is NP-complete, EXACT-3SAT clearly lies in \nptime.

		We shall reduce 3SAT to EXACT-3SAT to show NP-completeness. Consider a 3-CNF
		\begin{align*}
			\phi = C_1\wedge \cdots\wedge C_n.
		\end{align*}
		In particular, each clause $C_i$ is a disjunction of at most three literals. We note that
		EXACT-3SAT requires each $C_i$ to have exactly three literals. However, it is also worth
		mentioning that those literals need not be distinct. Therefore we may define a map
		\begin{align*}
			f:C_i \mapsto \begin{cases}
				l_1\vee l_1\vee l_1 & C_i = l_1 \text{ for some literal $l_1$}                              \\
				l_1\vee l_2\vee l_1 & C_i = l_1 \vee l_2 \text{ for some literal $l_1, l_2$}                \\
				l_1\vee l_2\vee l_3 & C_i = l_1 \vee l_2 \vee l_3 \text{ for some literals $l_1, l_2, l_3$}
			\end{cases}
		\end{align*}
		We observe that if $\phi$ has $n$ clauses and $k$ unique propositional variables, then we may represent
		it using at most
		\begin{align*}
			d=3n(2+\lg k)=O(n\lg k)
		\end{align*}
		bits where we added one bit per literal for the negation and another
		in order to avoid rounding and to make sure that at least one value exists to represent the absence of
		a literal, i.e.\ zero.

		Now we may note that, given a clause, $f$ may be computed as follows:
		\begin{enumerate}[label=F\arabic*]
			\item Extract $l_1$ into a temporary register $R_{j}$.
			\item Extract $l_2$ into a temporary register $R_{j+1}$
			\item If $R_{j+1}$ is zero (i.e.\ an invalid literal), copy $R_j$ into $R_{j+1}$
			\item Extract $l_3$ into a temporary register $R_{j+2}$
			\item If $R_{j+2}$ is zero (i.e.\ an invalid literal), copy $R_j$ into $R_{j+2}$
			\item Combine $R_j,R_{j+1},R_{j+2}$ into a clause.
		\end{enumerate}
		While certainly not the most efficient approach, this is clearly still polynomial in $d$.
		Finally, we are required to combine all the new clauses into one 3-CNF.\@ Doing this involves
		shifting left $d/n$ times and a single addition. The runtime of this is polynomial in $d$.
		We conclude that the whole procedure only takes polynomial time in general.

		The result is a CNF with exactly three literals per clause, i.e.\ an instance of EXACT-3SAT.\@
		Thus $\text{3SAT}\leq_p\text{EXACT-3SAT}$ and therefore EXACT-3SAT is NP-complete.
	\end{proof}
\end{claim*}

\begin{claim*}[b]
	$\text{EXACT-3SAT}\leq_p\text{3PRODEQNS}$.
	\begin{proof}
		Consider an exact 3-CNF with $k$ propositional variables,
		\begin{align*}
			\phi = C_1 \wedge \cdots \wedge C_n.
		\end{align*}
		The key observation is that an assignment $h$ satisfies a clause
		\begin{align}
			\label{ciexact}
			C_i = l_1 \vee l_2 \vee l_3
		\end{align}
		if and only if
		\begin{align}
			\label{prod}
			(a_1-x_1)(a_2-x_2)(a_3-x_3) = 0
		\end{align}
		where the $x_i$ are the truth values of the propositional variables in each $l_i$ assigned
		by $h$ converted to an integer in $\{0,1\}$ in the obvious way, and, for all $i$, $a_i=0$
		iff $l_i$ is a negation of some propositional variable.

		We may now convert $\phi$ to an equivalent instance of 3PRODEQNS by converting each clause
		(\ref{ciexact}) to a product (\ref{prod}). The resulting set of 3-products will then clearly
		be satisfiable iff $\phi$ is.

		Similar to previous considerations, we may represent $\phi$ using $O(n\lg k)$ bits.
		Thus the maximum number of bits required to represent the value in any register is $O(n\lg k + k)$.

		We see that the resulting instance of 3PRODEQNS may be encoded using $\lg k+1$ bits to represent
		an upper bound for the number of variables and $3n(\lg k + 2)=O(n\lg k)$ bits to represent the $n$
		clauses each containing three terms.

		Since the transformation from one representation to the other can be done by mapping literals in the CNF
		directly to factors in the 3-products, it is clear that each such step can be performed in polynomial time.
		The detailed analysis is omitted as it would not provide any additional insights.
		Since we require at most $3n$ such steps in total, the whole procedure may be completed
		in polynomial time. The claim follows.
	\end{proof}
\end{claim*}

\begin{claim*}
	INDSET is NP-complete.
	\begin{proof}
		Let $G=(V,E)$ be a graph. We shall denote by $G^c$ the graph that has an edge $(u,v)$ iff $(u,v)\not\in E$.
		I.e. $G^c=(V, V\times V\setminus E)$. Consider a set of vertices $X\subseteq V$. Then $X$ is an
		independent set in $G$ iff, for all distinct $u,v\in X$, $(u,v)\not\in E$. By construction, this is equivalent to
		the condition that, for all distinct $u,v\in X$, $(u,v)\in E$.

		In other words, $X$ is an independent set in $G$ iff it is a clique in $G^c$. Therefore solving
		INDSET on $(G, k)$ is equivalent to solving CLIQUE on $(G^c, k)$.

		We observe $(G^c)^c=G$. Reducing CLIQUE to INDSET and vice versa is therefore the same as converting
		a graph to its complement. This can be done in polynomial time. To see this, one may assume that
		$E$ is represented as a flattened adjacency matrix with $\abs V^2$ entries. In this case the
		conversion is the same as flipping each bit in the representation of $E$ and copying the rest.
		Na\"ively, this may be done by extracting each bit, fliping it, and inserting it into the new
		matrix. This completes the task in $\abs V^2$ polynomial time steps, and therefore in polynomial time
		overall.

		We have shown
		\begin{align}
			\text{INDSET} \leq_p \text{CLIQUE}
		\end{align}
		and
		\begin{align}
			\text{INDSET} \geq_p \text{CLIQUE}.
		\end{align}
		As CLIQUE is NP-complete, so is INDSET.
	\end{proof}
\end{claim*}

\section{Untyped and simply typed lambda-calculus}

\newcommand{\betareduce}{\xrightarrow{\beta}}
\newcommand{\alphareduce}{\xrightarrow{\alpha}}

\begin{claim*}[a]
	The term
	\begin{align}
		\label{eq:t}
		(\lambda m. \lambda n.\lambda f.\lambda x.mf(nfx))(\lambda f.\lambda x.fx)(\lambda f.\lambda x f(fx))
	\end{align}
	reduces to
	\begin{align*}
		\lambda f.\lambda x f(f(fx)).
	\end{align*}
	\begin{proof}
		Firstly, we $\alpha$-convert (\ref{eq:t}) to
		\begin{align*}
			T:=(\lambda m. \lambda n.\lambda f.\lambda x.mf(nfx))(\lambda g.\lambda y.gy)(\lambda h.\lambda z.h(hz))
		\end{align*}
		in order to avoid naming conflicts. We now perform a sequence of $\beta$-reductions,
		substituting $m,n,g,y,h,z$ in that order.
		\begin{align*}
			T
			 & \betareduce(\lambda n.\lambda f.\lambda x.(\lambda g.\lambda y.gy)f(nfx))(\lambda h.\lambda z.h(hz)) \\
			 & \betareduce\lambda f.\lambda x.(\lambda g.\lambda y.gy)f((\lambda h.\lambda z.h(hz))fx)              \\
			 & \betareduce\lambda f.\lambda x.(\lambda y.fy)((\lambda h.\lambda z.h(hz))fx)                         \\
			 & \betareduce\lambda f.\lambda x.f((\lambda h.\lambda z.h(hz))fx)                                      \\
			 & \betareduce\lambda f\lambda x.f((\lambda f.\lambda z.f(fz))x)                                        \\
			 & \betareduce\lambda f.\lambda x.f(f(fx)).
		\end{align*}
	\end{proof}
\end{claim*}

\begin{claim*}[b]
	Let
	\begin{align}
		\label{ycombinator}
		Y:=\lambda f.(\lambda x. f(xx))(\lambda x. f(xx))
	\end{align}
	and
	\begin{align}
		\label{newycombinator}
		Y':=\lambda f.(\lambda x. xx)(\lambda x. f(xx)).
	\end{align}
	Then $Y'G=G(YG)$ and $Y'\betareduce Y$.
	\begin{proof}
		We simplify
		\begin{align*}
			Y'G & = (\lambda f.(\lambda x. xx)(\lambda x. f(xx)))G     \\
			    & \betareduce (\lambda x. xx)(\lambda x. G(xx))        \\
			    & \betareduce (\lambda x. G(xx))(\lambda x. G(xx))     \\
			    & \betareduce G((\lambda x. G(xx))(\lambda x. G(xx))).
		\end{align*}
		We note that
		\begin{align*}
			G(YG) & = G((\lambda f.(\lambda x. f(xx))(\lambda x. f(xx)))G) \\
			      & \betareduce G((\lambda x. G(xx))(\lambda x. G(xx))).
		\end{align*}
		Therefore $Y'G=G(YG)$. Further, we may reduce $Y$ to $Y'$. To see this,
		we first $\alpha$-convert $Y'$ to
		\begin{align*}
			Y'':=\lambda f.(\lambda y. yy)(\lambda x. f(xx)).
		\end{align*}
		Then, by substituting $y$,
		\begin{align*}
			Y'' & \betareduce \lambda f. ((\lambda x.f(xx))(\lambda x.f(xx))) = Y
		\end{align*}
		Since $Y'\alphareduce Y''$, we find $Y'\betareduce Y$.
	\end{proof}
\end{claim*}
\newcommand{\nat}{\texttt{nat}}
\begin{claim*}[c]
	\begin{align*}
		(\lambda f: \nat\to\nat.\lambda x:\nat.f(fx)):(\nat\to\nat)\to\nat\to\nat
	\end{align*}
	\begin{proof}
		Firstly, we have
		\begin{align}
			\label{pf1}
			\infer{
				f:\nat\to\nat,x:\nat \vdash fx:\nat
			}{
				\infer{
					f:\nat\to\nat,x:\nat \vdash f:\nat\to\nat
				}{}
				\hs
				\infer{
					f:\nat\to\nat,x:\nat \vdash x:\nat
				}{}
			}
		\end{align}
		Using this, we find
		\begin{align*}
			\infer{
				\vdash(\lambda f: \nat\to\nat.\lambda x:\nat.f(fx)):(\nat\to\nat)\to\nat\to\nat
			}{
				\infer{
					f:\nat\to\nat \vdash (\lambda x : \nat. f(fx)):\nat\to\nat
				}{
					\infer{
						f:\nat\to\nat,x:\nat \vdash f(fx):\nat
					}{
						\infer{f:\nat\to\nat,x:\nat \vdash f:\nat\to\nat}{ }
						\hs
						\infer{
							f:\nat\to\nat,x:\nat \vdash fx:\nat
						}{(\ref{pf1})}
					}
				}
			}
		\end{align*}
	\end{proof}
\end{claim*}

\begin{claim*}[d]
	\begin{align*}
		(\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx))(\lambda f:\tau\to\tau.\lambda x:\tau.fx)(\lambda f:\tau\to\tau.\lambda x:\tau. f(fx)):T
	\end{align*}
	where
	\begin{align*}
		T=(\tau\to\tau)\to\tau\to\tau.
	\end{align*}
	\begin{proof}
		For readability, we adapt the application rule to be
		\begin{align*}
			\infer{\Sigma,\Gamma\vdash ts:\tau}{\Sigma\vdash t:\sigma\to\tau\hs\Gamma\vdash s:\sigma}.
		\end{align*}
		Clearly this is possible as we prove each branch with potentially fewer assumptions.
		We begin with an auxillary proof:
		\begin{align}
			\label{eq:h2}
			\infer{
				n:T,f:\tau\to\tau,x:\tau\vdash nfx:\tau
			}{
				\infer{
					n:T,f:\tau\to\tau\vdash nf:\tau\to\tau
				}{
					\infer{
						n:T\vdash n:(\tau\to\tau)\to\tau\to\tau
					}{}
					\hs
					\infer{
						f:\tau\to\tau\vdash f:\tau\to\tau
					}{}
				}
				\hs
				\infer{
					x:\tau\vdash x:\tau
				}{}
			}
		\end{align}
		Now we can prove the type of the first parenthesised term:
		\begin{align}
			\label{eq:abs1}
			\infer{
				\vdash\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx):T\to T\to T
			}{
				\infer{
					m:T\vdash\lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx):T\to T
				}{
					\infer{
						m:T,n:T\vdash\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx):T=(\tau\to\tau)\to\tau\to\tau
					}{
						\infer{
							m:T,n:T,f:\tau\to\tau\vdash\lambda x:\tau.mf(nfx):\tau\to\tau
						}{
							\infer{
								m:T,n:T,f:\tau\to\tau,x:\tau\vdash mf(nfx):\tau
							}{
								\infer{
									m:T,f:\tau\to\tau\vdash mf:\tau\to\tau
								}{
									\infer{
										m:T\vdash m:T
									}{}
									\hs
									\infer{
										f:\tau\to\tau\vdash f:\tau\to\tau
									}{}
								}
								\hs
								\infer{
									n:T,f:\tau\to\tau,x:\tau\vdash nfx:\tau
								}{
									(\ref{eq:h2})
								}
							}
						}
					}
				}
			}
		\end{align}
		Then the second one:
		\begin{align}
			\label{eq:abs2}
			\infer{
				\vdash\lambda f:\tau\to \tau.\lambda x:\tau. fx:T=(\tau\to\tau)\to\tau\to\tau
			}{
				\infer{f:\tau\to \tau\vdash\lambda x:\tau. fx:\tau\to\tau}{
					\infer{
						f:\tau\to \tau,x:\tau\vdash fx:\tau
					}{
						\infer{f:\tau\to \tau\vdash f:\tau\to \tau}{}
						\hs
						\infer{x:\tau\vdash x:\tau}{}
					}
				}
			}
		\end{align}
		And now the third:
		\begin{align}
			\label{eq:abs3}
			\infer{
				\vdash\lambda f:\tau\to\tau.\lambda x:\tau. f(fx):T=(\tau\to\tau)\to\tau\to\tau
			}{
				\infer{
					f:\tau\to\tau\vdash \lambda x:\tau. f(fx):\tau\to\tau
				}{
					\infer{
						f:\tau\to\tau,x:\tau \vdash f(fx):\tau
					}{
						\infer{f:\tau\to\tau\vdash f:\tau\to\tau}{}
						\hs
						\infer{
							f:\tau\to\tau,x:\tau\vdash fx:\tau
						}{
							\infer{f:\tau\to \tau\vdash f:\tau\to \tau}{}
							\hs
							\infer{x:\tau\vdash x:\tau}{}
						}
					}
				}
			}
		\end{align}
		Putting together the first two:
		\begin{align}
			\label{eq:join}
			\infer{
				\vdash(\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx))(\lambda f:\tau\to \tau.\lambda x:T.fx):T\to T
			}{
				\infer{
					\vdash\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx):T\to T\to T
				}{
					(\ref{eq:abs1})
				}
				\hs
				\infer{
					\vdash\lambda f:\tau\to \tau.\lambda x:T. fx:T
				}{
					(\ref{eq:abs2})
				}
			}
		\end{align}
		And finally, barely fitting the page,
		\begin{align*}
			\infer{
				(\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx))(\lambda f:\tau\to\tau.\lambda x:\tau\to\tau.fx)(\lambda f:\tau\to\tau.\lambda x:\tau\to\tau f(fx)):T
			}{
				\infer{
					\vdash(\lambda m:T. \lambda n:T.\lambda f:\tau\to\tau.\lambda x:\tau.mf(nfx))(\lambda f:\tau\to \tau.\lambda x:T.fx):T\to T
				}{
					(\ref{eq:join})
				}
				\hspace{.2cm}
				\infer{
					\lambda f:\tau\to\tau.\lambda x:\tau\to\tau f(fx):T
				}{(\ref{eq:abs3})}
			}
		\end{align*}
	\end{proof}
\end{claim*}

\end{document}