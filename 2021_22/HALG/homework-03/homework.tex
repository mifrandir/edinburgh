\documentclass{article}
\usepackage{homework-preamble}
\mkanonthms
\begin{document}
\title{Honours Algebra: Practising Proof 3}
\author{Franz Miltz (UUN: S1971811)}
\date{1 April 2022}
\maketitle
\begin{claim*}[1a]
	The set $\mathcal{A}\subseteq W:=\Mat(n;\R)$ of antisymmetric $(n\times n)$-matrices is an
	vector subspace.
	\begin{proof}
		Let $X,Y\in\mathcal{A}$ and $\lambda\in\R$. By definition, we have $X^\top=-X$ and $Y^\top=-Y$.
		Thus
		\begin{align*}
			(\lambda X)^\top = \lambda X^\top = -\lambda X
		\end{align*}
		and
		\begin{align*}
			(X+Y)^\top = X^\top + Y^\top = -(X+Y).
		\end{align*}
		I.e. $\lambda X,X+Y\in\mathcal{A}$. Finally, $0_n^\top = 0_n = -0_n$, where $0_n\in W$
		is the $(n\times n)$-zero matrix, so $0_n\in\mathcal{A}$. Therefore $\mathcal{A}$ is a vector subspace.
	\end{proof}
\end{claim*}
\begin{claim*}[1a]
	The set $\mathcal{T}\subseteq W$ of upper triangluar $(n\times n)$-matrices is a vector subspace.
	\begin{proof}
		Let $X=(x_{ij}),Y=(y_{ij})\in\mathcal{T}$ and $\lambda\in\R$. By definition, for all $1\leq j<i\leq n$,
		\begin{align*}
			x_{ij}=y_{ij}=0.
		\end{align*}
		Therefore
		\begin{align*}
			\lambda X=(\lambda x_{ij})
		\end{align*}
		is upper triangular as $\lambda\cdot 0=0$, and
		\begin{align*}
			X + Y =(x_{ij}+y_{ij})
		\end{align*}
		is upper triangular as $0+0=0$. Finally, $0_n$ is trivially upper triangular. Thus $0_n,\lambda X,X+Y\in\mathcal{T}$,
		making it a vector subspace.
	\end{proof}
\end{claim*}
\begin{claim*}[1a]
	\begin{align*}
		W=\mathcal{A}\oplus\mathcal{T}.
	\end{align*}
	\begin{proof}
		Consider the map $\phi:\mathcal{A}\times\mathcal{T}\to W$ given by
		\begin{align*}
			(X,U)\mapsto X+U.
		\end{align*}
		Let $X,Y\in\mathcal{A}$ , $U,V\in\mathcal{T}$ such that
		\begin{align*}
			\phi(X,U)=\phi(Y,V),
		\end{align*}
		i.e.
		\begin{align*}
			X-Y=V-U.
		\end{align*}
		As proven above, this shows $X-Y\in\mathcal{A}$ and $V-U\in\mathcal{T}$. However, the only matrix
		that is both upper triangular and antisymmetric is the zero matrix $0_n$.
		Thus
		\begin{align*}
			(X,U)=(Y,V),
		\end{align*}
		proving that $\phi$ is injective.

		Further, for all $X=(x_{ij})\in W$ we define the antisymmetric matrix $Q=(q_{ij})\in\mathcal{A}$
		by
		\begin{align*}
			q_{ij}=\begin{cases}
				-x_{ji} & i<j \\
				0       & i=j \\
				x_{ij}  & i>j
			\end{cases}.
		\end{align*}
		Clearly, $X-Q$ is now upper triangular, so
		\begin{align*}
			\phi(Q,X-Q)=Q+(X-Q)=X.
		\end{align*}
		Therefore $\phi$ is surjective. The claim is now immediate.
	\end{proof}
\end{claim*}

\begin{claim*}[1b]
	We define the maps $F,H:W\times W\to\R$ by
	\begin{align*}
		F:(A,B)\mapsto \tr(AB),\hs H:(A,B)\mapsto \tr(AB^\top).
	\end{align*}
	It is true that both $F$ and $H$ are bilinear forms on $W$.
	\begin{proof}
		Let $A,B,D,C\in W$ and $\lambda,\mu\in\R$.
		\begin{align*}
			F(\lambda A+B,\mu C + D) & =\tr((\lambda A+B)(\mu C+D))                                                            \\
			                         & =\tr(\lambda\mu AC + \lambda AD + \mu BC + BD)             & \text{(Proposition 2.1.9)} \\
			                         & =\lambda\mu\tr (AC) + \lambda\tr(AD) +\mu\tr(BC) + \tr(BD) & \text{(Exercise 41)}       \\
			                         & = \lambda\mu F(AC)+ \lambda F(A,D) + \mu F(B,C) + F(B,D).
		\end{align*}
		Thus $F$ is bilinear. Similarly,
		\begin{align*}
			H(\lambda A+B,\mu C + D) & =\tr((\lambda A+B)(\mu C+D)^\top)                                                                           \\
			                         & =\tr((\lambda A+B)(\mu C^\top + D^\top))                                       & \text{(Exercise 26)}       \\
			                         & =\tr(\lambda\mu AC^\top + \lambda AD^\top + \mu BC^\top + BD^\top)             & \text{(Proposition 2.1.9)} \\
			                         & =\lambda\mu\tr (AC^\top) + \lambda\tr(AD^\top) +\mu\tr(BC^\top) + \tr(BD^\top) & \text{(Exercise 41)}       \\
			                         & = \lambda\mu H(A,C)+ \lambda H(A,D) + \mu H(B,C) + H(B,D),
		\end{align*}
		showing that $H$ is bilinear, too.
	\end{proof}
\end{claim*}

\begin{claim*}[1c]
	It is true that, of $F$ and $H$, only $H$ gives an inner product space.
	\begin{proof}
		To see that $F$ is not an inner product, consider the matrix
		\begin{align*}
			A=\begin{pmatrix}
				0 & -1 \\
				1 & 0
			\end{pmatrix}.
		\end{align*}
		Then $F(A,A)=-2<0$. Thus $F$ is not positive definite. For $H$, clearly
		$H(0_n,0_n)=0$. To see $H(A,A)>0$ for $A=(a_{ij})\not=0$, observe
		\begin{align*}
			\tr(AA^\top) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}^2 > 0
		\end{align*}
		since the diagonal entries of the product are just the dot products of the column
		vectors with themselves. Note in general $\tr(AB)=\tr(BA)$ by \emph{Exercise 41}
		and $\tr A^\top=\tr A$ as the transpose leaves the diagonal unchanged. Now clearly,
		\begin{align*}
			H(A,B) = \tr(AB^\top) = \tr((AB^\top)^\top)  = \tr(A^\top B) = \tr(BA^\top) = H(B,A).
		\end{align*}
		We have shown that $H$ is a positive definite symmetric bilinear form, i.e. an inner
		product.
	\end{proof}
\end{claim*}

\begin{claim*}[2]
	Let $S,T:V\to V$ be commuting $F$-linear endomorphisms of a finite dimensional non-zero vector space $V$ over an
	algebraically closed field $F$. Then $S$ and $T$ have a common non-zero eigenvector.
	\begin{proof}
		Let $\vec v\in V$ be an eigenvector of $T$, i.e. $T(\vec v) = \lambda \vec v$ for some $\lambda\in F$.
		Then
		\begin{align*}
			T(S(\vec v)) = S(T(\vec v)) = S(\lambda\vec v) = \lambda S(\vec v),
		\end{align*}
		i.e. $S$ preserves eigenvectors of $T$. By \emph{Theorem 4.5.4}, we know that $T$ has
		a non-zero eigenvector $\vec v$ with eigenvalue $\lambda\in F$. Consider the eigenspace
		$E=E(\lambda, F)$. Clearly $\dim E \geq 1$. Now observe that the restriction
		$\eval{S}{E}:E\to E$ is an $F$-linear endomorphism of the finite dimensional non-zero vector
		subspace $E\subseteq V$ (\emph{Exercise 76}).
		Invoking \emph{Theorem 4.5.4} once more, we obtain a non-zero eigenvector $\vec u\in E$ of $\eval{S}{E}$
		with eigenvalue $\mu\in F$.
		Since $\vec u\in E$, it is an eigenvector of $T$. Further, since $\eval SE(\vec u) = \mu\vec u$,
		we have $S(\vec u)=\mu \vec u$, i.e. $\vec u$ is a non-zero eigenvector of $S$, too.
	\end{proof}
\end{claim*}

\end{document}