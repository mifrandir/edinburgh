\documentclass{article}
\usepackage{notes-preamble}
\usepackage{enumitem}
\begin{document}
\mkthmstwounified
\title{Honours Algebra (SEM6)}
\author{Franz Miltz}
\maketitle
\tableofcontents
\pagebreak

\section{Vector spaces}

\subsection{Solutions of simultaneous linear equations}

\begin{definition}
    Let $F$ be a field. A \emph{system of linear equations} with $n$ equations and
    $m$ unknowns are equations
    \begin{align*}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m & = b_1 \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m & = b_2 \\
        \vdots                                             \\
        a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m & = b_n \\
    \end{align*}
    where $a_{ij},b_i\in F$ are fixed.

    If all $b_i$ are zero, then we call our system \emph{homogeneous}. Every system
    has an associated \emph{homogenised} system of equations.
    The set $L\subseteq F^m$ consisting of all $m$-tuples that satisfy the
    $n$ equations is called the \emph{solution set} of our system.
\end{definition}

\begin{theorem}[Notes 1.1.4]
    If the solution set of a linear system of equations is non-empty, then we obtain
    all solutions by adding componentwise an arbitrary solution of the associated
    homogenised system to a fixed soltuion of the system itself.
\end{theorem}

\subsection{Fields and vector spaces}

\begin{definition}
    A \emph{field} $F$ is a set with functions
    \begin{align*}
        +     & : F\times F \to F ; (\lambda,\mu) \mapsto \lambda + \mu, \\
        \cdot & : F\times F \to F ; (\lambda,\mu) \mapsto \lambda \mu
    \end{align*}
    such that $(F,+)$ and $(F\setminus{0},\cdot)$ are abelian groups, with
    \begin{align*}
        \lambda(\mu + \nu) = \lambda \mu + \lambda \nu \in F
    \end{align*}
    for any $\lambda,\mu,\nu\in F$. The neutral elements are called $0_F,1_F$.
\end{definition}

\begin{definition}
    A \emph{vector space $V$ over a field $F$} is a pair consisting of an abelian
    group $V=(V,+)$ and a mapping
    \begin{align*}
        F\times V\to V:(\lambda,\vec v) \mapsto \lambda \vec v
    \end{align*}
    such that for all $\lambda,\mu\in F$ and $\vec v,\vec w\in V$ the following
    identities hold:
    \begin{align*}
        \lambda(\vec v + \vec w) & = (\lambda\vec v) + (\lambda \vec w) \\
        (\lambda +\mu)\vec v     & = (\lambda\vec v) + (\mu \vec v)     \\
        \lambda(\mu\vec v)       & = (\lambda\mu)\vec v                 \\
        1_F \vec v               & = \vec v
    \end{align*}
\end{definition}

\begin{lemma}[Notes 1.2.2]
    If $V$ is a vector space and $\vec v\in V$, then $0\vec v =\vec 0$.
\end{lemma}

\begin{lemma}[Notes 1.2.3]
    If $V$ is a vector space and $\vec v\in V$, then $(-1)\vec v = -\vec v$.
\end{lemma}

\begin{lemma}[Notes 1.2.4]
    If $V$ is a vector space over a field $F$, then $\lambda\vec 0=\vec 0$
    for all $\lambda F$. Further, if $\lambda\vec v=\vec 0$ then either
    $\lambda =0$ or $\vec v =\vec 0$.
\end{lemma}

\begin{definition}
    The one-element abelian group $V=0$ equipped with the
    obvious operation is a vector space over every field, and called the
    \emph{trivial vector space} or the \emph{zero vector space}.
\end{definition}

\subsection{Vector subspaces}

\begin{definition}
    A subset $U$ of a vector space $V$ is called a \emph{vector subspace} or
    \emph{subspace} if $U$ contains the zero vector and whenever $\vec u,\vec v\in U$
    and $\lambda\in F$ we have $\vec u + \vec v\in U$ and $\lambda\vec v\in U$.
\end{definition}

\begin{proposition}[Notes 1.4.5]
    Let $T$ be a subset of a vector space $V$ over a field $F$. Then amongst all
    vector subspaces of $V$ that include $T$ there is a smallest vector subspace
    \begin{align*}
        \lra{T} = \lra{T}_F\subseteq V
    \end{align*}
    called the \emph{span of $T$}.
    It can be described as the set of all vectors $\alpha_1\vec v_1+\cdots+\alpha_r\vec v_r$
    with $\alpha_1,...,\alpha_r\in F$ and $\vec v_1,...,\vec v_r\in T$, together
    with the zero vector in the case $T=\emptyset$.
\end{proposition}

\begin{definition}
    A subset $U$ of a vector space $V$ is called a \emph{generating set of $V$}
    if its span is all of $V$. A vector space that has a finite generating set
    is said to be \emph{finitely generated}.
\end{definition}

\subsection{Linear independence and bases}

\begin{definition}
    A subset $L$ of a vector space $V$ is called \emph{linearly independent}
    if for all pairwise different vectors $\vec v_1,...,\vec v_r\in L$ and
    arbitrary scalars $\alpha_1,...,\alpha_r\in F$,
    \begin{align*}
        \alpha_1\vec v_1+\cdots+\alpha_r\vec v_r = \vec 0
        \hs\text{implies}\hs
        \alpha_1=\cdots=\alpha_r=0.
    \end{align*}
\end{definition}

\begin{definition}
    A \emph{basis of a vector space} $V$ is a linearly independent generating
    set in $V$.
\end{definition}

\begin{theorem}[Notes 1.5.11]
    Let $F$ be a field, $V$ a vector space over $F$ and $\vec v_1,...,\vec v_r\in V$
    vectors. The family $\left(\vec v_i\right)_{1\leq i\leq r}$ is a basis of $V$
    if and only if the following mapping
    \begin{align*}
        \Phi:F^r                & \to V                                                \\
        (\alpha_1,...,\alpha_r) & \mapsto \alpha_1\vec v_1 + \cdots + \alpha_r\vec v_r
    \end{align*}
    is a bijection.
\end{theorem}

\begin{theorem}[Theorem 1.5.12]
    The following are equivalent for a subset $E$ of a vectors space $V$:
    \begin{enumerate}
        \item $E$ is a basis of $V$,
        \item $E$ is minimal among all generating sets,
        \item $E$ is maximal among all linearly independent subsets.
    \end{enumerate}
\end{theorem}

\begin{corollary}[Notes 1.5.13]
    Let $V$ be a finitely generated vector space over a field $F$. Then $V$
    has a basis.
\end{corollary}

\begin{theorem}[Notes 1.5.14]
    Let $V$ be a vector space.
    \begin{enumerate}
        \item Let $L\subseteq V$ be a linearly independent subset and $E$ is minimal
              amongst all generating sets of $V$ with the property that $L\subseteq E$,
              then $E$ is a basis.
        \item If $E\subseteq V$ is a generating set and if $L$ is maximal amongst all
              linearly independent subsets of $V$ with the property $L\subseteq E$, then
              $L$ is a basis.
    \end{enumerate}
\end{theorem}

\begin{definition}
    Let $X$ be a set and $F$ a field. The set $\text{Maps}(X,F)$ of all mappings
    $f:X\to F$ becomes an $F$-vector space with the operations of pointwise
    addition and multiplication by a scalar. The subset of all mappings which send
    almost all elements of $X$ to zero is a vector subspace
    \begin{align*}
        F\lra{X}\subseteq \text{Maps}(X,F).
    \end{align*}
    This vector subspace is called the \emph{free vector space on the set $X$}.
\end{definition}

\begin{theorem}[Notes 1.5.16]
    Let $F$ be a field, $V$ and $F$-vector space and $(\vec v_i)_{i\in I}$ a
    family of vectors from $V$. The following are equivalent:
    \begin{enumerate}
        \item The family $(\vec v_i)_{i\in I}$ is a basis for $V$,
        \item for each vector $\vec v\in V$ there is precisely one family $(a_i)_{i\in I}$
              of elements in $F$, almost all of which are zero and such that \begin{align*}
                  \vec v = \sum_{i\in I}a_i\vec v_i.
              \end{align*}
    \end{enumerate}
\end{theorem}

\subsection{Dimension of a vector space}

\begin{theorem}[Fundamental Estimate of Linear Algebra; Notes 1.6.1]
    No linearly independent subset of a given vector space has more elements than
    a generating set. Thus if $V$ is a vector space, $L\subset V$ a linearly independent
    subset and $E\subseteq V$ a generating set, then \begin{align*}
        \abs L \leq \abs E.
    \end{align*}
\end{theorem}

\begin{theorem}[Steinitz Exchange Theorem; Notes 1.6.2]
    Let $V$ be a vector space, $L\subset V$ a finite linearly independent subset
    and $E\subseteq V$ a generating set . Then there is an injection $\phi:L\to E$
    such that $(E\setminus\phi(L))\cup L$ is also a generating set for $V$.
\end{theorem}

\begin{lemma}[Exchange Lemma; Notes 1.6.3]
    Let $V$ be a vector space, $M\subseteq V$ a linearly independent subset, and
    $E\subseteq V$ a generating subset, such that $M\subseteq E$. If $\vec w\in V\setminus M$
    such that $M\cap\{\vec w\}$ is linearly independent, then there exists
    $\vec e\in E\setminus M$ such that $\{E\setminus\{\vec e\}\}\cup \{\vec w\}$ is a
    generating set for $V$.
\end{lemma}

\begin{corollary}[Notes 1.6.4]
    Let $V$ be a finitely generated vector space.
    \begin{enumerate}
        \item $V$ has a finite basis.
        \item $V$ cannot have an infinite basis.
        \item Any two bases of $V$ have the same number of elements.
    \end{enumerate}
\end{corollary}

\begin{definition}
    The cardinality of a basis of a finitely generated vector space $V$
    is called the \emph{dimension} of $V$ and will be denoted $\dim V$.
    If $\dim V=\infty$ we say $V$ is \emph{infinite dimensional}.
\end{definition}

\begin{corollary}[Notes 1.6.7]
    Let $V$ be a finitely generated vector space.
    \begin{enumerate}
        \item Each linearly independent subset $L\subseteq V$ has at most
              $\dim V$ elements, and if $\abs L=\dim V$ then $L$ a basis,
        \item each generating set $E\subset V$ has at least $\dim V$ elements,
              and if $\abs E=\dim V$ then $E$ is actually a basis.
    \end{enumerate}
\end{corollary}

\begin{corollary}[Notes 1.6.8]
    A proper vector subspace of a finite dimensional vector space has itself a
    strictly smaller dimension.
\end{corollary}

\subsection{Linear Mappings}

\begin{definition}
    Let $V,W$ be vector spaces over a field $F$. A mapping $f:V\to W$ is called
    \emph{$F$-linear} or a \emph{homomorphism} if for all $\vec v_1,\vec v_1\in V$ and $\lambda \in F$ we
    have
    \begin{align*}
        f(\vec v_1+\vec v_2) & = f(\vec v_1) + f(\vec v_2), \\
        f(\lambda \vec v_1)  & = \lambda f(\vec v_1).
    \end{align*}
    A bijective linear mapping is called an \emph{isomorphism} of vector spaces.
    A homomorphism from one vector space to itself is called an \emph{endomorphism}.
    An isomorphism of a vector space to itself is called an \emph{automorphism}.
\end{definition}

\begin{definition}
    A point that is sent to itself by a mapping is called a \emph{fixed point} of
    the mapping. Given a mapping $f:X\to X$, we denote the set of fixed points
    \begin{align*}
        X^f = \{x\in X : f(x)=x\}.
    \end{align*}
\end{definition}

\begin{definition}
    Two vector subspaces $V_1,V_2$ of a vector space $V$ are called \emph{complementary}
    if addition defines a bijection
    \begin{align*}
        V_1\times V_2 \to V.
    \end{align*}
\end{definition}

\begin{definition}
    Let $V$ be a vector space with vector subspaces $V_1,...,V_n$. Then the vector
    subspace of $V$ they generated is called the sum and denoted by $V_1+\cdots+V_n$.
    If the natural homomorphism given by addition $V_1+\cdots +V_n\to V$ is an
    injection then we say the \emph{sum of the vector subspaces $V_i$ is direct} and
    we write it as $V_1\oplus\cdots\oplus V_n$.
\end{definition}

\begin{theorem}[Notes 1.7.7]
    Let $n$ be a natural number. Then a vector space over a field is isomorphic
    to $F^n$ if and only if it has dimension $n$.
\end{theorem}

\begin{lemma}[Notes 1.7.8]
    Let $V,W$ be vector spaces over $F$ and let $B\subset V$ be a basis. Then
    restriction of a mapping gives a bijection
    \begin{align*}
        \Hom_F(V,W) & \to \text{Maps}(B,W) \\
        f           & \mapsto \eval{f}{B}
    \end{align*}
\end{lemma}

\begin{proposition}[Notes 1.7.9]
    Let $V,W$ be vector spaces.
    \begin{enumerate}
        \item Every injective mapping $f:V\to W$ has a \emph{left inverse}.
        \item Every surjective mapping $f:V\to W$ has a \emph{right inverse}.
    \end{enumerate}
\end{proposition}

\subsection{Rank-nullity theorem}

\begin{definition}
    The \emph{image} of a linear mapping $f:V\to W$ is the subset $\img(f)=f(V)\subset W$.
    The dimension of the image $\dim(\img V)$ is called the rank of $f$.
    The preimage of the zero vector of a linear mapping $f:V\to W$ is denoted by
    \begin{align*}
        \ker(f):=\inv f(0)=\{v\in V : f(v)=0\}
    \end{align*}
    and is called the \emph{kernel} of the linear mapping $f$. The dimension of the
    kernel $\dim(\ker V)$ is called the nullity of $f$.
\end{definition}

\begin{lemma}[Notes 1.8.2]
    A linear mapping is injective if and only if its kernel is zero.
\end{lemma}

\begin{theorem}[Rank-Nullity Theorem; Notes 1.8.4]
    Let $f:V\to W$ be a linear mapping between two vector spaces. Then
    \begin{align*}
        \dim V = \dim(\ker f) + \dim(\img f).
    \end{align*}
\end{theorem}

\begin{corollary}
    Let $V$ be a vector space, and $U,W\subseteq V$ vector subspaces. Then
    \begin{align*}
        \dim(U+W)+\dim(U\cap W) = \dim U + \dim W.
    \end{align*}
\end{corollary}

\section{Linear mappings and matrices}

\subsection{Linear mappings $F^m\to F^n$ and matrices}

\begin{theorem}[Notes 2.1.1]
    Let $F$ be a field and let $m,n\in\N$. There is a bijection between the space of
    linear mappings $F^m\to F^n$ and the set of matrices with $n$ rows and $m$
    columns and entries in $F$:
    \begin{align*}
        M:\Hom_F(F^m,F^n) & \tilde\to\Mat(n\times m; F) \\
        f                 & \mapsto  \left[f\right]
    \end{align*}
    This attaches to each linear mapping $f$ its \emph{representing matrix
        $M(f):=\left[f\right]$}. The columns of this matrix are the images under $f$
    of the standard basis elements of $F^m$:
    \begin{align*}
        \left[f\right]:=(f(\vec e_1)|\cdots|f(\vec e_m)).
    \end{align*}
\end{theorem}

\begin{definition}
    Let $n,m,l\in\N$, $F$ a field, and let $A\in\Mat(n\times m; F)$ and
    $B\in\Mat(m\times l; F)$ be matrices. The \emph{product}
    $A\circ B=AB\in\Mat(n\times l; F)$ is the matrix defined by
    \begin{align*}
        (AB)_{ik} = \sum_{j=1}^m A_{ij}B_{jk}.
    \end{align*}
\end{definition}

\begin{theorem}[Notes 2.1.8]
    Let $g:F^l\to F^m$ and $f:F^m\to F^n$ be linear mappings. The representing matrix
    of their composition is the product of their representing matrices:
    \begin{align*}
        \left[f\circ g\right]=\left[f\right]\circ\left[g\right].
    \end{align*}
\end{theorem}

\begin{proposition}[Notes 2.1.9]
    Let $k,l,m,n\in\N$, $A,A'\in\Mat(n\times m;F)$, $B,B'\in\Mat(m\times l;F)$,
    $C\in\Mat(l\times k; F)$ and $I=I_m$ the $(m\times m)$-identity matrix.
    Then the following hold:
    \begin{align*}
        (A+A')B & = AB + A'B, \\
        A(B+B') & = AB + AB', \\
        IB      & = B,        \\
        AI      & = A,        \\
        (AB)C   & = A(BC).
    \end{align*}
\end{proposition}

\subsection{Basic properties of matrices}

\begin{definition}
    A matrix $A$ is called \emph{invertible} if there exist matrices $B$ and $C$
    such that $BA=I$ and $AC=I$.
\end{definition}

\begin{definition}
    Let $A\in\Mat(n\times n;F)$. If $A$ is invertible then we denote by $\inv A$
    the unique square matrix such that $A\inv A=\inv AA=I$. This gives rise to the
    \emph{general linear group of $(n\times n)$-matrices}, written as
    \begin{align*}
        \GL(n;F) := \Mat(n;F)^\times.
    \end{align*}
\end{definition}

\begin{definition}
    An \emph{elementary matrix} is a square matrix that differes from the identity
    matrix in at most one entry.
\end{definition}

\begin{theorem}[Notes 2.2.3]
    Every square matrix with entries in a field can be written as a product of
    elementary matrices.
\end{theorem}

\begin{definition}[Smith Normal Form]
    Any matrix whose only non-zero entries lie on the diagonal, and which has
    first $1$s along the diagonal and then $0$s, is said to be in \emph{Smith Normal Form}.
\end{definition}

\begin{theorem}[Notes 2.2.5]
    For each matrix $A\in\Mat(n\times m;F)$ there exist invertible matrices $P$
    and $Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}

\begin{definition}
    The \emph{column rank} of a matrix $A\in\Mat(n\times m;F)$ is the dimension of
    the subspace of $F^n$ generated by the columns of $A$. Similarly, the \emph{row
        rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows
    of $A$.
\end{definition}

\begin{theorem}[Notes 2.2.8]
    The column rank and the row rank of any matrix are equal.
\end{theorem}

\begin{definition}
    The \emph{rank of a matrix} $A\in\Mat(n\times m;F)$, written $\rank(A)$, is the column rank and the
    row rank of $A$. Further, if $\rank(A) = \min\{m,n\}$, we say $A$ has
    \emph{full rank}.
\end{definition}

\subsection{Abstract linear mappings and matrices}

\begin{theorem}[Notes 2.3.1]
    Let $F$ be a field, $V$ and $W$ vector spaces over $F$ with ordered bases $A=(\vec v_1,...,\vec v_m)$
    and $B=(\vec w_1,...,\vec w_n)$. Then to each linear mapping $f:V\to W$ we associate a 
    \emph{representing matrix} $\repr{B}{f}{A}$ whose entries $a_{ij}$ are defined by the 
    identity
    \begin{align*}
        f(\vec v_j) = a_{1j}\vec w_1 + \cdots + a_{nj}\vec w_n \in W.
    \end{align*}
    This produces an isomorphism of vector spaces:
    \begin{align*}
        M^A_B:\Hom_F(V,W)           & \tilde\to \Mat(n\times m; F)\\
        f                           &   \mapsto \repr{B}{f}{A}
    \end{align*}
\end{theorem}

\begin{theorem}[Notes 2.3.2]
    Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases 
    $A,B,C$. If $f:U\to V$ and $g:V\to W$ are linear mappings, then the representing matrix of 
    the composition $g\circ f:U\to W$ is the matrix product of the representing matrices of 
    $f$ and $g$:
    \begin{align*}
        \repr{C}{g\circ f}{A}=\repr{C}{g}{B}\circ\repr{B}{f}{A}
    \end{align*} 
\end{theorem}

\begin{definition}
    Let $V$ be a finite dimensional vector space with an ordered basis $A=(\vec v_1,...,\vec v_m)$.
    We will denote the inverse to the bijection $\Phi_A:F^m\to V,(\alpha_1,...,\alpha_m)^T\mapsto \alpha_1\vec v_1+\cdots+\alpha_m\vec v_m$
    by
    \begin{align*}
        \vec v \mapsto \lrepr{A}{\vec v}
    \end{align*}
    The column vector $\lrepr{A}{\vec v}$ is called the \emph{representation of the vector $\vec v$
    with respect to the basis $A$}.
\end{definition}

\begin{theorem}[Notes 2.3.4]
    Let $V,W$ be finite dimensional $F$-vector spaces with ordered bases $A,B$ and let $f:V\to W$
    be a linear mapping. The following holds for all $\vec v\in V$:
    \begin{align*}
        \lrepr{B}{f(\vec v)}=\repr{B}{f}{A} \circ \lrepr{A}{\vec v}.
    \end{align*}
\end{theorem}

\subsection{Change of matrix by change of basis}

\begin{definition}
    Let $A=(\vec v_1, ...,\vec v_n)$ and $B=(\vec w_1,...,\vec w_n)$ be ordered bases of the same 
    $F$-vector space $V$. Then the matrix representing the identity mapping with respect to the 
    bases 
    \begin{align*}
        \repr{B}{1_V}{A}
    \end{align*}
    is called the \emph{change of basis matrix}. By definition, its entries are given by the 
    equalities $\vec v_j=\sum_{i=1}^n a_{ij}\vec w_i$.
\end{definition}

\begin{theorem}[Notes 2.4.3]
    Let $V,W$ be finite dimensional $F$-vector spaces and let $f:V\to W$ be a linear mapping.
    Suppose that $A,A'$ are ordered bases of $V$ and $B,B'$ are ordered bases of $W$. Then 
    \begin{align*}
        \repr{B'}{f}{A'} = \repr{B'}{1_W}{B}\circ\repr{B}{f}{A}\circ\repr{A}{1_V}{A'}.
    \end{align*} 
\end{theorem}

\begin{corollary}[Notes 2.4.4]
    Let $V$ be a finite dimensional vector space and let $f:V\to V$ be an endomorphism of $V$.
    Suppose that $A,A'$ are ordered bases of $V$. Then 
    \begin{align*}
        \repr{A'}{f}{A'}=\inv{\repr{A}{1_V}{A'}}\circ\repr{A}{f}{A}\circ\repr{A}{1_V}{A'}.
    \end{align*} 
\end{corollary}

\begin{theorem}[Notes 2.4.5]
    Let $f:V\to W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist
    and ordered basis $A$ of $V$ and an ordered basis $B$ of $W$ such that the representing matrix
    $\repr{B}{f}{A}$ has zero entries everywhere except possibly on the diagonal, and along the 
    diagonal there are $1$'s first, followed by $0$'s.
\end{theorem}

\begin{definition}
    The \emph{trace} $\tr(A)$ of a square matrix $A$ is defined to be the sum of its diagonal entries.
\end{definition}

\section{Rings and modules}

\subsection{Rings}

\begin{definition}
    A \emph{ring} is a set with two operations $(R,+,\cdot)$ that satisfy: 
    \begin{enumerate}
        \item $(R,+)$ is an abelian group (associativity, identity, inverse and commutativity);
        \item $(R,\cdot)$ is a monoid (associativity and identity);
        \item For all $a,b,c\in R$ \begin{align*}
            a\cdot (b+c) & = (a\cdot b) + (a\cdot c),\\
            (a+b)\cdot c & = (a\cdot c) + (b\cdot c).
        \end{align*}
    \end{enumerate}
    A ring in which multiplication is commutative is called a \emph{commutative ring}.
\end{definition}

\begin{definition}
    A \emph{field} is a non-zero commutative ring $F$ in which every non-zero element 
    $a\in F$ has a multiplicative inverse $\inv a\in F$.
\end{definition}

\begin{proposition}[Notes 3.1.11]
    Let $m\in\N$. The commutative ring $\Z/m\Z$ is a field iff $m$ is prime.
\end{proposition}

\subsection{Properties of rings}

\begin{lemma}[Notes 3.2.1]
    Let $(R,+,\cdot)$ be a ring and let $a,b\in R$. Then 
    \begin{enumerate}
        \item $0a=0=a0$.
        \item $(-a)b=-(ab)=a(-b)$.
        \item $(-a)(-b) = ab$.
    \end{enumerate} 
\end{lemma}

\begin{definition}
    Let $m\in\Z$. The \emph{$m$-th multiple $ma$ of an element $a$} in an abelian 
    group $R$ is 
    \begin{align*}
        ma = \underbrace{a + \cdots + a}_\text{$m$ terms} \hs \text{if } m >0,
    \end{align*}
    $0a=0$, and negative multiples are defined by $(-m)a=-(ma)$.
\end{definition}

\begin{lemma}[Notes 3.2.4]
    Let $R$ be a ring, let $a,b\in R$ and let $m,n\in\Z$. Then:
    \begin{enumerate}
        \item $m(a+b)=ma+mb$;
        \item $(m+n)a=ma+na$;
        \item $m(na)=(mn)a$;
        \item $m(ab)=(ma)b=a(mb)$;
        \item $(ma)(nb)=(mn)(ab)$.
    \end{enumerate} 
\end{lemma}

\begin{definition}
    Let $R$ be a ring. An element $a\in R$ is called a \emph{unit} if it has a 
    multiplicative inverse in $R$.
\end{definition}

\begin{proposition}
    Thet set $R^\times$ of units in a ring $R$ forms a group under multiplication.
\end{proposition}

\begin{definition}
    In a ring $R$ a non-zero element $a$ is called a \emph{zero-divisor} if 
    there exists a non-zero element $b$ such that either $ab=0$ or $ba=0$.
\end{definition}

\begin{definition}
    An \emph{integral domain} is a non-zero commutative ring that has no zero-divisors.
\end{definition}

\begin{proposition}[Notes 3.2.15]
    Let $R$ be an integral domain and let $a,b,c\in R$. If $ab=ac$ and $a\not=0$
    then $b=c$.
\end{proposition}

\begin{proposition}[Notes 3.2.16]
    Let $m\in\N$. Then $\Z/m\Z$ is an integral domain iff $m$ is prime. 
\end{proposition}

\begin{theorem}[Notes 3.2.17]
    Every finite integral domain is a field.
\end{theorem}

\subsection{Polynomials}

\begin{definition}
    Let $R$ be a ring. A \emph{polynomial over $R$} is an expression of the form 
    \begin{align*}
        P=a_0+a_1+X+\cdots+a_mX^m
    \end{align*} 
    for some non-negative integer $m$ and elements $a_i\in R$ for $i=0,...,m$.
    The set of all polynomials over $R$ is denoted by $R[X]$. In case $a_m$ 
    is non-zero, the polynomial $P$ has \emph{degree} $m$, written $\deg P$,
    and $a_m$ is its \emph{leading coefficient}. When the leading coefficient is 
    $1$ the polynomial is called \emph{monic}.
\end{definition}

\begin{definition}
    The set $R[X]$ is called the \emph{ring of polynomials with coefficients 
    in $R$}. The zero and the identity of $R[X]$ are the zero and the identity of 
    $R$, respectively.
\end{definition}

\begin{lemma}
    \begin{enumerate}
        \item If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors 
            and $\deg(PQ)=\deg P + \deg Q$ for non-zero $P,Q\in R[X]$.
        \item If $R$ is an integral domain then so is $R[X]$.
    \end{enumerate} 
\end{lemma}

\begin{theorem}[Notes 3.3.4]
    Let $R$ be an integral domain and let $P,Q\in R[X]$ with $Q$ monic. Then there exists 
    a unique $A,B\in R[X]$ such that $P=AQ+B$ and $\deg B<\deg Q$ or $B=0$. 
\end{theorem}

\begin{definition}
    Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$
    can be \emph{evaluated} at the element $\lambda\in R$ to produce $P(\lambda)$
    by replacing the powers of $X$ in $P$ by the corresponding powers of $\lambda$. 
    In this way we havea mapping 
    \begin{align*}
        R[X] \to \text{Maps}(R,R).
    \end{align*}
    An element $\lambda\in R$ is a \emph{root} of $P$ if $P(\lambda) = 0$.
\end{definition}

\begin{proposition}[Notes 3.3.9]
    Let $R$ be a commutative ring, let $\lambda\in R$ and $P(X)\in R[X]$. Then $\lambda$
    is a root of $P(X)$ iff $(X-\lambda)$ divides $P(X)$. 
\end{proposition}

\begin{theorem}[Notes 3.3.10]
    Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial
    $P\in R[X]\setminus\{0\}$ has at most $\deg P$ roots in $R$. 
\end{theorem}

\begin{definition}
    A field $F$ is \emph{algebraically closed} if each non-constant polynomial $P\in F[X]\setminus F$
    with coefficients in $F$ has a root in $F$.
\end{definition}

\begin{theorem}[Fundamental Theorem of Algebra]
    The field of complex numbers, $\C$, is algebraically closed. 
\end{theorem}

\begin{theorem}[Notes 3.3.14]
    If $F$ is an algebraically closed field, then every non-zero polynomial
    $P\in F[X]\setminus\{0\}$ \emph{decomposes into linear factors}
    \begin{align*}
        P=c(X-\lambda_1)\cdots(X-\lambda_n)
    \end{align*} 
    with $n\geq 0$, $c\in F^\times$ and $\lambda_1,...,\lambda_n\in F$. This 
    decomposition is unique up to reordering the factors.
\end{theorem}

\subsection{Homomorphisms, ideals and subrings}

\begin{definition}
    Let $R$ and $S$ be rings. A mapping $f:R\to S$ is a \emph{ring homomorphism} if 
    the following hold for all $x,y\in R$:
    \begin{align*}
        f(x+y) & = f(x) + f(y), \\
        f(xy)  & = f(x)f(y).
    \end{align*}
\end{definition}

\begin{lemma}[Notes 3.4.5]
    Let $R$ and $S$ be rings and $f:R\to S$ a ring homomorphism. Then for all $x,y\in R$ 
    and $m\in\Z$:
    \begin{enumerate}
        \item $f(0_R)=0_S$;
        \item $f(-x)=-f(x)$;
        \item $f(x-y)=f(x)-f(y)$;
        \item $f(mx)=mf(x)$,
    \end{enumerate} 
    where $mx$ denotes the $m$-th multiple of $x$.
\end{lemma}

\begin{definition}
    A subset $I$ of a ring $R$ is an \emph{ideal}, written $I\trianglelefteq R$, if 
    the following hold:
    \begin{enumerate}
        \item $I\not=\emptyset$;
        \item $I$ is closed under subtraction;
        \item for all $i\in I$ and $r\in R$ we have $ri,ir\in I$.
    \end{enumerate}
\end{definition}

\begin{definition}
    Let $R$ be a commutative ring and let $T\subset R$. Then the \emph{ideal of $R$
    generated by $T$} is the set 
    \begin{align*}
        _R\lra{T} = \{r_1t_1+\cdots+r_mt_m : t_1,...,t_m\in T,r_1,...,r_m\in R\},
    \end{align*}
    together with the zero element in case $T=\emptyset$. If $T$ is finite we will 
    abuse notation by writing $_R\lra{t_1,...,t_n}$.
\end{definition}

\begin{proposition}[Notes 3.4.14]
    Let $R$ be a commutative ring and let $T\subseteq R$. Then $_R\lra{T}$ is the 
    smallest ideal of $R$ that contains $T$. 
\end{proposition}

\begin{definition}
    Let $R$ be a commutative ring. An ideal $I$ of $R$ is called \emph{principal} if 
    $I=\lra{t}$ for some $t\in R$.
\end{definition}

\begin{definition}
    Let $R$ and $S$ be rings with zero elements $0_R$ and $0_S$ respectively and 
    let $f:R\to S$ be a ring homomorphism. Since $f$ is in particular a group homomorphism
    from $(R,+)$ to $(S,+)$, the \emph{kernel} of $f$ already has a meaning 
    \begin{align*}
        \ker f = \{r\in R : f(r) = 0_S\}.
    \end{align*}
\end{definition}

\begin{proposition}[Notes 3.4.18]
    Let $f:R\to S$ be a ring homomorphism. Then $\ker f$ is an ideal of $R$.
\end{proposition}

\begin{lemma}[Notes 3.4.20]
    $f$ is injective iff $\ker f = \{0\}$. 
\end{lemma}

\begin{lemma}[Notes 3.4.21]
    The intersection of any collection of ideals of a ring $R$ is an ideal of $R$.
\end{lemma}

\begin{lemma}[Notes 3.4.22]
    Let $I$ and $J$ be ideals of a ring $R$. Then 
    \begin{align*}
        I+J=\{a+b:a\in I, b\in J\}
    \end{align*}
    is an ideal of $R$.
\end{lemma}

\begin{definition}
    Let $R$ be a ring. A subset $R'$ of $R$ is a subring of $R$ if $R'$ itself 
    is a ring under the operations of addition and multiplication defined in $R$.
\end{definition}

\begin{proposition}[Notes 3.4.26]
    Let $R'$ be a subset of a ring $R$. Then $R'$ is a subring iff 
    \begin{enumerate}
        \item $R'$ has a multiplicative identity, and 
        \item $R'$ is closed under subtraction, and 
        \item $R'$ is closed under multiplication.
    \end{enumerate} 
\end{proposition}

\begin{proposition}[Notes 3.4.28]
    Let $R$ and $S$ be rings and $f:R\to S$ a ring homomorphism.
    \begin{enumerate}
        \item If $R'$ is a subring of $R$ then $f(R')$ is a subring of $S$. In particular
            $\im f$ is a subring of $S$.
        \item Assume that $f(1_R)=1_S$. Then if $x$ is a unit in $R$, $f(x)$ is a unit 
            in $S$ and $\inv{(f(x))} = f(\inv x)$.
    \end{enumerate} 
\end{proposition}

\end{document}
