\documentclass{article}
\usepackage{notes-preamble}
\usepackage{enumitem}
\begin{document}
\mkthmstwounified
\title{Honours Algebra (SEM6)}
\author{Franz Miltz}
\maketitle
\tableofcontents
\pagebreak

\section{Vector spaces}

\subsection{Solutions of simultaneous linear equations}

\begin{definition}
	Let $F$ be a field. A \emph{finitely generated}h{system of linear equations} with $n$ equations and
	$m$ unknowns are equations
	\begin{align*}
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m & = b_1 \\
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m & = b_2 \\
		\vdots                                             \\
		a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m & = b_n \\
	\end{align*}
	where $a_{ij},b_i\in F$ are fixed.

	Iff all $b_i$ are zero, then we call our system \emph{homogeneous}. Every system
	has an associated \emph{homogenised} system of equations.
	The set $L\subseteq F^m$ consisting of all $m$-tuples that satisfy the
	$n$ equations is called the \emph{solution set} of our system.
\end{definition}

\begin{theorem}[Theorem 1.1.4]
	If the solution set of a linear system of equations is non-empty, then we obtain
	all solutions by adding componentwise an arbitrary solution of the associated
	homogenised system to a fixed soltuion of the system itself.
\end{theorem}

\subsection{Fields and vector spaces}

\begin{definition}
	A \emph{field} $F$ is a set with functions
	\begin{align*}
		+     & : F\times F \to F ; (\lambda,\mu) \mapsto \lambda + \mu, \\
		\cdot & : F\times F \to F ; (\lambda,\mu) \mapsto \lambda \mu
	\end{align*}
	such that $(F,+)$ and $(F\setminus{0},\cdot)$ are abelian groups, with
	\begin{align*}
		\lambda(\mu + \nu) = \lambda \mu + \lambda \nu \in F
	\end{align*}
	for any $\lambda,\mu,\nu\in F$. The neutral elements are called $0_F,1_F$.
\end{definition}

\begin{definition}
	A \emph{vector space $V$ over a field $F$} is a pair consisting of an abelian
	group $V=(V,+)$ and a mapping
	\begin{align*}
		F\times V\to V:(\lambda,\vec v) \mapsto \lambda \vec v
	\end{align*}
	such that for all $\lambda,\mu\in F$ and $\vec v,\vec w\in V$ the following
	identities hold:
	\begin{align*}
		\lambda(\vec v + \vec w) & = (\lambda\vec v) + (\lambda \vec w) \\
		(\lambda +\mu)\vec v     & = (\lambda\vec v) + (\mu \vec v)     \\
		\lambda(\mu\vec v)       & = (\lambda\mu)\vec v                 \\
		1_F \vec v               & = \vec v
	\end{align*}
\end{definition}

\begin{lemma}[Lemma 1.2.2]
	If $V$ is a vector space and $\vec v\in V$, then $0\vec v =\vec 0$.
\end{lemma}

\begin{lemma}[Lemma 1.2.3]
	If $V$ is a vector space and $\vec v\in V$, then $(-1)\vec v = -\vec v$.
\end{lemma}

\begin{lemma}[Lemma 1.2.4]
	If $V$ is a vector space over a field $F$, then $\lambda\vec 0=\vec 0$
	for all $\lambda F$. Further, if $\lambda\vec v=\vec 0$ then either
	$\lambda =0$ or $\vec v =\vec 0$.
\end{lemma}

\begin{definition}
	The one-element abelian group $V=0$ equipped with the
	obvious operation is a vector space over every field, and called the
	\emph{trivial vector space} or the \emph{zero vector space}.
\end{definition}

\subsection{Vector subspaces}

\begin{definition}
	A subset $U$ of a vector space $V$ is called a \emph{vector subspace} or
	\emph{subspace} iff $U$ contains the zero vector and whenever $\vec u,\vec v\in U$
	and $\lambda\in F$ we have $\vec u + \vec v\in U$ and $\lambda\vec v\in U$.
\end{definition}

\begin{proposition}[Proposition 1.4.5]
	Let $T$ be a subset of a vector space $V$ over a field $F$. Then amongst all
	vector subspaces of $V$ that include $T$ there is a smallest vector subspace
	\begin{align*}
		\lra{T} = \lra{T}_F\subseteq V
	\end{align*}
	called the \emph{span of $T$}.
	It can be described as the set of all vectors $\alpha_1\vec v_1+\cdots+\alpha_r\vec v_r$
	with $\alpha_1,...,\alpha_r\in F$ and $\vec v_1,...,\vec v_r\in T$, together
	with the zero vector in the case $T=\emptyset$.
\end{proposition}

\begin{definition}
	A subset $U$ of a vector space $V$ is a \emph{generating set of $V$}
	iff its span is all of $V$. A vector space that has a finite generating set
	is said to be \emph{finitely generated}.
\end{definition}

\subsection{Linear independence and bases}

\begin{definition}
	A subset $L$ of a vector space $V$ is \emph{linearly independent}
	iff for all pairwise different vectors $\vec v_1,...,\vec v_r\in L$ and
	arbitrary scalars $\alpha_1,...,\alpha_r\in F$,
	\begin{align*}
		\alpha_1\vec v_1+\cdots+\alpha_r\vec v_r = \vec 0
		\hs\text{implies}\hs
		\alpha_1=\cdots=\alpha_r=0.
	\end{align*}
\end{definition}

\begin{definition}
	A \emph{basis of a vector space} $V$ is a linearly independent generating
	set in $V$.
\end{definition}

\begin{theorem}[Theorem 1.5.11]
	Let $F$ be a field, $V$ a vector space over $F$ and $\vec v_1,...,\vec v_r\in V$
	vectors. The family $\left(\vec v_i\right)_{1\leq i\leq r}$ is a basis of $V$
	iff the following mapping
	\begin{align*}
		\Phi:F^r                & \to V                                                \\
		(\alpha_1,...,\alpha_r) & \mapsto \alpha_1\vec v_1 + \cdots + \alpha_r\vec v_r
	\end{align*}
	is a bijection.
\end{theorem}

\begin{theorem}[Theorem 1.5.12]
	The following are equivalent for a subset $E$ of a vectors space $V$:
	\begin{enumerate}
		\item $E$ is a basis of $V$,
		\item $E$ is minimal among all generating sets,
		\item $E$ is maximal among all linearly independent subsets.
	\end{enumerate}
\end{theorem}

\begin{corollary}[Corollary 1.5.13]
	Let $V$ be a finitely generated vector space over a field $F$. Then $V$
	has a basis.
\end{corollary}

\begin{theorem}[Theorem 1.5.14]
	Let $V$ be a vector space.
	\begin{enumerate}
		\item Let $L\subseteq V$ be a linearly independent subset and $E$ is minimal
		      amongst all generating sets of $V$ with the property that $L\subseteq E$,
		      then $E$ is a basis.
		\item If $E\subseteq V$ is a generating set and if $L$ is maximal amongst all
		      linearly independent subsets of $V$ with the property $L\subseteq E$, then
		      $L$ is a basis.
	\end{enumerate}
\end{theorem}

\begin{definition}
	Let $X$ be a set and $F$ a field. The set $\text{Maps}(X,F)$ of all mappings
	$f:X\to F$ becomes an $F$-vector space with the operations of pointwise
	addition and multiplication by a scalar. The subset of all mappings which send
	almost all elements of $X$ to zero is a vector subspace
	\begin{align*}
		F\lra{X}\subseteq \text{Maps}(X,F).
	\end{align*}
	This vector subspace is called the \emph{free vector space on the set $X$}.
\end{definition}

\begin{theorem}[Theorem 1.5.16]
	Let $F$ be a field, $V$ and $F$-vector space and $(\vec v_i)_{i\in I}$ a
	family of vectors from $V$. The following are equivalent:
	\begin{enumerate}
		\item The family $(\vec v_i)_{i\in I}$ is a basis for $V$,
		\item for each vector $\vec v\in V$ there is precisely one family $(a_i)_{i\in I}$
		      of elements in $F$, almost all of which are zero and such that \begin{align*}
			      \vec v = \sum_{i\in I}a_i\vec v_i.
		      \end{align*}
	\end{enumerate}
\end{theorem}

\subsection{Dimension of a vector space}

\begin{theorem}[Fundamental Estimate of Linear Algebra; Theorem 1.6.1]
	No linearly independent subset of a given vector space has more elements than
	a generating set. Thus if $V$ is a vector space, $L\subset V$ a linearly independent
	subset and $E\subseteq V$ a generating set, then \begin{align*}
		\abs L \leq \abs E.
	\end{align*}
\end{theorem}

\begin{theorem}[Steinitz Exchange Theorem; Theorem 1.6.2]
	Let $V$ be a vector space, $L\subset V$ a finite linearly independent subset
	and $E\subseteq V$ a generating set . Then there is an injection $\phi:L\to E$
	such that $(E\setminus\phi(L))\cup L$ is also a generating set for $V$.
\end{theorem}

\begin{lemma}[Exchange Lemma; Lemma 1.6.3]
	Let $V$ be a vector space, $M\subseteq V$ a linearly independent subset, and
	$E\subseteq V$ a generating subset, such that $M\subseteq E$. If $\vec w\in V\setminus M$
	such that $M\cap\{\vec w\}$ is linearly independent, then there exists
	$\vec e\in E\setminus M$ such that $\{E\setminus\{\vec e\}\}\cup \{\vec w\}$ is a
	generating set for $V$.
\end{lemma}

\begin{corollary}[Corollary 1.6.4]
	Let $V$ be a finitely generated vector space.
	\begin{enumerate}
		\item $V$ has a finite basis.
		\item $V$ cannot have an infinite basis.
		\item Any two bases of $V$ have the same number of elements.
	\end{enumerate}
\end{corollary}

\begin{definition}
	The cardinality of a basis of a finitely generated vector space $V$
	is called the \emph{dimension} of $V$ and will be denoted $\dim V$.
	If $\dim V=\infty$ we say $V$ is \emph{infinite dimensional}.
\end{definition}

\begin{corollary}[Corollary 1.6.7]
	Let $V$ be a finitely generated vector space.
	\begin{enumerate}
		\item Each linearly independent subset $L\subseteq V$ has at most
		      $\dim V$ elements, and if $\abs L=\dim V$ then $L$ a basis,
		\item each generating set $E\subset V$ has at least $\dim V$ elements,
		      and if $\abs E=\dim V$ then $E$ is actually a basis.
	\end{enumerate}
\end{corollary}

\begin{corollary}[Corollary 1.6.8]
	A proper vector subspace of a finite dimensional vector space has itself a
	strictly smaller dimension.
\end{corollary}

\subsection{Linear Mappings}

\begin{definition}
	Let $V,W$ be vector spaces over a field $F$. A mapping $f:V\to W$ is called
	\emph{$F$-linear} or a \emph{homomorphism} iff for all $\vec v_1,\vec v_1\in V$ and $\lambda \in F$ we
	have
	\begin{align*}
		f(\vec v_1+\vec v_2) & = f(\vec v_1) + f(\vec v_2), \\
		f(\lambda \vec v_1)  & = \lambda f(\vec v_1).
	\end{align*}
	A bijective linear mapping is called an \emph{isomorphism} of vector spaces.
	A homomorphism from one vector space to itself is called an \emph{endomorphism}.
	An isomorphism of a vector space to itself is called an \emph{automorphism}.
\end{definition}

\begin{definition}
	A point that is sent to itself by a mapping is called a \emph{fixed point} of
	the mapping. Given a mapping $f:X\to X$, we denote the set of fixed points
	\begin{align*}
		X^f = \{x\in X : f(x)=x\}.
	\end{align*}
\end{definition}

\begin{definition}
	Two vector subspaces $V_1,V_2$ of a vector space $V$ are called \emph{complementary}
	iff addition defines a bijection
	\begin{align*}
		V_1\times V_2 \to V.
	\end{align*}
\end{definition}

\begin{definition}
	Let $V$ be a vector space with vector subspaces $V_1,...,V_n$. Then the vector
	subspace of $V$ they generated is called the sum and denoted by $V_1+\cdots+V_n$.
	If the natural homomorphism given by addition $V_1+\cdots +V_n\to V$ is an
	injection then we say the \emph{sum of the vector subspaces $V_i$ is direct} and
	we write it as $V_1\oplus\cdots\oplus V_n$.
\end{definition}

\begin{theorem}[Theorem 1.7.7]
	Let $n$ be a natural number. Then a vector space over a field is isomorphic
	to $F^n$ iff it has dimension $n$.
\end{theorem}

\begin{lemma}[Lemma 1.7.8]
	Let $V,W$ be vector spaces over $F$ and let $B\subset V$ be a basis. Then
	restriction of a mapping gives a bijection
	\begin{align*}
		\Hom_F(V,W) & \to \text{Maps}(B,W) \\
		f           & \mapsto \eval{f}{B}
	\end{align*}
\end{lemma}

\begin{proposition}[Proposition 1.7.9]
	Let $V,W$ be vector spaces.
	\begin{enumerate}
		\item Every injective mapping $f:V\to W$ has a \emph{left inverse}.
		\item Every surjective mapping $f:V\to W$ has a \emph{right inverse}.
	\end{enumerate}
\end{proposition}

\subsection{Rank-nullity theorem}

\begin{definition}
	The \emph{image} of a linear mapping $f:V\to W$ is the subset $\img(f)=f(V)\subset W$.
	The dimension of the image $\dim(\img V)$ is called the rank of $f$.
	The preimage of the zero vector of a linear mapping $f:V\to W$ is denoted by
	\begin{align*}
		\ker(f):=\inv f(0)=\{v\in V : f(v)=0\}
	\end{align*}
	and is called the \emph{kernel} of the linear mapping $f$. The dimension of the
	kernel $\dim(\ker V)$ is called the nullity of $f$.
\end{definition}

\begin{lemma}[Lemma 1.8.2]
	A linear mapping is injective if and only if its kernel is zero.
\end{lemma}

\begin{theorem}[Rank-Nullity Theorem; Theorem 1.8.4]
	Let $f:V\to W$ be a linear mapping between two vector spaces. Then
	\begin{align*}
		\dim V = \dim(\ker f) + \dim(\img f).
	\end{align*}
\end{theorem}

\begin{corollary}
	Let $V$ be a vector space, and $U,W\subseteq V$ vector subspaces. Then
	\begin{align*}
		\dim(U+W)+\dim(U\cap W) = \dim U + \dim W.
	\end{align*}
\end{corollary}

\section{Linear mappings and matrices}

\subsection{Linear mappings $F^m\to F^n$ and matrices}

\begin{theorem}[Theorem 2.1.1]
	Let $F$ be a field and let $m,n\in\N$. There is a bijection between the space of
	linear mappings $F^m\to F^n$ and the set of matrices with $n$ rows and $m$
	columns and entries in $F$:
	\begin{align*}
		M:\Hom_F(F^m,F^n) & \tilde\to\Mat(n\times m; F) \\
		f                 & \mapsto  \left[f\right]
	\end{align*}
	This attaches to each linear mapping $f$ its \emph{representing matrix
		$M(f):=\left[f\right]$}. The columns of this matrix are the images under $f$
	of the standard basis elements of $F^m$:
	\begin{align*}
		\left[f\right]:=(f(\vec e_1)|\cdots|f(\vec e_m)).
	\end{align*}
\end{theorem}

\begin{definition}
	Let $n,m,l\in\N$, $F$ a field, and let $A\in\Mat(n\times m; F)$ and
	$B\in\Mat(m\times l; F)$ be matrices. The \emph{product}
	$A\circ B=AB\in\Mat(n\times l; F)$ is the matrix defined by
	\begin{align*}
		(AB)_{ik} = \sum_{j=1}^m A_{ij}B_{jk}.
	\end{align*}
\end{definition}

\begin{theorem}[Theorem 2.1.8]
	Let $g:F^l\to F^m$ and $f:F^m\to F^n$ be linear mappings. The representing matrix
	of their composition is the product of their representing matrices:
	\begin{align*}
		\left[f\circ g\right]=\left[f\right]\circ\left[g\right].
	\end{align*}
\end{theorem}

\begin{proposition}[Proposition 2.1.9]
	Let $k,l,m,n\in\N$, $A,A'\in\Mat(n\times m;F)$, $B,B'\in\Mat(m\times l;F)$,
	$C\in\Mat(l\times k; F)$ and $I=I_m$ the $(m\times m)$-identity matrix.
	Then the following hold:
	\begin{align*}
		(A+A')B & = AB + A'B, \\
		A(B+B') & = AB + AB', \\
		IB      & = B,        \\
		AI      & = A,        \\
		(AB)C   & = A(BC).
	\end{align*}
\end{proposition}

\subsection{Basic properties of matrices}

\begin{definition}
	A matrix $A$ is called \emph{invertible} iff there exist matrices $B$ and $C$
	such that $BA=I$ and $AC=I$.
\end{definition}

\begin{definition}
	Let $A\in\Mat(n\times n;F)$. If $A$ is invertible then we denote by $\inv A$
	the unique square matrix such that $A\inv A=\inv AA=I$. This gives rise to the
	\emph{general linear group of $(n\times n)$-matrices}, written as
	\begin{align*}
		\GL(n;F) := \Mat(n;F)^\times.
	\end{align*}
\end{definition}

\begin{definition}
	An \emph{elementary matrix} is a square matrix that differes from the identity
	matrix in at most one entry.
\end{definition}

\begin{theorem}[Theorem 2.2.3]
	Every square matrix with entries in a field can be written as a product of
	elementary matrices.
\end{theorem}

\begin{definition}[Smith Normal Form]
	Any matrix whose only non-zero entries lie on the diagonal, and which has
	first $1$s along the diagonal and then $0$s, is said to be in \emph{Smith Normal Form}.
\end{definition}

\begin{theorem}[Theorem 2.2.5]
	For each matrix $A\in\Mat(n\times m;F)$ there exist invertible matrices $P$
	and $Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}

\begin{definition}
	The \emph{column rank} of a matrix $A\in\Mat(n\times m;F)$ is the dimension of
	the subspace of $F^n$ generated by the columns of $A$. Similarly, the \emph{row
		rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows
	of $A$.
\end{definition}

\begin{theorem}[Theorem 2.2.8]
	The column rank and the row rank of any matrix are equal.
\end{theorem}

\begin{definition}
	The \emph{rank of a matrix} $A\in\Mat(n\times m;F)$, written $\rank(A)$, is the column rank and the
	row rank of $A$. Further, if $\rank(A) = \min\{m,n\}$, we say $A$ has
	\emph{full rank}.
\end{definition}

\subsection{Abstract linear mappings and matrices}

\begin{theorem}[Theorem 2.3.1]
	Let $F$ be a field, $V$ and $W$ vector spaces over $F$ with ordered bases $A=(\vec v_1,...,\vec v_m)$
	and $B=(\vec w_1,...,\vec w_n)$. Then to each linear mapping $f:V\to W$ we associate a
	\emph{representing matrix} $\repr{B}{f}{A}$ whose entries $a_{ij}$ are defined by the
	identity
	\begin{align*}
		f(\vec v_j) = a_{1j}\vec w_1 + \cdots + a_{nj}\vec w_n \in W.
	\end{align*}
	This produces an isomorphism of vector spaces:
	\begin{align*}
		M^A_B:\Hom_F(V,W) & \tilde\to \Mat(n\times m; F) \\
		f                 & \mapsto \repr{B}{f}{A}
	\end{align*}
\end{theorem}

\begin{theorem}[Theorem 2.3.2]
	Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases
	$A,B,C$. If $f:U\to V$ and $g:V\to W$ are linear mappings, then the representing matrix of
	the composition $g\circ f:U\to W$ is the matrix product of the representing matrices of
	$f$ and $g$:
	\begin{align*}
		\repr{C}{g\circ f}{A}=\repr{C}{g}{B}\circ\repr{B}{f}{A}
	\end{align*}
\end{theorem}

\begin{definition}
	Let $V$ be a finite dimensional vector space with an ordered basis $A=(\vec v_1,...,\vec v_m)$.
	We will denote the inverse to the bijection $\Phi_A:F^m\to V,(\alpha_1,...,\alpha_m)^T\mapsto \alpha_1\vec v_1+\cdots+\alpha_m\vec v_m$
	by
	\begin{align*}
		\vec v \mapsto \lrepr{A}{\vec v}
	\end{align*}
	The column vector $\lrepr{A}{\vec v}$ is called the \emph{representation of the vector $\vec v$
		with respect to the basis $A$}.
\end{definition}

\begin{theorem}[Theorem 2.3.4]
	Let $V,W$ be finite dimensional $F$-vector spaces with ordered bases $A,B$ and let $f:V\to W$
	be a linear mapping. The following holds for all $\vec v\in V$:
	\begin{align*}
		\lrepr{B}{f(\vec v)}=\repr{B}{f}{A} \circ \lrepr{A}{\vec v}.
	\end{align*}
\end{theorem}

\subsection{Change of matrix by change of basis}

\begin{definition}
	Let $A=(\vec v_1, ...,\vec v_n)$ and $B=(\vec w_1,...,\vec w_n)$ be ordered bases of the same
	$F$-vector space $V$. Then the matrix representing the identity mapping with respect to the
	bases
	\begin{align*}
		\repr{B}{1_V}{A}
	\end{align*}
	is called the \emph{change of basis matrix}. By definition, its entries are given by the
	equalities $\vec v_j=\sum_{i=1}^n a_{ij}\vec w_i$.
\end{definition}

\begin{theorem}[Theorem 2.4.3]
	Let $V,W$ be finite dimensional $F$-vector spaces and let $f:V\to W$ be a linear mapping.
	Suppose that $A,A'$ are ordered bases of $V$ and $B,B'$ are ordered bases of $W$. Then
	\begin{align*}
		\repr{B'}{f}{A'} = \repr{B'}{1_W}{B}\circ\repr{B}{f}{A}\circ\repr{A}{1_V}{A'}.
	\end{align*}
\end{theorem}

\begin{corollary}[Corollary 2.4.4]
	Let $V$ be a finite dimensional vector space and let $f:V\to V$ be an endomorphism of $V$.
	Suppose that $A,A'$ are ordered bases of $V$. Then
	\begin{align*}
		\repr{A'}{f}{A'}=\inv{\repr{A}{1_V}{A'}}\circ\repr{A}{f}{A}\circ\repr{A}{1_V}{A'}.
	\end{align*}
\end{corollary}

\begin{theorem}[Theorem 2.4.5]
	Let $f:V\to W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist
	and ordered basis $A$ of $V$ and an ordered basis $B$ of $W$ such that the representing matrix
	$\repr{B}{f}{A}$ has zero entries everywhere except possibly on the diagonal, and along the
	diagonal there are $1$'s first, followed by $0$'s.
\end{theorem}

\begin{definition}
	The \emph{trace} $\tr(A)$ of a square matrix $A$ is defined to be the sum of its diagonal entries.
\end{definition}

\section{Rings and modules}

\subsection{Rings}

\begin{definition}
	A \emph{ring} is a set with two operations $(R,+,\cdot)$ that satisfy:
	\begin{enumerate}
		\item $(R,+)$ is an abelian group (associativity, identity, inverse and commutativity);
		\item $(R,\cdot)$ is a monoid (associativity and identity);
		\item For all $a,b,c\in R$ \begin{align*}
			      a\cdot (b+c) & = (a\cdot b) + (a\cdot c), \\
			      (a+b)\cdot c & = (a\cdot c) + (b\cdot c).
		      \end{align*}
	\end{enumerate}
	A ring in which multiplication is commutative is called a \emph{commutative ring}.
\end{definition}

\begin{definition}
	A \emph{field} is a non-zero commutative ring $F$ in which every non-zero element
	$a\in F$ has a multiplicative inverse $\inv a\in F$.
\end{definition}

\begin{proposition}[Proposition 3.1.11]
	Let $m\in\N$. The commutative ring $\Z/m\Z$ is a field iff $m$ is prime.
\end{proposition}

\subsection{Properties of rings}

\begin{lemma}[Lemma 3.2.1]
	Let $(R,+,\cdot)$ be a ring and let $a,b\in R$. Then
	\begin{enumerate}
		\item $0a=0=a0$.
		\item $(-a)b=-(ab)=a(-b)$.
		\item $(-a)(-b) = ab$.
	\end{enumerate}
\end{lemma}

\begin{definition}
	Let $m\in\Z$. The \emph{$m$-th multiple $ma$ of an element $a$} in an abelian
	group $R$ is
	\begin{align*}
		ma = \underbrace{a + \cdots + a}_\text{$m$ terms} \hs \text{if } m >0,
	\end{align*}
	$0a=0$, and negative multiples are defined by $(-m)a=-(ma)$.
\end{definition}

\begin{lemma}[Lemma 3.2.4]
	Let $R$ be a ring, let $a,b\in R$ and let $m,n\in\Z$. Then:
	\begin{enumerate}
		\item $m(a+b)=ma+mb$;
		\item $(m+n)a=ma+na$;
		\item $m(na)=(mn)a$;
		\item $m(ab)=(ma)b=a(mb)$;
		\item $(ma)(nb)=(mn)(ab)$.
	\end{enumerate}
\end{lemma}

\begin{definition}
	Let $R$ be a ring. An element $a\in R$ is a \emph{unit} iff it has a
	multiplicative inverse in $R$.
\end{definition}

\begin{proposition}
	Thet set $R^\times$ of units in a ring $R$ forms a group under multiplication.
\end{proposition}

\begin{definition}
	In a ring $R$ a non-zero element $a$ is called a \emph{zero-divisor} iff
	there exists a non-zero element $b$ such that either $ab=0$ or $ba=0$.
\end{definition}

\begin{definition}
	An \emph{integral domain} is a non-zero commutative ring that has no zero-divisors.
\end{definition}

\begin{proposition}[Proposition 3.2.15]
	Let $R$ be an integral domain and let $a,b,c\in R$. If $ab=ac$ and $a\not=0$
	then $b=c$.
\end{proposition}

\begin{proposition}[Proposition 3.2.16]
	Let $m\in\N$. Then $\Z/m\Z$ is an integral domain iff $m$ is prime.
\end{proposition}

\begin{theorem}[Theorem 3.2.17]
	Every finite integral domain is a field.
\end{theorem}

\subsection{Polynomials}

\begin{definition}
	Let $R$ be a ring. A \emph{polynomial over $R$} is an expression of the form
	\begin{align*}
		P=a_0+a_1+X+\cdots+a_mX^m
	\end{align*}
	for some non-negative integer $m$ and elements $a_i\in R$ for $i=0,...,m$.
	The set of all polynomials over $R$ is denoted by $R[X]$. In case $a_m$
	is non-zero, the polynomial $P$ has \emph{degree} $m$, written $\deg P$,
	and $a_m$ is its \emph{leading coefficient}. When the leading coefficient is
	$1$ the polynomial is called \emph{monic}.
\end{definition}

\begin{definition}
	The set $R[X]$ is called the \emph{ring of polynomials with coefficients
		in $R$}. The zero and the identity of $R[X]$ are the zero and the identity of
	$R$, respectively.
\end{definition}

\begin{lemma}
	\begin{enumerate}
		\item If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors
		      and $\deg(PQ)=\deg P + \deg Q$ for non-zero $P,Q\in R[X]$.
		\item If $R$ is an integral domain then so is $R[X]$.
	\end{enumerate}
\end{lemma}

\begin{theorem}[Theorem 3.3.4]
	Let $R$ be an integral domain and let $P,Q\in R[X]$ with $Q$ monic. Then there exists
	a unique $A,B\in R[X]$ such that $P=AQ+B$ and $\deg B<\deg Q$ or $B=0$.
\end{theorem}

\begin{definition}
	Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$
	can be \emph{evaluated} at the element $\lambda\in R$ to produce $P(\lambda)$
	by replacing the powers of $X$ in $P$ by the corresponding powers of $\lambda$.
	In this way we havea mapping
	\begin{align*}
		R[X] \to \text{Maps}(R,R).
	\end{align*}
	An element $\lambda\in R$ is a \emph{root} of $P$ iff $P(\lambda) = 0$.
\end{definition}

\begin{proposition}[Proposition 3.3.9]
	Let $R$ be a commutative ring, let $\lambda\in R$ and $P(X)\in R[X]$. Then $\lambda$
	is a root of $P(X)$ iff $(X-\lambda)$ divides $P(X)$.
\end{proposition}

\begin{theorem}[Theorem 3.3.10]
	Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial
	$P\in R[X]\setminus\{0\}$ has at most $\deg P$ roots in $R$.
\end{theorem}

\begin{definition}
	A field $F$ is \emph{algebraically closed} iff each non-constant polynomial $P\in F[X]\setminus F$
	with coefficients in $F$ has a root in $F$.
\end{definition}

\begin{theorem}[Fundamental Theorem of Algebra]
	The field of complex numbers, $\C$, is algebraically closed.
\end{theorem}

\begin{theorem}[Theorem 3.3.14]
	If $F$ is an algebraically closed field, then every non-zero polynomial
	$P\in F[X]\setminus\{0\}$ \emph{decomposes into linear factors}
	\begin{align*}
		P=c(X-\lambda_1)\cdots(X-\lambda_n)
	\end{align*}
	with $n\geq 0$, $c\in F^\times$ and $\lambda_1,...,\lambda_n\in F$. This
	decomposition is unique up to reordering the factors.
\end{theorem}

\subsection{Homomorphisms, ideals and subrings}

\begin{definition}
	Let $R$ and $S$ be rings. A mapping $f:R\to S$ is a \emph{ring homomorphism} iff
	the following hold for all $x,y\in R$:
	\begin{align*}
		f(x+y) & = f(x) + f(y), \\
		f(xy)  & = f(x)f(y).
	\end{align*}
\end{definition}

\begin{lemma}[Lemma 3.4.5]
	Let $R$ and $S$ be rings and $f:R\to S$ a ring homomorphism. Then for all $x,y\in R$
	and $m\in\Z$:
	\begin{enumerate}
		\item $f(0_R)=0_S$;
		\item $f(-x)=-f(x)$;
		\item $f(x-y)=f(x)-f(y)$;
		\item $f(mx)=mf(x)$,
	\end{enumerate}
	where $mx$ denotes the $m$-th multiple of $x$.
\end{lemma}

\begin{definition}
	A subset $I$ of a ring $R$ is an \emph{ideal}, written $I\trianglelefteq R$, iff
	the following hold:
	\begin{enumerate}
		\item $I\not=\emptyset$;
		\item $I$ is closed under subtraction;
		\item for all $i\in I$ and $r\in R$ we have $ri,ir\in I$.
	\end{enumerate}
\end{definition}

\begin{definition}
	Let $R$ be a commutative ring and let $T\subset R$. Then the \emph{ideal of $R$
		generated by $T$} is the set
	\begin{align*}
		_R\lra{T} = \{r_1t_1+\cdots+r_mt_m : t_1,...,t_m\in T,r_1,...,r_m\in R\},
	\end{align*}
	together with the zero element in case $T=\emptyset$. If $T$ is finite we will
	abuse notation by writing $_R\lra{t_1,...,t_n}$.
\end{definition}

\begin{proposition}[Proposition 3.4.14]
	Let $R$ be a commutative ring and let $T\subseteq R$. Then $_R\lra{T}$ is the
	smallest ideal of $R$ that contains $T$.
\end{proposition}

\begin{definition}
	Let $R$ be a commutative ring. An ideal $I$ of $R$ is called \emph{principal} iff
	$I=\lra{t}$ for some $t\in R$.
\end{definition}

\begin{definition}
	Let $R$ and $S$ be rings with zero elements $0_R$ and $0_S$ respectively and
	let $f:R\to S$ be a ring homomorphism. Since $f$ is in particular a group homomorphism
	from $(R,+)$ to $(S,+)$, the \emph{kernel} of $f$ already has a meaning
	\begin{align*}
		\ker f = \{r\in R : f(r) = 0_S\}.
	\end{align*}
\end{definition}

\begin{proposition}[Proposition 3.4.18]
	Let $f:R\to S$ be a ring homomorphism. Then $\ker f$ is an ideal of $R$.
\end{proposition}

\begin{lemma}[Lemma 3.4.20]
	$f$ is injective iff $\ker f = \{0\}$.
\end{lemma}

\begin{lemma}[Lemma 3.4.21]
	The intersection of any collection of ideals of a ring $R$ is an ideal of $R$.
\end{lemma}

\begin{lemma}[Lemma 3.4.22]
	Let $I$ and $J$ be ideals of a ring $R$. Then
	\begin{align*}
		I+J=\{a+b:a\in I, b\in J\}
	\end{align*}
	is an ideal of $R$.
\end{lemma}

\begin{definition}
	Let $R$ be a ring. A subset $R'$ of $R$ is a subring of $R$ iff $R'$ itself
	is a ring under the operations of addition and multiplication defined in $R$.
\end{definition}

\begin{proposition}[Proposition 3.4.26]
	Let $R'$ be a subset of a ring $R$. Then $R'$ is a subring iff
	\begin{enumerate}
		\item $R'$ has a multiplicative identity, and
		\item $R'$ is closed under subtraction, and
		\item $R'$ is closed under multiplication.
	\end{enumerate}
\end{proposition}

\begin{proposition}[Proposition 3.4.28]
	Let $R$ and $S$ be rings and $f:R\to S$ a ring homomorphism.
	\begin{enumerate}
		\item If $R'$ is a subring of $R$ then $f(R')$ is a subring of $S$. In particular
		      $\im f$ is a subring of $S$.
		\item Assume that $f(1_R)=1_S$. Then if $x$ is a unit in $R$, $f(x)$ is a unit
		      in $S$ and $\inv{(f(x))} = f(\inv x)$.
	\end{enumerate}
\end{proposition}

\subsection{Equivalence relations}

\begin{definition}
	A \emph{relation $R$} on a set $X$ is a subset $R\subseteq X\times X$. In this context we
	may write $xRy$ instead of $(x,y)\in R$. Then $R$ is an \emph{equivalence equation on $X$}
	iff it is reflexive, symmetric and transitive.
\end{definition}
\begin{definition}
	Let $\sim$ be an equivalence relation on a set $X$. For $x\in X$ the set $\eqc{x}:=\{z\in X:z\sim x\}$
	is called the \emph{equivalence class of $x$}. A subset $E\subseteq X$ is called an equivalence
	class for $\sim$ iff there is an $x\in X$ such that $E=E(x)$. A subset $Z\subseteq X$ containing
	precisely one element from each equivalence class is called a \emph{system of representatives}
	for the equivalence relation.
\end{definition}

\begin{definition}
	Given an equivalence relation $\sim$ on the set $X$ the \emph{set of equivalence classes},
	which is a subset of the power set $\mathcal{P}(X)$ by
	\begin{align*}
		(X/\sim) := \{\eqc x : x \in X\}.
	\end{align*}
	There is a canoncial surjective mapping $\eqc{-}:X\to (X/\sim),x\mapsto\eqc x$.
\end{definition}

\begin{definition}
	A map $g:(X/\sim)\to Z$ is \emph{well-defined} iff there exists a mapping $f:X\to Z$
	such that $f$ has the property $x\sim y\Leftrightarrow f(x)=f(y)$ and $f=g\circ\eqc-$.
\end{definition}

\subsection{Factor rings}

\begin{definition}
	Let $I$ be an ideal in a ring $R$. The set
	\begin{align*}
		x + I := \{x + i : i\in I\}\subset R
	\end{align*}
	is a \emph{coset of $I$ in $R$}.
\end{definition}

\begin{definition}
	Let $I$ be an ideal in a ring $R$ and $\sim$ the equivalence relation defined by
	$x\sim y\Leftrightarrow x-y\in I$. Then $R/I$, the \emph{factor ring of $R$ by $I$},
	is the set $(R/\sim)$.
\end{definition}

\begin{theorem}[Theorem 3.6.4]
	Let $I$ be an ideal in a ring $R$. Define $+':R/I\times R/I\to R/I$ by
	\begin{align*}
		(x+I)+'(y+I) = (x+y) + I
	\end{align*}
	and $\times' : R/I\times R/I\to R/I$ by
	\begin{align*}
		(x+I)\times' (y+I) = xy + I
	\end{align*}
	for all $x,y\in R$. Then $(R/I,+',\times')$ is itself a ring.
\end{theorem}

\begin{theorem}[Theorem 3.6.7]
	Let $I$ be an ideal in a ring $R$.
	\begin{enumerate}
		\item The mapping $\eqc -$ is a surjective ring homomorphism with kernel $I$.
		\item If $f:R\to S$ is a ring homomorphism so that $I\subseteq\ker f$, then there
		      is a unique ring homomorphism $\bar f:R/I\to S$ such that $f=\bar f\circ\eqc -$.
	\end{enumerate}
\end{theorem}

\begin{theorem}[First Isormorphism Theorem for Rings; Theorem 3.6.9]
	Let $R$ and $S$ be rings. Then every ring homomorphism $f:R\to S$ induces a ring isomorphism
	\begin{align*}
		\bar f:R/\ker f \tilde\to \im f.
	\end{align*}
\end{theorem}

\subsection{Modules}

\begin{definition}
	A \emph{(left) module $M$ over a ring $R$} or \emph{an $R$-module} is a pair consisting of an
	abelian group $M=(M,+_M)$ and a mapping
	\begin{align*}
		R\times M & \to M      \\
		(r,a)     & \mapsto ra
	\end{align*}
	such that for all $r,s\in R$ and $a,b\in M$ the following identities hold:
	\begin{align*}
		r(a +_M b) & = (ra) +_M (rb), \\
		(r+_Rs)a   & = (ra) +_M (sa), \\
		r(sa)      & = (rs)a,         \\
		1_Ra       & = a.
	\end{align*}
\end{definition}

\begin{lemma}[Lemma 3.7.8]
	Let $M$ be an $R$-module, $r\in R$ and $a\in M$.
	\begin{enumerate}
		\item $0_Ra=0_M$.
		\item $r0_M=0_M$.
		\item $(-r)a=r(-a)=-(ra)$.
	\end{enumerate}
\end{lemma}

\begin{definition}
	Let $M,N$ be $R$-modules. A mapping $f:M\to N$ is an \emph{$R$-homomorphism} iff the following
	hold for all $a,b\in M$ and $r\in R$:
	\begin{align*}
		f(a+b) & = f(a) + f(b) \\
		f(ra)  & = rf(a)
	\end{align*}
	The usual definitions of kernel, image and isomorphism apply.
\end{definition}

\begin{definition}
	A non-empty subset $M'$ of an $R$-module $M$ is a \emph{submodule} iff $M'$ is an
	$R$-module with respect to the operations of $M$ restricted to $M'$.
\end{definition}

\begin{lemma}[Lemma 3.7.21]
	Let $f:M\to N$ be an $R$-homomorphism. Then $\ker f$ is a submodule of $M$ and $\im f$
	is a submodule of $N$.
\end{lemma}

\begin{lemma}[Lemma 3.7.22]
	Let $R$ be a ring, let $M$ and $N$ be $R$-modules and let $f:M\to N$ be an $R$-homomorphism.
	Then $f$ is injective iff $\ker f =\{0_M\}$.
\end{lemma}

\begin{definition}
	Let $M$ be an $R$-module and let $T\subseteq M$. Then the \emph{submodule of $M$ generated by $T$}
	is the set
	\begin{align*}
		_R\lra T = \{r_1t_1+\cdots+r_mt_m:t_1,...,t_m\in T,r_1,...,r_m\in R\},
	\end{align*}
	together with the zero element in the case $T=\emptyset$. The module $M$ is \emph{finitely generated}
	iff it is generated by a finite set. It is called \emph{cyclic} iff it is generated by a singleton.
\end{definition}

\begin{lemma}[Lemma 3.7.28-30]
	Let $M$ an $R$-module.
	\begin{itemize}
		\item Let $T\subseteq M$. Then $_R\lra T$ is the smallest submodule of $M$ that contains $T$.
		\item The intersection of any collection of submodules of $M$ is a submodule of $M$.
		\item Let $M_1$ and $M_2$ be submodules of $M$. Then $M_1+M_2$ is a submodule of $M$.
	\end{itemize}
\end{lemma}

\begin{definition}
	Let $M$ an $R$-module and $N$ a submodule of $M$. For each $a\in M$ the \emph{coset of $a$ with
		respect to $N$ in $M$} is
	\begin{align*}
		a+N=\{a+b:b\in N\}.
	\end{align*}
	Let
	The \emph{factor of $M$ by $N$}, denoted $M/N$, is the set of all cosets of $N$ in $M$.
\end{definition}

\begin{theorem}[Theorem-Definition 3.7.31]
	Let $M$ an $R$-module and $N$ a submodule of $M$. Define $+',\times'$ as follows:
	\begin{align*}
		(a+N)+'(b+N) & = (a+b)+N \\
		r(a+N)       & = ra + N
	\end{align*}
	for all $a,b\in M,r\in R$. Then $(M/N,+',\times')$ is itself an $R$-module.
\end{theorem}

\begin{lemma}[Exercise 66]
	Let $V$ and $F$-vector space and $W\subseteq V$ a subspace of $V$. The quotient
	$V/W$ is an $F$-vector space, called the \emph{quotient vector space}.

	If $\dim V<\infty$, then
	\begin{align*}
		\dim (V/W) = \dim V - \dim W.
	\end{align*}
\end{lemma}

\begin{theorem}[Theorem 3.7.32]
	Let $L,M$ be $R$-modules and $N$ a submodule of $M$. Then
	\begin{enumerate}
		\item The mapping $\eqc - : M\to M/N$ is a surjective $R$-homomorphism with kernel $N$.
		\item If $f:M\to L$ is a $R$-homomorphism so that $N\subseteq\ker f$, then there
		      is a unique ring homomorphism $\bar f:M/N\to L$ such that $f=\bar f\circ\eqc -$.
	\end{enumerate}
\end{theorem}

\begin{theorem}[First Isormorphism Theorem for Modules; Theorem 3.6.9]
	Let $M$ and $N$ be $R$-modules. Then every $R$-homomorphism $f:M\to N$ induces an $R$-isomorphism
	\begin{align*}
		\bar f:M/\ker f \tilde\to \im f.
	\end{align*}
\end{theorem}

\section{Determinants and Eigenvalues}

\subsection{Permutations}

\begin{definition}
	The \emph{$n$-th symmetric group} is the group of bijections from the set $\{1,...,n\}$ to itself
	and denoted by $S_n$. It is a group under composition and has $n!$ elements.
\end{definition}

\begin{definition}
	The \emph{length} $\len:S_n\to\N$ of a permutation $\sigma\in S_n$ is given by
	\begin{align*}
		\len \sigma = \abs{\{(i,j):i<j,\sigma(i)>\sigma(j)\}}.
	\end{align*}
	The \emph{sign} $\sgn:S_n\to\{-1,1\}$ of a permutation $\sigma\in S_n$ is then
	given by
	\begin{align*}
		\sgn \sigma = (-1)^{\len \sigma}
	\end{align*}
\end{definition}

\begin{lemma}[Lemma 4.1.5]
	Let $n\in\N$. Then $\sgn:S_n\to\{-1,1\}$ is a group homomorphism. In particular, for all $\sigma,\tau\in S_n$,
	\begin{align*}
		\sgn(\sigma\tau) = (\sgn\sigma)(\sgn\tau).
	\end{align*}
\end{lemma}

\begin{definition}
	Let $n\in\N$. Then the set of even permutations $A_n\subseteq S_n$ is the kernel of the
	group homomorphism $\sgn$. In particular, it is a subgroup of $S_n$ called the \emph{alternating
		group}.
\end{definition}

\subsection{Determinants}

\begin{definition}
	Let $R$ be a commutative ring and $n\in\N$. The \emph{determinant} is a mapping
	$\det:\Mat(n;R)\to R$ given by
	\begin{align*}
		\det((a_{ij}))=\sum_{\sigma\in S_n}\sgn(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}.
	\end{align*}
\end{definition}

\begin{definition}
	Let $V_1,...,V_n,W$ be $F$-vector spaces. A mapping $H:V_1\times\cdots V_n\to W$
	is \emph{multilinear} iff it is linear in each of its arguments. A multilinear form
	$H:V\times \cdots V\to W$ is \emph{alternating} iff it vanishes on every $n$-tuple of
	elements that has at least two entries equal.
\end{definition}

\begin{theorem}[Theorem 4.3.6]
	Let $F$ be a field. The mapping
	\begin{align*}
		\det : \Mat(n; F) \to F
	\end{align*}
	is the unique alternating bilinear form on $n$-tuples of column vectors with values in $F$
	that takes the value $1_F$ on the identity matrix.
\end{theorem}

\begin{theorem}[Theorem 4.4.1]
	Let $R$ be a commutative ring and let $A,B\in\Mat(n; R)$. Then
	\begin{align*}
		\det (AB) = (\det A) (\det B).
	\end{align*}
\end{theorem}

\begin{theorem}[Theorem 4.4.2]
	A matrix $A$ is invertible iff $\det A \not= 0$.
\end{theorem}

\begin{corollary}
	Let $A,B$ be square matrices with $A$ invertible. Then
	\begin{align*}
		\det (\inv A) = \inv{(\det A)} \hs\text{and}\hs
		\det (\inv ABA) = \det B.
	\end{align*}
\end{corollary}

\begin{lemma}[Lemma 4.4.4]
	Let $A\in\Mat(n;R)$ with $R$ commutative. Then
	\begin{align*}
		\det A^T = \det A.
	\end{align*}
\end{lemma}

\begin{definition}
	Let $A\in\Mat(n;R)$ with $R$ commutative and $1\leq i,j\leq n$.  Then the \emph{$(i,j)$ cofactor of $A$} is
	\begin{align*}
		C_{ij}=(-1)^{i+j}\det(A\lra{i,j})
	\end{align*}
	where $A\lra{i,j}$ is the matrix obtained from $A$ by deleting the $i$-th row and the $j$-th column.
\end{definition}

\begin{theorem}[Theorem 4.4.7]
	Let $A=(a_{ij})\in\Mat(n; R)$ with $R$ commutative. For a fixed $i$ the $i$-th row expansion
	of the determinant is
	\begin{align*}
		\det A = \sum_{j=1}^n a_{ij}C_{ij}
	\end{align*}
	and for a fixed $j$ the $j$-th column expansion of the determinant is
	\begin{align*}
		\det A = \sum_{i=1}^n a_{ij}C_{ij}.
	\end{align*}
\end{theorem}

\begin{definition}
	Let $A\in\Mat(R;n)$ with $R$ commutative. The \emph{adjugate matrix $\adj A$} is the
	$(n\times n)$ matrix whose entries are $(\adj A)_{ij} = C_{ji}$.
\end{definition}

\begin{theorem}[Cramer's rule; Theorem 4.4.9]
	Let $A\in\Mat(R;n)$ with $R$ commutative. Then
	\begin{align*}
		A(\adj A) = (\det A)I_n.
	\end{align*}
\end{theorem}

\begin{corollary}[Corollary 4.4.11]
	Let $A\in\Mat(n;R)$ with $R$ commutative. Then $A$ is invertible iff $\det A\in R^\times$.
\end{corollary}

\begin{theorem}[Jacobi's formula; Theorem 4.4.14]
	Let $A=(a_{ij})$ where $a_{ij}:\R\to\R$. Then
	\begin{align*}
		\frac{d}{dt}\det A = \tr(\adj A)\frac{dA}{dt}.
	\end{align*}
\end{theorem}

\subsection{Eigenvalues and Eigenvectors}

\begin{definition}
	Let $f:V\to V$ be an endomorphism of an $F$-vector space $V$. A scalar $\lambda\in F$ is
	an \emph{eigenvalue of $f$} iff there exists a non-zero vector $\vec v \in V$ such that
	$f(\vec v)=\lambda \vec v$. Each such vector is called an \emph{eigenvector of $f$ with
		eigenvalue $\lambda$}. For any $\lambda\in F$, the \emph{eigenspace of $f$ with eigenvalue $\lambda$}
	is
	\begin{align*}
		E(\lambda,f) = \{\vec v\in V:f(\vec v)=\lambda\vec v\}.
	\end{align*}
\end{definition}

\begin{definition}
	Let $R$ be a commutative ring and let $A\in\Mat(n; R)$ be a square matrix with entries in $R$.
	The polynomial $\det(xI_n-A)\in R[x]$ is called the \emph{characteristic polynomial of the matrix
		$A$}. It is denoted $\chi_(x)$.
\end{definition}

\begin{theorem}[Theorem 4.5.4]
	Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field
	has an eigenvalue.
\end{theorem}

\begin{theorem}[Theorem 4.5.8]
	Let $F$ be a field and $A\in\Mat(n;F)$ a square matrix with entries in $F$. The eigenvalues of the
	linear mapping $A:F^n\to F^n$ are exactly the roots of the characteristic polynomial $\chi_A$.
\end{theorem}

\begin{lemma}[Lemma, Remark 4.5.9]
	Let $A,B\in\Mat(n; R)$ be conjugate, i.e. there exists an invertible $P\in\Mat(n; R)$ such that
	$A=PB\inv P$. Then $\chi_A(x)=\chi_B(x)$.
\end{lemma}

\subsection{Cayley-Hamilton theorem}

\begin{proposition}
	Let $f:V\to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. The following
	statements are equivalent:
	\begin{enumerate}
		\item The vector space $V$ hs an ordered basis $B=(\vec v_1,...,\vec v_n)$ such that the matrix
		      $\repr{B}{f}{B}$ is upper triangular.
		\item The characteristic polynomial $\chi_f(x)$ of $f$ decomposes into linear factors in $F[x]$.
	\end{enumerate}
\end{proposition}

\begin{definition}
	An endomorphism $f:V\to V$ of an $F$-vector space $V$ is \emph{diagonalisable} iff there exists
	a basis of $V$ consisting of eigenvectors of $f$.

	A square matrix $A\in\Mat(n;F)$ is \emph{diagonalisable} iff the corresponding linear mapping
	given by left multiplication by $A$ is.
\end{definition}

\begin{theorem}
	A square matrix $A\in\Mat(n;F)$ is diagonalisable iff it is conjugate to a diagonal
	matrix, i.e. there exists an invertible $P\in\Mat(n;F)$ such that
	$\inv PAP=\text{diag}(\lambda_1,...,\lambda_n)$.
\end{theorem}

\begin{lemma}[Lemma 4.6.9]
	Let $f:V\to V$ be an endomorphism of a vector space $V$ and let $\vec v_1,...,\vec v_n$ be
	eigenvectors of $f$ with pairwise different eigenvalues $\lambda_1,...,\lambda_n$. Then
	the vectors $\vec v_1,...,\vec v_n$ are linearly independent.
\end{lemma}

\begin{theorem}[Cayley-Hamilton]
	Let $A\in\Mat(n;R)$ be a square matrix with entries in a commutative ring $R$. Then evaluating the
	characteristic polynomial $\chi_A(x)\in R[x]$ at the matrix $A$ results in zero.
\end{theorem}

\subsection{Markov matrices}

\begin{definition}
	A matrix $M$ whose entries are non-negative and such that the sum of the entries of each
	column equals $1$ is a \emph{Markov matrix}.
\end{definition}

\begin{lemma}[Lemma 4.7.6]
	Suppose that $M\in\Mat(n;\R)$ is a Markov matrix. Then $\lambda=1$ is an eigenvalue of $M$.
\end{lemma}

\begin{theorem}[Perron]
	If $M\in\Mat(n;\R)$ is a Markov matrix all of whose entries are positive, then the eigenspace
	$E(1,M)$ is one dimensional. There exists a unique basis vector $\vec v\in E(1,M)$ all of whose
	entries are positive real numbers, $v_i>0$ for all $i$, and such that the sum of its entries is $1$,
	$\sum_{i=1}^n v_i=1$.
\end{theorem}

\section{Inner product spaces}

\subsection{Definition of an inner product space}

\begin{definition}
	Let $V$ be an $\R$-vector space. An \emph{inner product} on $V$ is positive definite symmetric bilinear
	form
	\begin{align*}
		\lra{-,-}:V\times V\to\R.
	\end{align*}
	I.e. it satisfies the following for all $\vec x,\vec y,\vec z\in V$ and $\lambda,\mu\in\R$:
	\begin{enumerate}
		\item $\lra{\lambda\vec x+\mu\vec y,\vec z}=\lambda\lra{\vec x,\vec z}+\mu\lra{\vec y,\vec z}$,
		\item $\lra{\vec x,\vec y}=\lra{\vec y,\vec x}$,
		\item $\lra{\vec x,\vec x}\geq 0$, with equality iff $\vec x =0$.
	\end{enumerate}
	The pair $(V,\lra{-,-})$ is called a \emph{real inner product space}.
\end{definition}

\begin{definition}
	Let $V$ be a $\C$-vector space. An \emph{inner product} on $V$ is positive definite sesquilinear
	form
	\begin{align*}
		\lra{-,-}:V\times V\to\C.
	\end{align*}
	I.e. it satisfies the following for all $\vec x,\vec y,\vec z\in V$ and $\lambda,\mu\in\R$:
	\begin{enumerate}
		\item $\lra{\lambda\vec x+\mu\vec y,\vec z}=\lambda\lra{\vec x,\vec z}+\mu\lra{\vec y,\vec z}$,
		\item $\lra{\vec x,\vec y}=\overline{\lra{\vec y,\vec x}}$,
		\item $\lra{\vec x,\vec x}\geq 0$, with equality iff $\vec x =0$.
	\end{enumerate}
	The pair $(V,\lra{-,-})$ is called a \emph{complex inner product space}.
\end{definition}

\begin{definition}
	In a real or complex inner product space $V$ the \emph{norm} $\vabs{\vec v}\in\R$ of a vector $v\in V$
	is defined by
	\begin{align*}
		\vabs{\vec v}=\sqrt{\lra{\vec v, \vec v}}.
	\end{align*}
	Whenever $\vabs{\vec v}=1$ we call $\vec v\in V$ a \emph{unit}. Two vectors $\vec v,\vec w\in V$
	are \emph{orthogonal}, written $\vec v\perp \vec w$, iff $\lra{\vec v,\vec w}=0$.
\end{definition}

\begin{definition}
	Let $V$ an inner product space. A family $(\vec v_i\in V)_{i\in I}$ is an \emph{orthonormal family}
	iff $\lra{\vec v_i,\vec v_j}=\delta_{ij}$. An orthonormal family that is a basis is an
	\emph{orthonormal basis}.
\end{definition}

\begin{theorem}[Theorem 5.1.10]
	Every finite dimensional inner product space has an orthonormal basis.
\end{theorem}

\subsection{Orthogonal complements and orthogonal projections}

\begin{definition}
	Let $V$ be an  inner product space and let $T\subseteq V$ be an arbitrary subset. Define
	the \emph{orthogonal} to $T$ by
	\begin{align*}
		T^\bot = \{\vec v \in V : \vec v \perp \vec t \text{ for all }\vec t \in T\}.
	\end{align*}
\end{definition}

\begin{proposition}[Proposition 5.2.2]
	Let $V$ be an inner product space and let $U$ be a finite dimensional subspace $U\subseteq V$.
	Then $U$ and $U^\bot$ are complementary, i.e.
	\begin{align*}
		V = U\oplus U^\bot.
	\end{align*}
\end{proposition}

\begin{definition}
	Let $U\subseteq V$ be a finite dimensional subspace of an inner product space $V$. The space $U^\bot$
	is the \emph{orthogonal complement to $U$}. The \emph{orthogonal projection from $V$ to $U$} is
	the mapping
	\begin{align*}
		\pi_U:V\to V
	\end{align*}
	that sends $\vec v = \vec p + \vec r$ to $\vec p$ where $\vec v\in V$, $\vec p\in U$, and $\vec r\in U^\bot$.
\end{definition}

\begin{proposition}[Proposition 5.2.4]
	Let $U$ be a finite dimensional subspace of an inner product space $V$ and let $\pi_U$ be the
	orthogonal projection from $V$ onto $U$.
	\begin{enumerate}
		\item $\pi_U$ is a linear mapping with $\im \pi_U=U$ and kernel $\ker \pi_U = U^\bot$.
		\item If $\{\vec v_1,...,\vec v_n\}$ is an orthonormal basis of $U$, then $\pi_U$ is given
		      by
		      \begin{align*}
			      \pi_U(\vec v)=\sum_{i=1}^n\lra{\vec v,\vec v_i}\vec v_i,
		      \end{align*}
		      for all $\vec v\in V$.
		\item $\pi_U^2=\pi_U$, i.e. $\pi_U$ is an idempotent.
	\end{enumerate}
\end{proposition}

\begin{theorem}[Cauchy-Schwarz Inequality]
	Let $V$ an inner product space and $\vec v,\vec w\in V$ . Then
	\begin{align*}
		\abs{\lra{v,w}}\leq \vabs{\vec v}\vabs{\vec w}
	\end{align*}
	with equality iff $\vec v$ and $\vec w$ are linearly dependent.
\end{theorem}

\begin{theorem}[Theorem 5.2.7]
	Let $\vec v_1,...,\vec v_k$ be linearly independent vectors in an inner product space $V$. Then
	there exists an orthonormal family $\vec w_1,...,\vec w_k$ such that, for all $1\leq i\leq k$,
	\begin{align*}
		\vec w_i\in \R_{>0}\vec v_i + \lra{\vec v_1,...,\vec v_{i-1}}.
	\end{align*}
	\begin{proof}
		Decompose $\vec v_i=\vec p_i+\vec r_i$ where $\vec p_i$ is the orthogonal projection of $\vec v_i$
		onto the subspace $\lra{\vec v_1,...,\vec v_{i-1}}$ and where $\vec r_i$ belongs to the orthogonal
		complement of this subspace. The vectors $\vec w_i=\vec r_i/\vabs{\vec r_i}$ are then an orthogonal
		family with the displayed property.
	\end{proof}
\end{theorem}

\subsection{Adjoints and self-adjoints}

\begin{definition}
	Let $V$ be an inner product space. Then two endomorphisms $T,S:V\to V$ are \emph{adjoint}
	to one another iff the follwing holds, for all $\vec v, \vec w\in V$,
	\begin{align*}
		\lra{T\vec v,\vec w}=\lra{\vec v, S\vec w}.
	\end{align*}
	We write $S=T^*$ and call $S$ the \emph{adjoint} of $T$.
\end{definition}

\begin{theorem}[Theorem 5.3.4]
	Let $V$ be a finite dimensional inner product space. Let $T:V\to V$ be an endomorphism. Then $T^*$
	exists. That is, there exists a unique linear mapping $T^*:V\to V$ such that, for all $\vec v,\vec w\in V$,
	\begin{align*}
		\lra{T\vec v,\vec w}=\lra{\vec v, T^*\vec w}.
	\end{align*}
\end{theorem}

\begin{definition}
	An endomorphism of an inner product space $T:V\to V$ is \emph{self-adjoint} iff $T=T^*$.
\end{definition}

\begin{theorem}[Theorem 5.3.7]
	Let $T:V\to V$ be a self-adjoint linear mapping on an inner product space $V$.
	\begin{enumerate}
		\item Every eigenvalue of $T$ is real.
		\item If $\lambda$ and $\mu$ are distinct eigenvalues of $T$ with corresponding eigenvectors
		      $\vec v,\vec u$, then $\lra{\vec v,\vec u}=0$.
		\item $T$ has an eigenvalue.
	\end{enumerate}
\end{theorem}

\begin{theorem}[The spectral theorem for self-adjoint endomorphisms]
	Let $V$ be a finite dimensional inner product space and let $T:V\to V$ be a self-adjoint linear
	mapping. Then $V$ has an orthonormal basis consisting of eigenvectors of $T$.
\end{theorem}

\begin{definition}
	An \emph{orthogonal matrix} is an $(n\times n)$-matrix $P$ with real entries such that $\inv P=P^\top$.
\end{definition}

\begin{corollary}[The spectral theorem for real symmetric matrices]
	Let $A$ be a real symmetric $(n\times n)$-matrix. Then there is an $(n\times n)$-orthogonal matrix $P$
	such that
	\begin{align*}
		P^\top AP = \inv PAP = \diag(\lambda_1,...,\lambda_n)
	\end{align*}
	where $\lambda_1,...,\lambda_n$ are the eigenvalues of $A$, repeated according to their multiplicity.
\end{corollary}

\begin{definition}
	A matrix $P\in\Mat(n;\C)$ is \emph{unitary} iff $\inv P=\overline{P}^\top$.
\end{definition}

\section{Jordan Normal Form}

\begin{definition}
	Given $r\in\N$ define $J(r)\in\Mat(n;F)$, called the \emph{nilpotent Jordan block of
		size} $r$, by the rule $J(r)_{ij}=\delta_{i+1,j}$. I.e.
	\begin{align*}
		J(r) = \begin{pmatrix}
			       0      & 1      & 0      & \cdots & 0      & 0      \\
			       0      & 0      & 1      & \cdots & 0      & 0      \\
			       0      & 0      & 0      & \cdots & 0      & 0      \\
			       \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
			       0      & 0      & 0      & \cdots & 0      & 1      \\
			       0      & 0      & 0      & \cdots & 0      & 0
		       \end{pmatrix}
	\end{align*}
	Given $r\in\N$ and a scalar $\lambda\in F$ define $J(r,\lambda)\in\Mat(r; F)$, called the
	\emph{Jordan block of size $r$ and eigenvalue $\lambda$}, by the rule
	\begin{align*}
		J(r,\lambda)=\lambda I_r + J(r) = D + N,
	\end{align*}
	i.e.
	\begin{align*}
		J(r,\lambda) = \begin{pmatrix}
			               \lambda & 1       & 0       & \cdots & 0       & 0       \\
			               0       & \lambda & 1       & \cdots & 0       & 0       \\
			               0       & 0       & \lambda & \cdots & 0       & 0       \\
			               \vdots  & \vdots  & \vdots  & \ddots & \vdots  & \vdots  \\
			               0       & 0       & 0       & \cdots & \lambda & 1       \\
			               0       & 0       & 0       & \cdots & 0       & \lambda
		               \end{pmatrix}.
	\end{align*}
	In particular, $J(1)=(0)$, $\lambda I_r=\diag(\lambda, ..., \lambda)$, and $DN=ND$.
\end{definition}

\begin{lemma}
	Let $V$ an $r$-dimensional $F$-vector space with basis $B=(\vec v_1,...,\vec v_r)$ and
	$\lambda\in F$ a scalar. Then the endomorphism $f:V\to V$ defined by
	\begin{align*}
		f(\vec v_1) & =\lambda \vec v_1,              \\
		f(\vec v_2) & =\vec v_1+\lambda \vec v_2,     \\
		            & \vdots                          \\
		f(\vec v_r) & =\vec v_{r-1}+\lambda \vec v_r.
	\end{align*}
	has matrix $\repr{B}{f}{B}$. The endomorphism $e=f-\lambda 1_V$ has nilpotent matrix
	$\repr{B}{e}{B}=J(r)$. The characteristic polynomial of $f$ is $\chi_f(x)=(x-\lambda)^r\in F[x]$.
	Further, for $j=1,...,r-1$,
	\begin{align*}
		V_j := \ker(e^j)=\lra{\vec v_1,...,\vec v_j}\subset V
	\end{align*}
	is a $j$-dimensional subspace such that $f(V_j)\subseteq V_j$. In particular, the $\lambda$-eigenspace
	$E(\lambda, f)=V_1=\lra{\vec v_1}$ is $1$-dimensional.
\end{lemma}

Let $F$ be an algebraically closed field. Let $V$ be a finite dimensional vector space and let
$\phi:V\to V$ be an endomorphism of $V$ with characteristic polynomial
\begin{align*}
	\chi_\phi(x)=(x-\lambda_1)^{a_1}\cdots (x-\lambda_s)^{a_s}\in F[x], \hs a_i\geq 1,\sum_{i=1}^s a_i=n,
\end{align*}
for distinct $\lambda_1,...,\lambda_s\in F$.

\begin{theorem}[Jordan Normal Form]
	There exists an ordered basis $B$ of $V$ such that
	the matrix of $\phi$ with respect to $B$ is block diagonal with Jordan blocks on the diagonal. I.e.
	\begin{align*}
		\repr{B}{\phi}{B}=\diag(J(r_{11},\lambda_1),...,J(r_{1m_1},\lambda_1),...,J(r_{s1},\lambda_s),...,J(r_{sm_s},\lambda_s))
	\end{align*}
	with $r_{11},...,r_{1m_1},...,r_{s1},...,r_{sm_s}\in\N$ such that
	\begin{align*}
		a_i=\sum_{j=1}^{m_i} r_{i,j} \hs\text{for all }1\leq i\leq s.
	\end{align*}
\end{theorem}

\begin{lemma}[Lemma 6.3.1]
	For $1\leq j\leq s$ define,
	\begin{align*}
		P_j(x)=\prod_{\substack{i=1 \\ i\neq j}}^{a_i}(x-\lambda_i)^{a_i}.
	\end{align*}
	Then there exist polynomials $Q_1(x),...,Q_s(x)\in F[x]$ such that
	\begin{align*}
		\sum_{j=1}^s P_j(x)Q_j(x)=1.
	\end{align*}
\end{lemma}

\begin{definition}
	The \emph{generalised eigenspace} of $\phi$ with eigenvalue $\lambda_i$, $E^\text{gen}(\lambda_i,\phi)$,
	is the following subspace of $V$:
	\begin{align*}
		E^\text{gen}(\lambda_i,\phi)=\{\vec v\in V : (\phi-\lambda_i 1_V)^{a_i}(\vec v)=\vec 0\}.
	\end{align*}
	The dimension of $E^\text{gen}(\lambda_i,\phi)$ is called the \emph{algebraic multiplicity of $\phi$
		with eigenvalue $\lambda_i$} while the dimension of the eigenspace $E(\lambda_i,\phi)$ is called
	the \emph{geometric multiplicity of $\phi$ with eigenvalue $\lambda_i$}.
\end{definition}

\begin{lemma}[Remark 6.3.3]
	\begin{align*}
		E(\lambda_i,\phi)\subseteq E^\text{gen}(\lambda_i,\phi).
	\end{align*}
\end{lemma}

\begin{definition}
	Let $f:X\to X$ be a mapping from a set $X$ to itself. A subset $Y\subseteq X$ is \emph{stable under}
	$f$ precisely when $f(Y)\subseteq Y$.
\end{definition}

\begin{proposition}[Proposition 6.3.5]
	For each $1\leq i\leq s$, let
	\begin{align*}
		B_i=\{\vec v_{ij}\in V : 1\leq j\leq a_i\}
	\end{align*}
	be a basis of $E^\text{gen}(\lambda_i,\phi)$. Then
	\begin{enumerate}
		\item Each $E^\text{gen}(\lambda_i,\phi)$ is stable under $\phi$.
		\item For each $\vec v\ih V$ there exist unique $\vec v_i\in E^\text{gen}(\lambda_i,\phi)$
		      such that $\vec v = \sum_{i=1}^s \vec v_i$. In other words, there is a direct sum decomposition
		      \begin{align*}
			      V = \bigoplus_{i=1}^s E^\text{gen}(\lambda_i,\phi)
		      \end{align*}
		      with $\phi$ restricting to endomorphisms of the summands \begin{align*}
			      \phi_i = \eval{\phi}{E^\text{gen}(\lambda_i,\phi)}:E^\text{gen}(\lambda_i,\phi)\to E^\text{gen}(\lambda_i,\phi).
		      \end{align*}
		\item Then $B=B_1\cup\cdots\cup B_s$ is a basis of $V$. Further, \begin{align*}
			      \repr{B}{\phi}{B}=\begin{pmatrix}
				                        \repr{B_1}{\phi_1}{B_1} & \cdots & 0                       \\
				                        \vdots                  & \ddots & \vdots                  \\
				                        0                       & \cdots & \repr{B_s}{\phi_s}{B_s}
			                        \end{pmatrix}
		      \end{align*}
		      is block diagonal.
	\end{enumerate}
\end{proposition}

\begin{corollary}[Corollary, Exercise 90]
	For each $A\in\Mat(n; F)$, there exist unique $D,N\in\Mat(n; F)$ such that $D$ is diagonalisable,
	$N$ is nilpotent, $DN=ND$, and $A=D+N$. This is called the \emph{Jordan decomposition} of $A$.
\end{corollary}

\begin{lemma}[Lemma 6.3.6]
	Let $W$ be a finite dimensional $F$-vector space and $\psi:W\to W$ an endomorphism such that
	$\psi^{m-1}\neq 0$ and $\psi^m=0$ for some $m$. For $0\leq i\leq m$, define a subsapce
	\begin{align*}
		W_i=\ker(\psi^i)
	\end{align*}
	and a linear mapping
	\begin{align*}
		\psi_i:\frac{W_i}{W_{i-1}}\to \frac{W_{i-1}}{W_{i-2}}
	\end{align*}
	by $\psi_i(\vec w+W_{i-1})=\psi(\vec w)+W_{i-2}$ for $\vec w\in W_i$. Then $\psi_i$ is well-defined
	and injective.
\end{lemma}

\begin{lemma}[Lemma 6.3.7]
	Let $f:X\to Y$ be an injective map between $F$-vector spaces $X$ and $Y$. If $S\subseteq X$
	is linearly independent in $X$, then $f(S)\subseteq Y$ is linearly independent in $Y$.
\end{lemma}

\begin{lemma}[Exercise 91]
	Let $\psi:V\to V$ be a nilpotent endomorphism. Then there exists a basis $B$ of $V$ such that 
	\begin{align*}
		\repr{B}{\psi}{B}=\diag(J(b_1),...,J(b_s))
	\end{align*}
	where $b_1,...,b_s\in\N$ are unique up to reordering.
\end{lemma}

\end{document}
