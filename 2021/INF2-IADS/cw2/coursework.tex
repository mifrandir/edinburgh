\documentclass{article}
\usepackage{homework-preamble}
\mkthms

\title{INF2-IADS: Coursework 2}
\author{S1971811}
\begin{document}
\maketitle

\tableofcontents

\section{Memory recycling}

\subsection{Mark pseudocode}

\begin{pseudo}
	$\text{Mark}(\text{\emph{void}}) \to \text{\emph{void}}$\\+
	$q\leftarrow \text{empty Queue}$\\
	\textbf{for} all stack entries $s$ \textbf{do}\\+
	\textbf{if} $s\not=\textit{null}$ \textbf{and} $s.\text{colour} = \textit{white}$ \textbf{then}\\+
	$s.\text{colour} \leftarrow \textit{black}$\\
	$q$.enqueue$(s)$\\--
	\textbf{while} $\neg(q.\text{isEmpty}())$ \textbf{do}\\+
	$r\leftarrow q.\text{dequeue}()$\\
	\textbf{if} $v.\text{right}.\text{colour} = \textit{white}$ \textbf{then}\\+
	$v.\text{right}.\text{colour}\leftarrow\textit{black}$\\
	$q.\text{enqueue}(v.\text{right})$\\-
	\textbf{if} $v.\text{left}.\text{colour} = \textit{white}$ \textbf{then}\\+
	$v.\text{left}.\text{colour}\leftarrow\textit{black}$\\
	$q.\text{enqueue}(v.\text{left})$\\-
\end{pseudo}

\subsection{Sweep pseudocode}

\begin{pseudo}
	$\text{Sweep}(\textit{void})\to\textit{void}$\\+
	\textbf{for} all heap entries $c$ \textbf{do}\\+
	\textbf{if} occupied$(c)$ \textbf{do}\\+
	\textbf{if} $\text{obj}(c).\text{colour} = \textit{white}$ \textbf{then}\\+
	reclaim$(c)$\\-
	\textbf{else}\\+
	$\text{obj}(c).\text{colour}\leftarrow\textit{white}$
\end{pseudo}

\subsection{Informal asymptotic upper bound for runtime of Mark-Sweep}

Consider the runtime of Mark $T_{Mark}$. Observe that there are two loops
and only one line that gets run exactly once, namely the queue initialisation.
Thus we have
\begin{align*}
	T_{Mark} = T_{for} + T_{while} + O(1)
\end{align*}
Since the body of the first loop runs in for every one of the $m$ stack entries
in $O(1)$ time, we have
\begin{align*}
	T_{for} = O(m)
\end{align*}
and similarly the body of the second loop runs for every single one of
the $n$ heap cells at most once
(we made sure to only insert each node once into the queue and each iteration
removes precisely one item) and thus
\begin{align*}
	T_{while} = O(n)
\end{align*}
so we find
\begin{align*}
	T_{Mark} = O(m) + O(n) + O(1) = O(m + n).
\end{align*}
Now, let's consider the runtime of Sweep $T_{Sweep}$. The entire body gets
executed as often as the single loop that is present. The loop runs for every
heap cell, thus
\begin{align*}
	T_{Sweep} = O(n).
\end{align*}
Finally, we find an upper bound for the total Mark-Sweep runtime $T_{MS}$
\begin{align}
	\label{infhims}
	T_{MS} = T_{Mark} + T_{Sweep} = O(m+n) + O(n) = O(m + n).
\end{align}

\subsection{Informal asymptotic lower bound for runtime of Mark-Sweep}

Let's start with Mark again. Observe that the second loop only runs if
the first loop finds a valid stack entry. This may not happen at all, for
example at the beginning of the program execution. Thus, the time associated
with the second loop cannot be bounded below by anything but a constant value,
i.e.
\begin{align*}
	T_{while} = \Omega(1).
\end{align*}
The for loop checks every stack entry though, regardless of its contents, i.e.
\begin{align*}
	T_{for} = \Omega(n).
\end{align*}
Therefore
\begin{align*}
	T_{Mark} = \Omega(n) + \Omega(1) + \Omega(1) = \Omega(n).
\end{align*}
Now, notice that the same is true for Sweep: The for loop runs for every
heap cell regardless of the state of the program. We have
\begin{align*}
	T_{Sweep} = \Omega(m)
\end{align*}
and then
\begin{align}
	\label{infloms}
	T_{MS} = \Omega(n) + \Omega(m) = \Omega(m+n).
\end{align}
By combining (\ref{infhims}) and (\ref{infloms}) we find
\begin{align*}
	T_{MS} = \Theta(m+n)
\end{align*}
even though no $\Theta$ can be given for the Mark procedure itself.

\subsection{Formal asymptotic upper bound for runtime of Mark-Sweep}

\begin{claim}
	Let $m,n\in\N$ be the number of stack entries and heap cells respectively and
	let $T_{MS}$ be the number of line executions of the Mark-Sweep procedure
	given in 1.1 and 1.2. Then
	\begin{align*}
		T_{MS} = O(m+n).
	\end{align*}
\end{claim}
\begin{proof}
	Let $T_{MS}$ be the number of line executions in a single Mark-Sweep procedure.
	Then we have
	\begin{align}
		\label{forlms}
		T_{MS} = T_{Mark} + T_{Sweep}
	\end{align}
	where $T_{Mark}$ and $T_{Sweep}$ are the number of line executions in Mark and
	Sweep respectively. Let
	\begin{align*}
		k = \max\{m,n\}.
	\end{align*}
	Then consider $T_{Mark}$. As noted in
	1.4 there is only one line outside the loops. The \texttt{for} loop runs once for each
	stack entry and each run leads to at most $4$ lines being executed. Thus
	\begin{align*}
		T_{for} \leq 4m
	\end{align*}
	where $T_{for}$ is the number of line executions associated with the loop. For
	the \texttt{while} loop we observe that each vertex that is being inserted into the
	queue is white right before and then gets recoloured (cf. lines 4-6 and lines 9-14).
	Additionally, no vertex changes colour from black to white (within the Mark procedure).
	Therefore each vertex may only be inserted into the queue once. Since each loop iteration
	removes the head of the queue, the loop can only run at most $n$ times.
	Considering that each iteration leads to at most $7$ line executions, we have
	\begin{align*}
		T_{while} \leq 7n
	\end{align*}
	where $T_{while}$ is the number of line executions associated with this loop.
	Using this we find
	\begin{align*}
		T_{Mark} \leq 4m + 7n + 1.
	\end{align*}
	For the Sweep procedure we observe that
	\begin{align*}
		T_{Sweep} \leq 6n
	\end{align*}
	because there are at most $6$ lines being run for every loop iteration and
	the loop gets run for each of the $n$ heap entries. We can now apply (\ref{forlms})
	to find
	\begin{align}
		\label{tmsbound}
		T_{MS} \leq 4m + 7n + 1 + 6n = 4m + 12n + 1 \leq 18k
	\end{align}
	since $k\geq m,n \geq 1$. Now let $C=18$ and $M=N=1$ be a constants.
	Then for all $m,n$ such that $m\geq M$ or $n\geq N$ we have
	\begin{align*}
		C(m+n) = 18(m+n) \geq 18k \geq T_{MS}
	\end{align*}
	since for all $a,b\geq 0$, $a+b \geq \max\{a,b\}$ and thus $m+n\geq k$.
	This shows, by definition of $O$, that
	\begin{align}
		\label{tmsbound}
		T_{MS} = O(m+n).
	\end{align}
\end{proof}

\subsection{Informal bounds for total and amortised recycling time}

Since the program may never fill the heap completely, memory recylcing may not occur
at all and thus we cannot give a lower bound for the total runtime penalty associated
with it. Therefore we are only concerned with upper bounds.\\
Observe that each program action can only ever populate one additional heap cell.
Therefore, since at the beginning of the execution and after every Mark-Sweep
procedure there are at least $\ceil{n/2}$ empty heap cells, at least $\ceil{n/2}$ program operations
need to occur before every Mark-Sweep run. To find an upper bound, let's assume that
such a run is required precisely every $\ceil{n/2}$ operations. Then throughout the
entire program execution, $p/\ceil{n/2}$ such runs occur. Therefore, the bound
for the total time spent on memory recycling $T_{RP}$ is bounded by

\begin{align*}
	T_{RP} \leq \frac{p\cdot T_{MS}}{\ceil{\frac{n}{2}}} \leq \frac{2p\cdot T_{MS}}{n}.
\end{align*}

Using the result from 1.5 and $m=10$ this shows that
\begin{align*}
	T_{RP} = \frac{2p\cdot O(m+n)}{n} = O(p).
\end{align*}

To find the average per line we divide this by the number of lines to get
\begin{align*}
	T_{RL} = T_{RP}/p = O(p)/p = O(1).
\end{align*}


\subsection{Formal bounds for total and amortised recycling time}

\begin{claim}
	Consider a program consisting of $p$ actions that uses exactly $m=10$
	stack entries, has access to $n$ heap cells and that is guaranteed to only
	have at most $n/2$ heap cells that are reachable through references
	from the stack at any point in time. Then the total time spent on memory recycling $T_{RP}$ is
	bounded by $O(p)$.
\end{claim}
\begin{proof}
	Firstly, note that the number of heap cells reachable through references
	from the stack has to be an integer, call it $r$. Then notice
	\begin{align*}
		r \leq n / 2 \hs\Rightarrow\hs r \leq \floor{n/2}.
	\end{align*}
	In the case $r=\floor{n/2}$ the maximum number of unreachable heap cells
	is bounded above by $n-\floor{n/2}=\ceil{n/2}$.\\
	\indent Further, observe that each program operation may populate at most one
	additional heap cell and after each Mark-Sweep cycle there are no unreachable cells.
	This means that
	$\ceil{n/2}$ further allocations are required to fill the heap entirely and
	thereby initiate the Mark-Sweep procedure. This holds immediately after
	the start of the program too, assuming that heap cells can only become unreachable
	through program operations. Let $c$ be the number of times the Mark-Sweep
	procedure gets invoked. Then we have
	\begin{align*}
		c \leq \frac{p}{\ceil{\frac{n}{2}}} \leq \frac{2p}{n}.
	\end{align*}
	Since $T_{RP}$ is the sum of all Mark-Sweep executions, we have
	\begin{align*}
		T_{RP} \leq\frac{2p \cdot T_{MS}}{n}.
	\end{align*}
	Using (\ref{tmsbound}) we find
	\begin{align}
		\label{trbound}
		T_{RP}\leq\frac{2p\cdot 18k}{n}=\frac{36p\cdot k}{n}
	\end{align}
	where $k=\max\{m,n\}$. Now let $P=1$, $N=10$ and $C=360$ be constants. Then
	for all $p\geq P$ or $n\geq N$ we have
	\begin{align*}
		T_{RP} & \leq \frac{36p\cdot k}{n} \leq \frac{36p\cdot 10n}{n} = 360p = Cp.
	\end{align*}
	By definition of $O$ we now have shown
	\begin{align*}
		T_{RP} = O(p).
	\end{align*}
\end{proof}
\emph{Note: We are using $\forall n\in\N,\:\max\{10,n\}\leq 10n$ which is
	easy enough to prove that I did not want to clutter the submission with a
	rigorous argument. The statement will be used throughout the text.}

\begin{claim}
	Consider a program consisting of $p$ actions that uses exactly $m=10$
	stack entries, has access to $n$ heap cells and that is guaranteed to only
	have at most $\ceil{n/2}$ heap cells that are reachable through references
	from the stack at any point int time.
	Then the average time spent on memory recycling per program
	action $T_{RL}$ is bounded by $O(1)$.
\end{claim}
\begin{proof}
	The average time per operation is the total time divided by the number
	of operations.
	I.e.
	\begin{align*}
		T_{RL} = \frac{T_{RP}}{p}.
	\end{align*}
	We can use (\ref{trbound}) to obtain
	\begin{align*}
		T_{RL} \leq \frac{36p\cdot k}{pn}.
	\end{align*}
	By simplifying we get
	\begin{align*}
		T_{RL} \leq \frac{36k}{n}.
	\end{align*}
	Now let $C=360$ and $N=10$ be constants. Then for all $n\geq N$
	we have
	\begin{align*}
		T_{RL} & \leq \frac{36k}{n} \leq \frac{36\cdot10n}{n} = 360 = C.
	\end{align*}
	It follows from the definition of $O$ that
	\begin{align*}
		T_{RL} = O(1).
	\end{align*}
\end{proof}

\subsection{Formal bounds for total and amortised recycling time in general}

Note that we can, without loss of generality, require that $rn\in\N$ ($r^{-1}$ divides $n$)
because in the context of the problem only integer values make sense and if
$rn\not\in\N$ then there exists an $r'\in\R$ such that $r'n\in\N$ and $r'$ leads
to the same behaviour as $r$. This $r'$ is given by
\begin{align*}
	r'=\frac{\floor{rn}}{n}
\end{align*}
since then $r'n=\floor{r'n}=\floor{rn}$.

\begin{claim}
	Consider a program consisting of $p$ actions that uses exactly $m=10$
	stack entries, has access to $n$ heap cells and that is guaranteed to only
	have at most $rn$ for some $0<r<1$ heap cells that are reachable through references
	from the stack at any point in time.
	Then the total time spent on memory recycling $T_{RP}$ is bounded by $O(pn)$ for
	the largest $r$ and by $O(p)$ for the smallest $r$.
\end{claim}

\begin{proof}
	Consider the number of Mark-Sweep runs $c$. For any $r$ we have
	\begin{align*}
		c \leq \frac{p}{\ceil{(1-r)n}}\leq \frac{p}{(1-r)n}
	\end{align*}
	because every operation can only fill a single heap cell and $\ceil{(1-r)n}$
	heap cells are guaranteed to be empty after each Mark-Sweep run
	(and at the start of the program execution). Further, observe that
	since $0<r<1$ we can find the smallest and largest $r$ for any $n$. We have
	\begin{align}
		\label{rlims}
		r_{max} = \frac{n-1}{n} \hs\text{and}\hs r_{min} = \frac{1}{n}
	\end{align}
	where the values lead to one heap cell being unoccupied or occupied respectively.\\
	Consider $r=r_{min}=1/n$. Then, similar to 1.7, we have the maximum number of Mark-Sweep
	runs $c$ as
	\begin{align*}
		c \leq \frac{p}{\left(1-1/n\right)n}=\frac{p}{n-1}.
	\end{align*}
	We find
	\begin{align*}
		T_{RP} \leq \frac{p\cdot T_{MS}}{n-1} \leq \frac{18pk}{n-1}.
	\end{align*}
	Let $C=340$, $N=1$ and $P=1$. Then, for all $n\geq N$ or $p\geq P$ we have
	\begin{align*}
		T_{RP} \leq \frac{18pk}{n-1} \leq \frac{18p\cdot 10n}{n-1} = 180p\left(\frac{n}{n-1}\right)
		\leq 180p\cdot 2 = 360p.
	\end{align*}
	Therefore, by definition of $O$, we have $T_{RP}=O(p)$.\\
	Now consider $r=r_{max}=(n-1)/n$. Then we have
	\begin{align}
		\label{cmax}
		c\leq \frac{p}{(1-(n-1)/n)n} = \frac{p}{n-n+1} = p.
	\end{align}
	Thus we find
	\begin{align*}
		T_{RP} \leq p\cdot T_{MS} \leq 18pk.
	\end{align*}
	We let $C=180$, $N=1$ and $P=1$. Then for all $n\geq N$ or $p\geq P$ we have
	\begin{align*}
		T_{RP} \leq 18pk \leq 18p \cdot 10n \leq 180pn \leq Cpn.
	\end{align*}
	By definition of $O$, we have $T_{RP}=O(pn)$.
\end{proof}


\begin{claim}
	Consider a program consisting of $p$ actions that uses exactly $m=10$
	stack entries, has access to $n$ heap cells and that is guaranteed to only
	have at most $rn$ for some $0<r<1$ heap cells that are reachable through references
	from the stack at any point in time.
	Then the average time spent on memory recycling per operation $T_{RL}$ is bounded by $O(n)$ for
	the largest $r$ and by $O(1)$ for the smallest $r$.
\end{claim}

\begin{proof}
	Note the limits obtained in (\ref{rlims}). Consider $r=r_{min}=1/n$. Then we find
	the number of Mark-Sweep runs $c$ to be
	\begin{align*}
		c \leq \frac{p}{n-1}.
	\end{align*}
	We find
	\begin{align*}
		T_{RL} = \frac{T_{RP}}{p} = \frac{cT_{MS}}{p} \leq \frac{18k}{n-1}.
	\end{align*}
	We let $C=360$ and $N=1$. Then for all $n\geq N$ we have
	\begin{align*}
		T_{RL} \leq \frac{18k}{n-1} \leq \frac{18\cdot 10n}{n-1} = 180\left(\frac{n}{n-1}\right) \leq 360 = C.
	\end{align*}
	By definition of $O$, we have $T_{RL}=O(1)$.\\
	Consider $r=r_{max}=(n-1)/n$. Then we have $c\leq p$ as derived in (\ref{cmax}).
	This leads to
	\begin{align*}
		T_{RL}=\frac{cT_{MS}}{p}\leq T_{MS} \leq 18k.
	\end{align*}
	Let $C=180$ and $N=1$. Then for all $n\geq N$ we have
	\begin{align*}
		T_{RL} \leq 18k \leq 180n = Cn
	\end{align*}
	i.e.
	\begin{align*}
		T_{RL} = O(n).
	\end{align*}
\end{proof}
This shows that if only one heap cell is required to be empty after a Mark-Sweep run
(or, though this has not been shown, any constant number) significantly more time is
spent on memory recycling than in the $n/2$ case. Specifically, the amortised per-line
cost goes from constant to linear with respect to $n$.

\subsection{Maximum queue length}

\begin{claim}
	\label{claimb}
	Consider a program with $m$ stack entries and $n$ heap cells.
	Let $b$ denote the number of black vertices not in the queue. Then
	the queue length in a Mark procedure will never exceed $m+b$.
\end{claim}

\begin{proof}
	Consider the loop in Mark.
	Let $b_i$ be the number of black vertices not in the queue after $i$ iterations,
	let $q_i$ be the length of the queue after $i$ iterations and
	let $P_0,...,P_n$ be statements. And let those statements be
	the following
	\begin{align*}
		P_i:\hs q_i \leq m+b_i
	\end{align*}
	Consider the statement $P_0$. Note that the only nodes in the queue before
	the first iteration are those directly referenced by stack entries. Therefore
	the length of the queue can be at most $m\leq m+b_0$ since in general $b_i\geq 0$.
	So $P_0$ always holds.\\
	\indent For some $i<l$ where $l$ is the number of iterations of the while loop
	assume $P_i$ holds. Then $q_i\leq m+b_i$. Now consider the
	$(i+1)$th iteration. A single element gets removed from the queue, therefore
	one more black vertex is outside ($b_{i+1}= b_i + 1$). Additionally
	at most two more vertices may be inserted into the queue which leads to a
	maximum net increase of the queue size by $1$ i.e. $q_{i+1}\leq q_i + 1$.
	By combining, we obtain
	\begin{align*}
		m + b_{i+1} = m + b_{i} + 1 \geq q_{i} + 1 \geq q_{i+1}.
	\end{align*}
	This shows $P_i\Rightarrow P_{i+1}$ if $i<l$.\\
	\indent Assume $l\leq i \leq n$. Then $q_i = 0$ since the loop only ends
	when the queue is empty. Thus $q_i \leq m + b_i$. This shows that
	$P_i$ holds for all $l\leq i\leq n$.
	Now we have $P_i\Rightarrow P_{i+1}$ for all $i\leq n-1$ and $P_0$ in
	general. By the \emph{Principle of Mathematical Induction}
	$P_i$ for all $0\leq i\leq n$ follows.
\end{proof}

\begin{claim}
	Consider a program with $m$ stack entries and $n$ heap cells.
	Then the queue length in a Mark procedure will never exceed $(m+n)/2$.
\end{claim}

\begin{proof}
	Let $q_i$ and $b_i$ be as above. Then we know from \emph{Claim \ref{claimb}}
	that for all $i$
	\begin{align}
		\label{ineq1}
		q_i \leq m + b_i.
	\end{align}
	Further, we know that there are only $n$ nodes in total and every node
	is either outside the queue or inside, never both. This leads to
	\begin{align*}
		n \geq q_i + b_i
	\end{align*}
	or, equivalently,
	\begin{align}
		\label{ineq2}
		q_i \leq n-b_i.
	\end{align}
	Adding (\ref{ineq1}) and (\ref{ineq2}) gives
	\begin{align*}
		2q_i \leq m + n
	\end{align*}
	or equivalently
	\begin{align*}
		q_i \leq \frac{m+n}{2}
	\end{align*}
	as required.
\end{proof}

\begin{claim}
	Consider a stack size $m$ and a heap size $n$ where $m\leq n$ and
	$m+n$ is even. Then there exists a memory layout such that a run of Mark
	will reach a queue size of $(m+n)/2$.
\end{claim}

\begin{proof}
	Let $k$ be the largest integer such that
	\begin{align*}
		\sum_{i=1}^k 2^{i-1}m \leq n.
	\end{align*}
	Then consider a memory layout created by the following algorithm:
	\begin{enumerate}
		\item For each stack entry construct a complete binary tree of vertex objects
		      of height $k$ and reference the root from the stack such that no vertex is assigned twice.
		\item Assign the remaining $n-k$ vertex objects in such a way that the tree referenced by the $i$th
		      stack entry does not receive a new node before the $(i-1)$th stack entry references a
		      complete binary tree of height $k+1$
		      and such that for no node the longest path to any leaf of the left sub-tree is shorter
		      than the longest path to any leaf of the right sub-tree.
	\end{enumerate}
	Let $b_i$ be the number of black nodes after $i$ iterations of the while loop and let
	$q_i$ be the length of the queue at that point. Then let $P_0,...,P_{n-m}$ be the
	following statements:
	\begin{align*}
		P_i:\hs q_i = m+b_i.
	\end{align*}
	Observe that in this case the loop can only possibly run $n-m$ times because $m$ items have
	already been added. Also note that since $n-m$ is even there are an even number of nodes
	in the trees that are not roots. Due to the arrangement this means that every node has either
	two or no children. Now consider $P_0$. Since $q_i=m$ and $b_i=0$ this statement holds.\\
	\indent Assume $P_i$. Then there are two cases. If the next node in the queue has two children
	then $q_{i+1} = q_i+1$ because one node gets removed and both children get added. In that
	case since $q_i = m + b_i$ it follows that $q_i + 1 = m + b_i + 1$ and therefore
	$q_{i+1} = m + b_{i+1}$. If the
	node has no children then $q_i$ must be the maximum queue length. All subsequent iterations
	will remove one element from the queue each and $P_{i+1}$ does not hold.\\
	This shows that the queue length behaves as follows for some $1\leq l \leq n$
	\begin{enumerate}
		\item For the first $l$ iterations $q_i = m + i$
		\item For the subsequent iterations $q_i = q_l - (i - l)$
	\end{enumerate}
	We know $q_0 = m$ and $q_n=0$ and can now deduce that $q_i$ needs to be composed of
	the two sequences
	\begin{align*}
		x_i & = m + i \\
		y_i & = n - i
	\end{align*}
	By setting $x_i=y_i$ we find
	\begin{align*}
		i = \frac{n-m}{2}
	\end{align*}
	and for this $i$ we have
	\begin{align*}
		x_i = y_i = \frac{n+m}{2}\hs\text{and thus}\hs q_i = \frac{n+m}{2}
	\end{align*}
	as desired.


\end{proof}

\subsection{Discussion of queue implementation}

Since we have just shown that the number of elements in the queue is bounded
above and that bound should be known at the time of execution, using a
circular buffer with a fixed size array would make sense here. This also avoids
tedious memory management during each Mark-Sweep cycle as the required
space can be allocated upon launch and deallocated after the program has exited.
Compare this to an implementation based on a linked list where every element would
have to be deallocated individually. A linked list would further be impractical
since each element would need to store a reference to its successor and therefore
effectively double the space required.


\section{The knapsack problem}


\subsection{Pseudocode for fractional greedy implementation}

\begin{pseudo}
	Greedy$(S, s_1, ..., s_n, v_1, ..., v_n)\to$ list of fractions\\+
	$R\leftarrow\text{array of rationals with length }n$\\
	$A\leftarrow\text{array of integers with length }n$\\
	\textbf{for $i$ from $1$ to $n$ do}\\+
	$A[i-1] \leftarrow i$\\
	$R[i-1] \leftarrow 0$\\-
	$C\leftarrow \lambda i.\:v_i/s_i$\\
	$Q\leftarrow\text{Build-Max-Heap}(A,C)$\\
	\textbf{while} $Q.\text{heap\_size}>0$ \textbf{do}\\+
	$i\leftarrow\text{Heap-Extract-Max}(Q)$\\
	$R[i-1]\leftarrow \min\{1, S/s_i\}$\\
	\textbf{if $S>s_i$ do}\\+
	$S\leftarrow S-s_i$\\-
	\textbf{else}\\+
	\textbf{break}\\--
	\textbf{return} $R$
\end{pseudo}

Note that $C$ is a function calculating the value per unit volume
for any given index and Build-Max-Heap$(A,C)$ creates a heap from $A$
by applying $C$ before comparing elements.

\subsection{Formal bounds for runtime of fractional greedy implementation}

\begin{claim}
	Let $T_G$ be the worst-case runtime of the Greedy procedure. Then $T_G=O(n\lg n)$.
\end{claim}

\begin{proof}
	Firstly, observe that lines 2, 3 and 7 run in $O(1)$ time. Further,
	lines 4 to 6 require $O(n)$ time since the iterate over all $n$
	possible indices. Line 8 creates a heap which has been shown to be possible
	in $O(n)$ time. Note that using the special cost function $C$ is not
	an issue because $C$ itself runs in $O(1)$ time due to only containing
	mathematical operations.\\
	Except for the first line of the while loop, all remaining lines are
	possible to be run in constant time as well. Line 10 uses Heap-Extract-Max
	which has been shown to run in $O(\lg n)$ time. If the \texttt{while}
	loop does not break early, it runs for every single one of the $n$
	elements on the heap. Therefore we have
	\begin{align*}
		T_{while} = n(4+T) = 4n + nT
	\end{align*}
	for some $T=O(\lg n)$. We find $4n=O(n)$. By definition of $O$
	\begin{align*}
		T \leq C\lg n
	\end{align*}
	for some constants $C$ and $N$ and all $n\geq N$. Therefore
	\begin{align*}
		nT \leq Cn\lg n
	\end{align*}
	for the same choices of $C$ and $N$. Thus $nT=O(n\lg n)$. It follows that
	\begin{align*}
		T_{while} = O(n) + O(n\lg n) = O(n\lg n)
	\end{align*}
	for the while loop and thus in total
	\begin{align*}
		T_G = O(1) + O(n) + O(n\lg n) = O(n\lg n).
	\end{align*}
\end{proof}

\subsection{Proof of optimality for fractional greedy implementation}

\begin{claim}
	The Greedy algorithm described in 2.1 is optimal for all possible inputs.
\end{claim}

\begin{proof}
	Assume the available volume fits into the knapsack or more precisely
	\begin{align*}
		S \geq \sum_{i=1}^n s_i.
	\end{align*}
	Then, since all $v_i>0$, taking all $G_i$ results in the highest value.
	Therefore the algorithm should return a list of length $n$ filled with
	$1$s. Observe that the while loop runs for all $n$ elements unless it
	breaks early. It only breaks early if the volume of the remaining goods
	exceeds the capacity of the knapsack. This does not happen in this case
	so the value written on line 11 is guaranteed to be 1 in all $n$
	iterations. Since all $n$ indices on the heap are distinct, the entire
	list has to be filled with $1$s.\\
	Now assume the available volume does not fit into the knapsack or more
	precisely
	\begin{align*}
		S < \sum_{i=0}^n s_i.
	\end{align*}
	Let $\hat G_1, ...,\hat G_n$, $\hat v_1, ...,\hat v_n$ and $\hat s_1, ...,\hat s_n$
	elements, their volume and their value ordered in decreasing value of value per unit
	volume. In other words, order them such that
	\begin{align}
		\label{iord}
		\forall i,j,\: i < j \Rightarrow \frac{\hat v_i}{\hat s_i}\geq \frac{\hat v_j}{\hat s_j}.
	\end{align}
	Then let $k$ be the largest integer such that
	\begin{align*}
		S \geq \sum_{i=1}^k \hat s_i.
	\end{align*}
	Then the optimal solution $R$ is the list of length $n$ such that for all $i$
	\begin{align}
		\label{optsol}
		R[i-1] = \begin{cases}
			         1              & \text{if } G_i = \hat G_j \text{ for some } j \leq k, \\
			         S/\hat s_{i+1} & \text{if } G_i = \hat G_{k+1},                        \\
			         0              & \text{if } G_i = \hat G_j \text{ for some } j > k+1.
		         \end{cases}
	\end{align}
	Note that all $G_1, ..., G_n$ and $\hat G_1, ..., \hat G_n$ are distinct
	and only equal to themselves, i.e.
	\begin{align*}
		\forall i,j,\hs i=j \hs \Leftrightarrow \hs G_i = G_j \text{ and } \hat G_i = \hat G_j.
	\end{align*}
	To show that (\ref{optsol}) is indeed optimal,
	we consider the lists $R_1, ..., R_n$ which are the optimal
	solutions of the knapsack problem with $\hat G_1, ..., \hat G_n$ and for all $R_i$
	\begin{align*}
		S = \sum_{j=1}^i \hat s_j
	\end{align*}
	so the optimal solution for the problem where the items $\hat G_1, ...,\hat G_i$
	fit the knapsack perfectly. Then choosing these $i$ elements
	is optimal because swapping any amount of volume $\Delta s$ from an
	element with value density $d_a = \hat v_a / \hat s_a$ for another element
	with density $d_b = \hat v_b / \hat s_b$ with $a\leq i < b$ results in a
	loss of value
	\begin{align*}
		(d_a - d_b) \cdot \Delta s \geq 0
	\end{align*}
	because by (\ref{iord}) $d_a\geq d_b$. Therefore the optimal solution $R$
	should include the $k$ most dense items. This leaves a remaining volume
	of
	\begin{align*}
		S - \sum_{i=1}^k \hat s_i
	\end{align*}
	to fill. By similar argument we find that the optimal strategy is to take
	as much as we can from the item with size $\hat s_{k+1}$. Since we know
	that the item is larger than the remainder of the knapsack, the fraction
	of the element we can take is
	\begin{align*}
		\left(S - \sum_{i=1}^k \hat s_i\right)/\hat s_{k+1}
	\end{align*}
	which is the middle case of the optimal solution in (\ref{optsol}).
	Since all other cells are not touched after initialisation, this shows (\ref{optsol}).\\
	Let's now verify that the Greedy algorithm indeed returns this $R$.
	Firstly, observe that in the $j$th iteration then
	$i$ is assigned precisely the value that leads to $G_i=\hat G_j$,
	i.e. the $j$th densest item.
	Further, at the beginning of the $j$th iteration the value of $S$, let's denote it $S_j$,
	is
	\begin{align}
		\label{sj}
		S_j = S - \sum_{i=1}^{j-1} \hat s_i.
	\end{align}
	This is certainly the case in the beginning as $S_1=S$. Further, if the code
	has reached the $j$th iteration for some $j>1$, then line 13 must have been run in the
	$(j-1)$th iteration. Since the $i$ behaves as described above, we have
	$S_j=S_{j-1}-\hat s_j$. Assuming that
	\begin{align*}
		S_{j-1} = S - \sum_{i=1}^{j-2} \hat s_i
	\end{align*}
	this results in
	\begin{align*}
		S_j = S - \sum_{i=1}^{j-2} \hat s_i - \hat s_{j-1} = S - \sum_{i=1}^{j-1} \hat s_i.
	\end{align*}
	By the principle of mathematical induction this proves that (\ref{sj}) holds.
	By our definition of $k$, the assignment on line 11 will yield 1 for the first $k$
	iterations and only use $S/s_i$ on the $(k+1)$th run. The assigned value then is
	\begin{align*}
		S_{k+1}/\hat s_{k+1}.
	\end{align*}
	With (\ref{sj}) this is exactly equal to the middle case of (\ref{optsol}).
	Since this is being assigned to the $R[i-1]$ where $G_i = \hat G_{k+1}$ this
	is precisely the behaviour described in (\ref{optsol}).
	Since the loop is then ended on line 14, the list is returned on line 15,
	and no other nonzero values are written to $R$,
	we know that the solution is optimal.
\end{proof}

\subsection{Disproving optimality for greedy 01-knapsack algorithms}

\subsubsection{Highest value-per-unit-volume}

Consider the following problem: Let $S=1$, $G=\{G_1, G_2\}$, $s_1=1/4$, $s_2=1$,
$v_1=1$ and $v_2=2$. The we find the densities (value-per-unit-volume)
\begin{align*}
	d_1 = \frac{1}{1/4} = 4 \hs\text{and}\hs d_2 =\frac{2}{1}=2.
\end{align*}
Observe that we can only fit one of the items into the given $S$.
Therefore the algorithm would select $G_1$ and end up with a value of $1$.
However, selecting $G_2$ would result in a total value of $2$ which makes
this algorithm suboptimal.

\subsubsection{Highest value}

Consider the following problem: Let $S=1$, $G=\{G_1, G_2, G_3\}$ and
\begin{align*}
	\begin{split}
		s_1&=1/2,\\
		s_2&=1/2,\\
		s_3&=1,
	\end{split}
	\begin{split}
		v_1 &= 1\\
		v_2 &= 1\\
		v_3 &= 3/2
	\end{split}
\end{align*}
The algorithm would select $G_3$ because it has the highest value
and end up with a value of $3/2$ in total. Selecting $G_1$ and
$G_2$ would, however, result in a total value of $2$. Thus the
algorithm is suboptimal in this case.

\subsection{Optimal algorithm for the 0-1 knapsack problem}

\begin{pseudo}
	Dynamic$(S,s_1,...,s_n,v_1,...,v_n)\leftarrow$ list of fractions\\+
	$A\leftarrow S\times (n+1)\text{ array of integers}$\\
	$P\leftarrow S\times (n+1)\text{ array of integers}$\\
	\textbf{for $s$ from $0$ to $S$ do}\\+
	$A[0,s] \leftarrow 0$\\-
	\textbf{for $i$ from $1$ to $n$ do}\\+
	\textbf{for $s$ from $0$ to $S$ do}\\+
	\textbf{if $s_i>s$ or $A[i-1,s]\geq A[i-1,s-s_i] + v_i$ then}\\+
	$A[i, s] \leftarrow A[i-1,s]$\\
	$P[i, s] \leftarrow 0$\\-
	\textbf{else}\\+
	$A[i, s] \leftarrow A[i-1,s-s_i] + v_i$\\
	$P[i, s] \leftarrow 1$\\---
	$R\leftarrow \text{array of rationals with length }n$\\
	\textbf{for $i$ from $0$ to $n-1$ do}\\+
	$R[i] \leftarrow 0$\\-
	$s\leftarrow S$\\
	\textbf{for $i$ from $n$ to $1$ do}\\+
	\textbf{if $P[i,s]=1$ do}\\+
	$R[i-1]\leftarrow 1$\\
	$s \leftarrow s - s_i$\\--
	\textbf{return $R$}
\end{pseudo}

\paragraph{Runtime analysis}

Observe that every line may be executed in $O(1)$ time. Notably, this
can even be assumed for the memory allocations in lines 2, 3 and 14.
Let $T_D$ denote the total runtime of Dynamic.
Then we have four distinct \texttt{for}-loops. Let their runtimes be
$T_1, ..., T_4$. Then we know
\begin{align*}
	T_D = O(1) + T_1 + T_2 + T_3 + T_4.
\end{align*}
The values of $T_1$, $T_3$ and $T_4$ are easy to calculate because
they don't contain any nested loops or other operation with runtimes
other than $O(1)$. In fact the number of line executions is bounded
above by a constant value. Therefore, based on the number of
iterations we have
\begin{align*}
	T_1 = O(S),\hs T_3 = O(n),\hs T_4 = O(n).
\end{align*}
Let's now consider $T_2$. Note that there is a nested loop for which
each iteration will execute 3 lines of code. Therefore it's easy to show that
\begin{align*}
	T_2 = O(nS).
\end{align*}
This leads us to the total runtime
\begin{align*}
	T_D = O(1) + O(S) + O(nS) + O(n) + O(n) = O(nS)
\end{align*}
since $O(1),O(n),O(S)\subset O(nS)$.
\end{document}