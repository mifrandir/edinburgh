\documentclass{article}
\usepackage[a4paper]{geometry}
\geometry{tmargin=3cm, bmargin=3cm, lmargin=2cm, rmargin=2cm}
\usepackage[british]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{nicefrac}
\usepackage{siunitx}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{fontspec}
\setmainfont{arial}
\usepackage{minted}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\arccot}{\text{cot}^{-1}}
\DeclareMathOperator{\arccsc}{\text{csc}^{-1}}
\DeclareMathOperator{\arccosh}{\text{cosh}^{-1}}
\DeclareMathOperator{\arcsinh}{\text{sinh}^{-1}}
\DeclareMathOperator{\arctanh}{\text{tanh}^{-1}}
\DeclareMathOperator{\arcsech}{\text{sech}^{-1}}
\DeclareMathOperator{\arccsch}{\text{csch}^{-1}}
\DeclareMathOperator{\arccoth}{\text{coth}^{-1}} 
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator{\sech}{sech}
\newtheoremstyle{sltheorem} {}                % Space above
{}                % Space below
{\upshape}        % Theorem body font % (default is "\upshape")
{}                % Indent amount
{\bfseries}       % Theorem head font % (default is \mdseries)
{.}               % Punctuation after theorem head % default: no punctuation
{ }               % Space after theorem head
{}                % Theorem head spec
\theoremstyle{sltheorem}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\C}{\mathbb{C}} % \C defined in `hyperref'
\DeclareMathOperator{\lub}{LUB}
\DeclareMathOperator{\glb}{GLB}
\DeclareMathOperator{\hcf}{hcf}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\cl}{cl}
\newcommand*\lneg[1]{\overline{#1}}
\newcommand*\B[1]{\textbf{#1}}
\newcommand*\T[1]{\texttt{#1}}
\usepackage{expl3}[2012-07-08]
\ExplSyntaxOn
\cs_new_eq:NN \fpeval \fp_eval:n
\ExplSyntaxOff
\begin{document}
\title{Introduction to Algorithms and Data Structures (YEAR2)}
\author{Franz Miltz}
\maketitle
\tableofcontents  
\section{Asymptotics Analysis}
\B{Asymptotic theory} makes precise quantitive statements about efficiency of algorithms themselves.
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$ be functions. Then
	\text{$f\in o(g)$} if, and only if, 
	\begin{align*}
		\forall c>0,\:\exists N\st \forall n \geq N, f(n)<cg(n)
	\end{align*}
	where $c\in\R$ and $N,n\in\N$.
\end{definition}
\begin{theorem}
	Let $f:\N\to\R_{\geq 0}$ and let $o(f)$ refer to some
	function within the set $o(f)$. Then
	\begin{itemize}
		\item $co(f)=o(f)$ where $c\in\R$,
		\item $o(f) + o(f) = o(f)$.
	\end{itemize}
\end{theorem}
\begin{theorem}
	Let $f,r:\N\to\R_{\geq 0}$ and let $a,b\in\R$. Then
	\begin{align*}
		f=o(g) \Leftrightarrow af=o(bg).	
	\end{align*}
\end{theorem}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f=\omega(g)$ if, and only if, $g=o(f)$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in O(g)$ if, and only if,
	\begin{align*}
		\exists C > 0\st \exists N \st \forall n \geq N,\: f(n) \geq Cg(n)
	\end{align*}
	where $C\in\R$ and $N,n\in\N$.\\
	We call $g$ an \B{asymptotic upper bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Omega(g)$ if, and only if, $g\in O(f)$.\\
	We call $g$ an \B{asymptotic lower bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $f\in O(f) \cap \Omega(f)$.\\
	We call $g$ an \B{asymptotic tight bound} for $f$.
\end{definition}
\begin{theorem}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $g\in\Theta(f)$.
\end{theorem}
\subsection{Recurrence relations}
\begin{theorem}[The Master Theorem]
	Assume a recurrence relation $T$ has the form
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(1) &\text{if $n\leq n_0$}\\
			aT(n/b) + \Theta(n^k) &\text{if $n>n_0$}
		\end{cases}.
	\end{align*}
	Then, with $e=\log_b a$, 
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(n^e) &\text{if $e>k$}\\
			\Theta(n^k\lg n) &\text{if $e=k$}\\
			\Theta(n^k) &\text{if $e<k$}
		\end{cases}
	\end{align*}
\end{theorem}
\section{Algorithms and cost models}
\begin{definition}
	The \B{cost} of an algorithm is a quantity to measure its performance.\\
	The cost may be defined in different ways depending on
	how in depth the analysis is supposed to be and what
	is of interest in a particular situation.
\end{definition}
Note that the cost model needs to be specified when comparing algorithms.
\begin{definition}
	We considering the cost of an algorithm for a specific input size there are different cases to consider:
	\begin{itemize}
		\item \B{worst-case} cost: the single worst cost out of all possible inputs
		\item \B{best-case} cost: the single best cost out of all possible inputs
		\item \B{average-case} cost: the average over all the costs for all possible inputs
	\end{itemize}
\end{definition}
\begin{theorem}
	Let $A$ be an algorithm and let $T_w$ be its worst-case runtime. Then, if $T_w=O(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $O(g)$.
\end{theorem}
\begin{theorem}
	Let $A$ be an algorithm and let $T_b$ be its best-case runtime. Then, if $T_b=\Omega(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $\Omega (g)$.
\end{theorem}
\section{Collections}
\subsection{Sets and dictionaries}
\begin{definition}
	A \B{finite set} containing values of type $X$ is a datastructure with the following interface
	\begin{align*}
		\textbf{contains} &: X\to \texttt{bool}\\
		\textbf{insert} &: X \to \texttt{void}\\
		\textbf{delete} &: X \to \texttt{void}\\
		\textbf{isEmpty} &: \texttt{void} \to \texttt{bool}
	\end{align*}
\end{definition}
\begin{definition}
	A \B{dictionary} mapping keys of type $X$ to values of type $Y$
	is a datastructure with the following interface
	\begin{align*}
		\textbf{lookup} &: X \to Y\\
		\textbf{insert} &: X \to Y \to \texttt{void}\\
		\textbf{delete} &: X \to \texttt{void}\\
		\textbf{isEmpty} &: \texttt{void} \to \texttt{bool}
	\end{align*}
\end{definition}
\begin{definition}
	A \B{hash table} is a datastructure that uses a \B{hash function} $f: X \to [0,m-1]$
	to store key value pairs $(X,Y)$ in an array of length $m$.\\
	The \B{load factor} $\alpha$ is the average number of elements associated with each cell
	of the underlying array. If $f$ is a perfect hash function, i.e. it maps all inputs uniformly
	onto $[0,m-1]$, then $\alpha$ is given by 
	\begin{align*}
		\alpha = n / m.
	\end{align*}
\end{definition}
\begin{proposition}
	Using linked lists to manage the elements in each bucket of a hash table, then
	\begin{itemize}
		\item a lookup of a key $k$  takes on average $\Theta(\alpha)$ comparisons, and
		\item a lookup of a key $k$ takes in the worst-case $\Theta(n)$ comparisons.
	\end{itemize}
\end{proposition}
\begin{definition}
	\B{Open addressing} is an algorithm to handle hash collisions within a hash table 
	by using a hash function $f: (X, [0,m-1]) \to [0,m-1]$ that generates a permutation of all possible
	hash codes for each possible input $X$. An element $x\in X$ is then stored the hash code $f(x, i)$
	if and only if all $f(x,j)$ for $j < i$ are already occupied.
\end{definition}
\begin{proposition}
	In a hash table with load factor $\alpha$ that uses open addressing requires, on average, $1/(1-\alpha)$ 
	probes for an unsuccessful lookup and less for a successful one.
\end{proposition}
\begin{definition}
	A \B{binary tree} consists of a value and at most one left child and at most one right child where
	children are other binary trees.
\end{definition}
\begin{definition}
	An \B{ordered binary tree} with a given ordering $(<) : X \to X \to \texttt{bool}$ is a binary tree 
	with a root $x$ such that
	\begin{align*}
		\forall y \in L(x), y.\texttt{key} < x.\texttt{key} 
		\text{ and } \forall y \in R(x), x.\texttt{key} < y.\texttt{key},
	\end{align*}
	and both $L(x)$ and $R(x)$ are ordered.
\end{definition}
\begin{proposition}
	For a prefectly balanced ordered binary tree, the lookup time is be $O(\log n)$.\\
	More generally, for any such tree with a maximum depth $d$ of at most $d=2 \lg n$ the
	lookup time is $O(\log n)$.
\end{proposition}
\begin{center}
	
\begin{tabular}{| l | l |}
	\hline
	Type & \\
	\hline
	Set & \begin{tabular}{ l | l | l | l }
		Implementation & \texttt{contains} & \texttt{insert} & \texttt{delete}\\
		\hline
		Linked List (simple) 
		& $\Theta(n)$, $\Theta(n)$
		& $\Theta(1)$, $\Theta(1)$
		& $\Theta(n)$, $\Theta(n)$\\
		\hline
		Ordered Linked List (simple) 
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$\\
		\hline
		Hash Table (linked bucket list)
		& $\Theta(\alpha)$, $\Theta(n)$
		& $\Theta(1)$, $\Theta(1)$
		& $\Theta(\alpha)$, $\Theta(n)$\\
		\hline
		Hash Table (open addressing)
		& $\Theta(1/(1-\alpha))$, $\Theta(n)$
		& $\Theta(1/(1-\alpha))$, $\Theta(n)$
		& - \\
		\hline
		Ordered Binary Tree
		& $\Theta(\lg n)$, $\Theta(n)$
		& $\Theta(\lg n)$, $\Theta(n)$ 
		& $\Theta(\lg n)$, $\Theta(n)$\\
		\hline
		Red-Black Tree
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
	\end{tabular} \\
	\hline
\end{tabular}
\end{center}
\subsection{The Heap data structure}
\begin{definition}
	A heap is an \B{almost-complete} binary tree:
	\begin{itemize}
		\item All leaves are either at depth $h-1$ or $h$ (where $h$ is the height of the tree)
		\item The depth-$h$ leaves all appear consecutively from left-to-right
	\end{itemize}
\end{definition}
\begin{lemma}
	The height $h$ of a heap with $n$ is in the range given by
	\begin{align*}
		\lg(n)-1 < h \leq \lg(n).
	\end{align*}
\end{lemma}
\begin{theorem}
	The heap operations have the following runtimes:
	\\
	\begin{center}
	\begin{tabular}{| l | c |}
		\hline
		Operation & Asymptotic Runtime\\
		\hline
		Max & $\Theta(1)$\\
		\hline
		Heapify & $O(\lg(n))$\\
		\hline
		Extract-Max & $O(\lg(n))$\\
		\hline
		Insert & $O(\lg(n))$\\
		\hline
		Build & $O(n)$\\
		\hline
	\end{tabular}
	\end{center}
\end{theorem}
\end{document}
