\documentclass{article}
\usepackage{notes-preamble}

\begin{document}
\title{Introduction to Algorithms and Data Structures (YEAR2)}
\author{Franz Miltz}
\maketitle
\tableofcontents  



\section{Asymptotics Analysis}



\B{Asymptotic theory} makes precise quantitive statements about efficiency of algorithms themselves.
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$ be functions. Then
	\text{$f\in o(g)$} if, and only if, 
	\begin{align*}
		\forall c>0,\:\exists N\st \forall n \geq N, f(n)<cg(n)
	\end{align*}
	where $c\in\R$ and $N,n\in\N$.
\end{definition}
\begin{theorem}
	Let $f:\N\to\R_{\geq 0}$ and let $o(f)$ refer to some
	function within the set $o(f)$. Then
	\begin{itemize}
		\item $co(f)=o(f)$ where $c\in\R$,
		\item $o(f) + o(f) = o(f)$.
	\end{itemize}
\end{theorem}
\begin{theorem}
	Let $f,r:\N\to\R_{\geq 0}$ and let $a,b\in\R$. Then
	\begin{align*}
		f=o(g) \Leftrightarrow af=o(bg).	
	\end{align*}
\end{theorem}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f=\omega(g)$ if, and only if, $g=o(f)$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in O(g)$ if, and only if,
	\begin{align*}
		\exists C > 0\st \exists N \st \forall n \geq N,\: f(n) \geq Cg(n)
	\end{align*}
	where $C\in\R$ and $N,n\in\N$.\\
	We call $g$ an \B{asymptotic upper bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Omega(g)$ if, and only if, $g\in O(f)$.\\
	We call $g$ an \B{asymptotic lower bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $f\in O(f) \cap \Omega(f)$.\\
	We call $g$ an \B{asymptotic tight bound} for $f$.
\end{definition}
\begin{theorem}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $g\in\Theta(f)$.
\end{theorem}
\begin{theorem}
	\begin{align*}
		\lg n! = \Theta(n \lg n).
	\end{align*}
\end{theorem}


\subsection{Recurrence relations}


\begin{theorem}[The Master Theorem]
	Assume a recurrence relation $T$ has the form
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(1) &\text{if $n\leq n_0$}\\
			aT(n/b) + \Theta(n^k) &\text{if $n>n_0$}
		\end{cases}.
	\end{align*}
	Then, with $e=\log_b a$, 
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(n^e) &\text{if $e>k$}\\
			\Theta(n^k\lg n) &\text{if $e=k$}\\
			\Theta(n^k) &\text{if $e<k$}
		\end{cases}
	\end{align*}
\end{theorem}



\section{Algorithms and cost models}



\begin{definition}
	The \B{cost} of an algorithm is a quantity to measure its performance.\\
	The cost may be defined in different ways depending on
	how in depth the analysis is supposed to be and what
	is of interest in a particular situation.
\end{definition}
Note that the cost model needs to be specified when comparing algorithms.
\begin{definition}
	We considering the cost of an algorithm for a specific input size there are different cases to consider:
	\begin{itemize}
		\item \B{worst-case} cost: the single worst cost out of all possible inputs
		\item \B{best-case} cost: the single best cost out of all possible inputs
		\item \B{average-case} cost: the average over all the costs for all possible inputs
	\end{itemize}
\end{definition}
\begin{theorem}
	Let $A$ be an algorithm and let $T_w$ be its worst-case runtime. Then, if $T_w=O(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $O(g)$.
\end{theorem}
\begin{theorem}
	Let $A$ be an algorithm and let $T_b$ be its best-case runtime. Then, if $T_b=\Omega(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $\Omega (g)$.
\end{theorem}


\subsection{Bubblesort}


\subsection{Insertion Sort}


\subsection{Mergesort}


\subsection{Heapsort}


\subsection{Quicksort}


\subsubsection{Algorithm}

Algorithm for input array $A$:
\begin{enumerate}
	\item If $|A|<2$, do nothing. Otherwise partition the array:
	\begin{tabular}{|c|c|c|}
		\hline
		$\leq$ pivot & pivot & $\geq$pivot\\
		\hline
	\end{tabular}
	\item Sort the resulting subarrays recursively.
\end{enumerate}

\noindent Main algorithm:
\begin{minted}{python}
def quick_sort(A, p, r):
  if p < r:
    split = partition(A, p, r)
	quick_sort(A, p, split-1)
	quick_sort(A, split+1, r)
\end{minted}

\noindent Partitioning algorithm:
\begin{minted}{python}
def partition(A, p, r):
  pivot = A[r].key
  i = p - 1
  for j in range(p,r):
    if A[j] <= pivot:
      i += 1
      exchange(A, i, j)
  exchange(A, i+1, r)
  return i+1
\end{minted}

\subsubsection{Analysis}

\begin{center}
	\begin{tabular}{| l | c |}
		\hline
		Worst Case & $\Theta(n^2)$\\
		\hline
		Average Case & $\Theta(n\lg n)$\\
		\hline
	\end{tabular}
	\end{center}


\section{Collections}



\subsection{Sets and dictionaries}


\begin{definition}
	A \B{finite set} containing values of type $X$ is a datastructure with the following interface
	\begin{align*}
		\B{contains} &: X\to \texttt{bool}\\
		\B{insert} &: X \to \texttt{void}\\
		\B{delete} &: X \to \texttt{void}\\
		\B{isEmpty} &: \texttt{void} \to \texttt{bool}
	\end{align*}
\end{definition}
\begin{definition}
	A \B{dictionary} mapping keys of type $X$ to values of type $Y$
	is a datastructure with the following interface
	\begin{align*}
		\B{lookup} &: X \to Y\\
		\B{insert} &: X \to Y \to \texttt{void}\\
		\B{delete} &: X \to \texttt{void}\\
		\B{isEmpty} &: \texttt{void} \to \texttt{bool}
	\end{align*}
\end{definition}
\begin{definition}
	A \B{hash table} is a datastructure that uses a \B{hash function} $f: X \to [0,m-1]$
	to store key value pairs $(X,Y)$ in an array of length $m$.\\
	The \B{load factor} $\alpha$ is the average number of elements associated with each cell
	of the underlying array. If $f$ is a perfect hash function, i.e. it maps all inputs uniformly
	onto $[0,m-1]$, then $\alpha$ is given by 
	\begin{align*}
		\alpha = n / m.
	\end{align*}
\end{definition}
\begin{proposition}
	Using linked lists to manage the elements in each bucket of a hash table, then
	\begin{itemize}
		\item a lookup of a key $k$  takes on average $\Theta(\alpha)$ comparisons, and
		\item a lookup of a key $k$ takes in the worst-case $\Theta(n)$ comparisons.
	\end{itemize}
\end{proposition}
\begin{definition}
	\B{Open addressing} is an algorithm to handle hash collisions within a hash table 
	by using a hash function $f: (X, [0,m-1]) \to [0,m-1]$ that generates a permutation of all possible
	hash codes for each possible input $X$. An element $x\in X$ is then stored the hash code $f(x, i)$
	if and only if all $f(x,j)$ for $j < i$ are already occupied.
\end{definition}
\begin{proposition}
	In a hash table with load factor $\alpha$ that uses open addressing requires, on average, $1/(1-\alpha)$ 
	probes for an unsuccessful lookup and less for a successful one.
\end{proposition}
\begin{definition}
	A \B{binary tree} consists of a value and at most one left child and at most one right child where
	children are other binary trees.
\end{definition}
\begin{definition}
	An \B{ordered binary tree} with a given ordering $(<) : X \to X \to \texttt{bool}$ is a binary tree 
	with a root $x$ such that
	\begin{align*}
		\forall y \in L(x), y.\texttt{key} < x.\texttt{key} 
		\text{ and } \forall y \in R(x), x.\texttt{key} < y.\texttt{key},
	\end{align*}
	and both $L(x)$ and $R(x)$ are ordered.
\end{definition}
\begin{proposition}
	For a prefectly balanced ordered binary tree, the lookup time is be $O(\log n)$.\\
	More generally, for any such tree with a maximum depth $d$ of at most $d=2 \lg n$ the
	lookup time is $O(\log n)$.
\end{proposition}
\begin{center}
	
\begin{tabular}{| l | l |}
	\hline
	Type & \\
	\hline
	Set & \begin{tabular}{ l | l | l | l }
		Implementation & \texttt{contains} & \texttt{insert} & \texttt{delete}\\
		\hline
		Linked List (simple) 
		& $\Theta(n)$, $\Theta(n)$
		& $\Theta(1)$, $\Theta(1)$
		& $\Theta(n)$, $\Theta(n)$\\
		\hline
		Ordered Linked List (simple) 
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$\\
		\hline
		Hash Table (linked bucket list)
		& $\Theta(\alpha)$, $\Theta(n)$
		& $\Theta(1)$, $\Theta(1)$
		& $\Theta(\alpha)$, $\Theta(n)$\\
		\hline
		Hash Table (open addressing)
		& $\Theta(1/(1-\alpha))$, $\Theta(n)$
		& $\Theta(1/(1-\alpha))$, $\Theta(n)$
		& - \\
		\hline
		Ordered Binary Tree
		& $\Theta(\lg n)$, $\Theta(n)$
		& $\Theta(\lg n)$, $\Theta(n)$ 
		& $\Theta(\lg n)$, $\Theta(n)$\\
		\hline
		Red-Black Tree
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
		& $\Theta(\lg n)$, $\Theta(\lg n)$
	\end{tabular} \\
	\hline
\end{tabular}
\end{center}


\subsection{The Heap data structure}


\begin{definition}
	A heap is an \B{almost-complete} binary tree:
	\begin{itemize}
		\item All leaves are either at depth $h-1$ or $h$ (where $h$ is the height of the tree)
		\item The depth-$h$ leaves all appear consecutively from left-to-right
	\end{itemize}
\end{definition}
\begin{lemma}
	The height $h$ of a heap with $n$ is in the range given by
	\begin{align*}
		\lg(n)-1 < h \leq \lg(n).
	\end{align*}
\end{lemma}
\begin{theorem}
	The heap operations have the following runtimes:
	\\
	\begin{center}
	\begin{tabular}{| l | c |}
		\hline
		Operation & Asymptotic Runtime\\
		\hline
		Max & $\Theta(1)$\\
		\hline
		Heapify & $O(\lg(n))$\\
		\hline
		Extract-Max & $O(\lg(n))$\\
		\hline
		Insert & $O(\lg(n))$\\
		\hline
		Build & $O(n)$\\
		\hline
	\end{tabular}
	\end{center}
\end{theorem}


\subsection{Graphs}


\subsubsection{Directed and Undirected Graphs}

\begin{definition}
	A graph $G$ is defined as
	\begin{align*}
		G = (V,E)
	\end{align*}
	where $V$ is the set of vertices and $E\subseteq V\times V$ is the set of edges.
\end{definition}

\begin{definition}
	A graph $G$ is \B{undirected} if and only if
	\begin{align*}
		\forall v,w\in V,\: (v,w)\in E \Leftrightarrow (w,v)\in E.
	\end{align*}
	If a graph is not undirected, it is \B{directed}.
\end{definition}

\subsection{Adjacency Matrices vs Lists}

\begin{definition}
	Let $G=(V,E)$ be a graph with $n$ vertices $V=\{v_1, ..., v_n\}$.
	Then the \B{adjacency matrix} of $G$ is the $n\times n$ matrix
	$A=(a_{ij})_{i,j\in[1,n]}$ with 
	\begin{align*}
		a_{ij} = \begin{cases}
			1 &\text{if } (v_i,v_j)\in E\\
			0 &\text{otherwise}
		\end{cases}.
	\end{align*}
\end{definition}

\begin{theorem}
	Let $G=(V,E)$ with $|V|=n$ and $|E|=m$. Then the following 
	complexities apply:
	\begin{center}
	\begin{tabular}{| l | c | c |}
		\hline
		 Measure & Matrix & List\\
		\hline
		Space & $\Theta(n^2)$ & $\Theta(n+m)$ \\
		\hline
		Time to check if $w$ adjacent to $v$ & $\Theta(1)$ & $\Theta(\text{out}(v))$\\
		\hline
		Time to visit all $W$ adjacent to $v$ & $\Theta(n)$ & $\Theta(\text{out}(v))$\\
		\hline
		Time to visit all edges & $\Theta(n^2)$ & $\Theta(n+m)$\\
		\hline
	\end{tabular}
\end{center}
\begin{lemma}
	A graph $G=(V,E)$ with $|V|=n$ and $|E|=m$ is
	\begin{itemize}
		\item \B{dense} if $m$ is close to $n^2$,
		\item \B{sparse} if $m$ is much smaller than $n^2$. 
	\end{itemize}
\end{lemma}
\end{theorem}

\subsection{Graph traversal}

\begin{definition}
	A \B{traversal} is a strategy for visiting all vertices of a graph.
\end{definition} 

\end{document}
