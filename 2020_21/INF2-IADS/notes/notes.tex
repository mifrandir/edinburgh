\documentclass{article}
\usepackage{notes-preamble}
\mkfpmthms
\usepackage{pseudo}
\usepackage{multicol}
\usepackage{enumitem}

\begin{document}
\title{Introduction to Algorithms and Data Structures (YEAR2)}
\author{Franz Miltz}
\date{10 May 2021}
\maketitle
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak


\section{Asymptotics analysis}

\subsection{Formal definition}

\emph{Asymptotic theory} makes precise quantitive statements about efficiency of algorithms themselves.
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$ be functions. Then
	\text{$f\in o(g)$} if, and only if, 
	\begin{align*}
		\forall c>0,\:\exists N\st \forall n \geq N, f(n)<cg(n)
	\end{align*}
	where $c\in\R$ and $N,n\in\N$.
\end{definition}
\begin{theorem}
	Let $f:\N\to\R_{\geq 0}$ and let $o(f)$ refer to some
	function within the set $o(f)$. Then
	\begin{itemize}
		\item $co(f)=o(f)$ where $c\in\R$,
		\item $o(f) + o(f) = o(f)$.
	\end{itemize}
\end{theorem}
\begin{theorem}
	Let $f,r:\N\to\R_{\geq 0}$ and let $a,b\in\R$. Then
	\begin{align*}
		f=o(g) \Leftrightarrow af=o(bg).	
	\end{align*}
\end{theorem}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f=\omega(g)$ if, and only if, $g=o(f)$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in O(g)$ if, and only if,
	\begin{align*}
		\exists C > 0\st \exists N \st \forall n \geq N,\: f(n) \geq Cg(n)
	\end{align*}
	where $C\in\R$ and $N,n\in\N$.\\
	We call $g$ an \emph{asymptotic upper bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Omega(g)$ if, and only if, $g\in O(f)$.\\
	We call $g$ an \emph{asymptotic lower bound} for $f$.
\end{definition}
\begin{definition}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $f\in O(f) \cap \Omega(f)$.\\
	We call $g$ an \emph{asymptotic tight bound} for $f$.
\end{definition}
\begin{theorem}
	Let $f,g:\N\to\R_{\geq 0}$. Then $f\in\Theta(g)$ if, and only if, $g\in\Theta(f)$.
\end{theorem}
\begin{theorem}
	\begin{align*}
		\lg n! = \Theta(n \lg n).
	\end{align*}
\end{theorem}


\subsection{Recurrence relations}


\begin{theorem}[The Master Theorem]
	Assume a recurrence relation $T$ has the form
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(1) &\text{if $n\leq n_0$}\\
			aT(n/b) + \Theta(n^k) &\text{if $n>n_0$}
		\end{cases}.
	\end{align*}
	Then, with $e=\log_b a$, 
	\begin{align*}
		T(n) = \begin{cases}
			\Theta(n^e) &\text{if $e>k$}\\
			\Theta(n^k\lg n) &\text{if $e=k$}\\
			\Theta(n^k) &\text{if $e<k$}
		\end{cases}
	\end{align*}
\end{theorem}



\subsection{Cost models}

\begin{definition}
	The \emph{cost} of an algorithm is a quantity to measure its performance.\\
	The cost may be defined in different ways depending on
	how in depth the analysis is supposed to be and what
	is of interest in a particular situation.
\end{definition}
Note that the cost model needs to be specified when comparing algorithms.
\begin{definition}
	We considering the cost of an algorithm for a specific input size there are different cases to consider:
	\begin{itemize}
		\item \emph{worst-case} cost: the single worst cost out of all possible inputs
		\item \emph{best-case} cost: the single best cost out of all possible inputs
		\item \emph{average-case} cost: the average over all the costs for all possible inputs
	\end{itemize}
\end{definition}
\begin{theorem}
	Let $A$ be an algorithm and let $T_w$ be its worst-case runtime. Then, if $T_w=O(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $O(g)$.
\end{theorem}
\begin{theorem}
	Let $A$ be an algorithm and let $T_b$ be its best-case runtime. Then, if $T_b=\Omega(g)$ for some function $g$,
	we know that the runtime of $A$ \emph{in general} is $\Omega (g)$.
\end{theorem}

\section{Sorting}

\subsection{Bubblesort}

\begin{minted}{python}
def bubble_sort(arr):                       # O(n**2)
  for n in range(len(start), 2, -1):        # O(n) iterations 
    for i in range(n-1):                    # O(n) iterations
      if arr[i] > arr[i+1]:
        arr[i], arr[i+1] = arr[i+1], arr[i]
\end{minted}


\subsection{Insertion Sort}

\begin{minted}{python}
def insert_sort(arr):                       # O(n**2)
  for i in range(1, n):                     # O(n) iterations
    t = arr[i]
    for j in range(i-1,0,-1):               # O(n) iterations
      if arr[j] > x:
        break
      arr[j+1] = arr[j]
    arr[j+1] = x
\end{minted}

\subsection{Mergesort}

\begin{minted}{python}
def merge_sort(arr):                        # O(n*log(n)), Master Theorem
  n = len(arr)
  if n < 2:
    return arr
  left = merge_sort(arr[:n // 2])           # O(n//2*log(n//2))
  right = merge_sort(arr[n // 2:])          # O(n//2*log(n//2))
  return merge(left, right)                 # O(n)

# We're merging backwards and then reversing the list because
# popping from/appending to the front is very inefficient.
def merge(left, right):                     # O(n)
  merged = []                               # n = len(left) + len(right)
  while len(left) > 0 and len(right) > 0:   # O(n) iterations
    if left[-1] > right[-1]:
      merged.append(left.pop())
    else:
      merged.append(right.pop())
  merged.extend(left)                       # O(len(left)) = O(n)
  merged.extend(right)                      # O(len(right)) = O(n)
  return reversed(merged)                   # O(n)
\end{minted}

\subsection{Heapsort}

\begin{minted}{python}
def heap_sort(arr):                         # O(n*log(n))
  n = len(arr)
  heapify(arr, n)                           # O(n)
  for i in range(n):                        # O(n) iterations
    arr[-i] = extract_max(arr, n-1)         # O(log(n))
\end{minted}

\subsection{Quicksort}

Algorithm for input array $A$:
\begin{enumerate}
	\item If $|A|<2$, do nothing. Otherwise partition the array:
	\begin{tabular}{|c|c|c|}
		\hline
		$\leq$ pivot & pivot & $\geq$pivot\\
		\hline
	\end{tabular}
	\item Sort the resulting subarrays recursively.
\end{enumerate}

\begin{minted}{python}
def quick_sort(arr, p, r):                  # O(n**2)
  if p < r:
    split = partition(arr, p, r)            # O(n)
    quick_sort(arr, p, split-1)             # O(n**2)
    quick_sort(arr, split+1, r)             # O(n**2)

def partition(arr, p, r):                   # O(n), n=r-p
  pivot = arr[r].key
  i = p - 1
  for j in range(p, r):                     # O(n) iterations
    if arr[j] <= pivot:
      i += 1
      arr[i], arr[j] = arr[j], arr[i]
  arr[i+1], arr[j] = arr[j], arr[i+1]
  return i+1
\end{minted}

\subsection{Comparison}

\begin{center}
	\begin{tabular}{l | l | l | l}
		\textbf{Algorithm} & \textbf{Worst-case} & \textbf{Average-case} & \textbf{Best-case}
		\\\hline
		BubbleSort & $\Theta(n^2)$     & $\Theta(n^2)$     & $\Theta(1)$ swaps
		\\\hline
		InsertSort & $\Theta(n^2)$     & $\Theta(n^2)$     & $\Theta(1)$ swaps
		\\\hline
		MergeSort  & $\Theta(n\log n)$ & $\Theta(n\log n)$ & $\Theta(n\log n)$
		\\\hline
		QuickSort  & $\Theta(n^2)$     & $\Theta(n\log n)$ & $\Theta(n\log n)$
		\\\hline
		HeapSort   & $\Theta(n\log n)$ & $\Theta(n\log n)$ & $\Theta(n\log n)$
	\end{tabular}
\end{center}

\section{Collections}

\subsection{Lists}

\begin{definition}
	A list of items of type $X$ is a datastructure with the
	following interface:
	\begin{align*}
		\textbf{get} &: \Z \to X \\
		\textbf{set} &: \Z\times X\to\{*\}\\
		\textbf{cons} &: X\to \{*\}\\
		\textbf{append} &: X\to \{*\}\\
		\textbf{insert} &: \Z\times X \to \{*\}\\
		\textbf{delete} &: \Z\to \{*\}\\
		\textbf{length} &: \{*\} \to \Z
	\end{align*}
\end{definition}

\subsubsection{Fixed-size arrays}
\begin{multicols}{2}
\noindent Strengths:
\begin{itemize}
	\item fast \texttt{get} and \texttt{set} operations
	\item fixed, predictable size; good for memory management
\end{itemize}

\noindent Weaknesses:
\begin{itemize}
	\item limited size
	\item unsuitable for computations with unpredictable size
\end{itemize}

\begin{minted}{python}
def get(self, i):
  return self.arr[i]
\end{minted}

\begin{minted}{python}
def append(self, x):
  self.arr[self.n] = x
  self.n += 1
\end{minted}

\begin{minted}{python}
def insert(self, i, x):
  n += 1
  for j in range(n, i-1, -1):
    self.arr[j+1] = self.arr[j]
  self.arr[i] = x
\end{minted}
\end{multicols}

\subsubsection{Extensible arrays}

\begin{multicols}{2}


\noindent Strengths:
\begin{itemize}
	\item unlimited, variable size
	\item fast \texttt{get} and \texttt{set}
	\item fast amortised \texttt{append}
\end{itemize}

\noindent Weaknesses:
\begin{itemize}
	\item bad worst-case \texttt{append}
\end{itemize}

\begin{pseudo}
\textbf{function} \textsf{append}$(x)$:												\\+
  \textbf{if} $n=|A|$:																\\+
    $B \leftarrow \text{new array of size }\ceil{n\times r}$						\\
	copy contents of $A$ into $B$													\\
	$A\leftarrow B$																	\\-
  $A[n]\leftarrow x$																\\
  $n\leftarrow n + 1$
\end{pseudo}
\end{multicols}



\subsubsection{Linked list}

Strengths:
\begin{itemize}
	\item fast insertions and deletion at front 
	\item naturally allow for sharing
\end{itemize}

Weaknesses:
\begin{itemize}
	\item slow random access
	\item unpredictable structure in memory
\end{itemize}

\subsubsection{Comparison}

\begin{center}
\begin{tabular}{ l | l | l | l | l | l | l | l }
	\textbf{Implementation} 
	& \texttt{get} 
	& \texttt{set} 
	& \texttt{cons}
	& \texttt{append}
	& \texttt{insert}
	& \texttt{delete}
	& \texttt{length}
	\\
	\hline
	Fixed-size array
	& $O(1)$
	& $O(1)$
	& *$O(n)$
	& *$O(1)$
	& *$O(n)$
	& $O(n)$
	& $O(1)$
	\\
	\hline
	Extensible array
	& $O(1)$
	& $O(1)$
	& $O(n)$
	& $O(n)$, amortised $O(1)$
	& $O(n)$
	& $O(n)$
	& $O(1)$
	\\
	\hline
	Linked list
	& $O(n)$
	& $O(n)$
	& $O(1)$
	& $O(n)$
	& $O(n)$
	& $O(n)$
	& $O(1)$
\end{tabular}
\end{center}

\subsection{Stacks}

\begin{definition}
	A stack containing items of type $X$ is a datastructure with
	the following interface:
	\begin{align*}
		\textbf{empty} &: \{*\}\to\{0, 1\}\\
		\textbf{peek} &: X \to \{*\}\\
		\textbf{pop} &: \{*\}\to X\\
		\textbf{push} &: \{*\}\to X
	\end{align*}
\end{definition}

\begin{center}
\begin{tabular}{ l | l | l | l | l}
	\textbf{Implementation} 
	& \texttt{empty} 
	& \texttt{peek} 
	& \texttt{pop}
	& \texttt{push}
	\\
	\hline
	Extensible array
	& $O(1)$
	& $O(1)$
	& $O(1)$
	& $O(n)$, amortised $O(1)$
	\\
	\hline
	Linked list
	& $O(1)$
	& $O(1)$
	& $O(1)$
	& $O(1)$
\end{tabular}
\end{center}

\subsection{Queues}

\begin{definition}
	A queue containing items of type $X$ is a datastructure with
	the following interface:
	\begin{align*}
		\textbf{empty} &: \{*\}\to\{0, 1\}\\
		\textbf{peek} &: X \to \{*\}\\
		\textbf{dequeue} &: \{*\}\to X\\
		\textbf{enqueue} &: \{*\}\to X
	\end{align*}
\end{definition}

\begin{center}
\begin{tabular}{ l | l | l | l | l}
	\textbf{Implementation} 
	& \texttt{empty} 
	& \texttt{peek} 
	& \texttt{dequeue}
	& \texttt{enqueue}
	\\
	\hline
	Wraparound array
	& $O(1)$
	& $O(1)$
	& $O(1)$
	& $O(n)$, amortised $O(1)$
	\\
	\hline
	Doubly linked list
	& $O(1)$
	& $O(1)$
	& $O(1)$
	& $O(1)$
\end{tabular}
\end{center}

\subsection{Sets and dictionaries}


\begin{definition}
	A \emph{finite set} containing values of type $X$ is a datastructure with the following interface
	\begin{align*}
		\textbf{contains} &: X\to \{0,1\}\\
		\textbf{insert} &: X \to \{*\}\\
		\textbf{delete} &: X \to \{*\}\\
		\textbf{isEmpty} &: \{*\} \to \{0,1\}
	\end{align*}
\end{definition}
\begin{definition}
	A \emph{dictionary} mapping keys of type $X$ to values of type $Y$
	is a datastructure with the following interface
	\begin{align*}
		\textbf{lookup} &: X \to Y\\
		\textbf{insert} &: X \times Y \to \{*\}\\
		\textbf{delete} &: X \to \{*\}\\
		\textbf{isEmpty} &: \{*\} \to \{0,1\}
	\end{align*}
\end{definition}
\begin{definition}
	A \emph{hash table} is a datastructure that uses a \emph{hash function} $f: X \to [0,m-1]$
	to store key value pairs $(X,Y)$ in an array of length $m$.\\
	The \emph{load factor} $\alpha$ is the average number of elements associated with each cell
	of the underlying array. If $f$ is a perfect hash function, i.e. it maps all inputs uniformly
	onto $[0,m-1]$, then $\alpha$ is given by 
	\begin{align*}
		\alpha = n / m.
	\end{align*}
\end{definition}
\begin{proposition}
	Using linked lists to manage the elements in each bucket of a hash table, then
	\begin{itemize}
		\item a lookup of a key $k$  takes on average $\Theta(\alpha)$ comparisons, and
		\item a lookup of a key $k$ takes in the worst-case $\Theta(n)$ comparisons.
	\end{itemize}
\end{proposition}
\begin{definition}
	\emph{Open addressing} is an algorithm to handle hash collisions within a hash table 
	by using a hash function $f: (X, [0,m-1]) \to [0,m-1]$ that generates a permutation of all possible
	hash codes for each possible input $X$. An element $x\in X$ is then stored the hash code $f(x, i)$
	if and only if all $f(x,j)$ for $j < i$ are already occupied.
\end{definition}
\begin{proposition}
	In a hash table with load factor $\alpha$ that uses open addressing requires, on average, $1/(1-\alpha)$ 
	probes for an unsuccessful lookup and fewer for a successful one.
\end{proposition}
\begin{definition}
	A \emph{binary tree} consists of a value and at most one left child and at most one right child where
	children are other binary trees.
\end{definition}
\begin{definition}
	An \emph{ordered binary tree} with a given ordering $(<) : X\times X \to \{0, 1\}$ is a binary tree 
	with a root $x$ such that
	\begin{align*}
		\forall y \in L(x), y.\texttt{key} < x.\texttt{key} 
		\text{ and } \forall y \in R(x), x.\texttt{key} < y.\texttt{key},
	\end{align*}
	and both $L(x)$ and $R(x)$ are ordered.
\end{definition}

\begin{proposition}
	For a perfectly balanced ordered binary tree, the lookup time is $O(\log n)$.\\
	More generally, for any such tree with a maximum depth $d$ of at most $d=2 \lg n$ the
	lookup time is $O(\log n)$.
\end{proposition}

\begin{theorem}[L9]
	A \emph{red-black tree} is a ordered binary tree that follows the
	following rules:
	\begin{enumerate}
		\item Every node is either \texttt{red} or \texttt{black}.
		\item The root and all leaves are \texttt{black}.
		\item All paths from the root to a leaf contain the same number $b$ of \texttt{black} nodes.
		\item The children of a \texttt{red} node are \texttt{black}.
	\end{enumerate}
	For every tree with $n$ nodes that satisfies these conditions, 
	the height $h$ is bounded by
	\begin{align*}
		h\leq 2\lg(n+1)+1.
	\end{align*}
\end{theorem}

\paragraph{Comparison}
\begin{center}
\begin{tabular}{ l | l | l | l }
	\textbf{Implementation} & \texttt{contains} & \texttt{insert} & \texttt{delete}\\
	\hline
	Linked List (simple) 
	& $\Theta(n)$, $\Theta(n)$
	& $\Theta(1)$, $\Theta(1)$
	& $\Theta(n)$, $\Theta(n)$\\
	\hline
	Ordered Linked List (simple) 
	& $\Theta(\lg n)$, $\Theta(\lg n)$
	& $\Theta(\lg n)$, $\Theta(\lg n)$
	& $\Theta(\lg n)$, $\Theta(\lg n)$\\
	\hline
	Hash Table (linked bucket list)
	& $\Theta(\alpha)$, $\Theta(n)$
	& $\Theta(1)$, $\Theta(1)$
	& $\Theta(\alpha)$, $\Theta(n)$\\
	\hline
	Hash Table (open addressing)
	& $\Theta(1/(1-\alpha))$, $\Theta(n)$
	& $\Theta(1/(1-\alpha))$, $\Theta(n)$
	& - \\
	\hline
	Ordered Binary Tree
	& $\Theta(\lg n)$, $\Theta(n)$
	& $\Theta(\lg n)$, $\Theta(n)$ 
	& $\Theta(\lg n)$, $\Theta(n)$\\
	\hline
	Red-Black Tree
	& $\Theta(\lg n)$, $\Theta(\lg n)$
	& $\Theta(\lg n)$, $\Theta(\lg n)$
	& $\Theta(\lg n)$, $\Theta(\lg n)$
\end{tabular}
\end{center}


\subsection{The Heap data structure}


\begin{definition}
	A heap is an \textbf{almost-complete} binary tree:
	\begin{itemize}
		\item All leaves are either at depth $h-1$ or $h$ (where $h$ is the height of the tree)
		\item The depth-$h$ leaves all appear consecutively from left-to-right
	\end{itemize}
\end{definition}
\begin{lemma}
	The height $h$ of a heap with $n$ is in the range given by
	\begin{align*}
		\lg(n)-1 < h \leq \lg(n).
	\end{align*}
\end{lemma}
\begin{theorem}
	The heap operations have the following runtimes:
	\\
	\begin{center}
	\begin{tabular}{| l | l |}
		\hline
		Operation & Runtime\\
		\hline
		Max & $\Theta(1)$\\
		\hline
		Heapify & $O(\lg(n))$\\
		\hline
		Extract-Max & $O(\lg(n))$\\
		\hline
		Insert & $O(\lg(n))$\\
		\hline
		Build & $O(n)$\\
		\hline
	\end{tabular}
	\end{center}
\end{theorem}


\section{Graphs}


\subsection{Directed and Undirected Graphs}

\begin{definition}
	A graph $G$ is defined as
	\begin{align*}
		G = (V,E)
	\end{align*}
	where $V$ is the set of vertices and $E\subseteq V\times V$ is the set of edges.
\end{definition}

\begin{definition}
	A graph $G$ is \emph{undirected} if and only if
	\begin{align*}
		\forall v,w\in V,\: (v,w)\in E \Leftrightarrow (w,v)\in E.
	\end{align*}
	If a graph is not undirected, it is \emph{directed}.
\end{definition}

\subsection{Adjacency Matrices vs Lists}

\begin{definition}
	Let $G=(V,E)$ be a graph with $n$ vertices $V=\{v_1, ..., v_n\}$.
	Then the \emph{adjacency matrix} of $G$ is the $n\times n$ matrix
	$A=(a_{ij})_{i,j\in[n]}$ with 
	\begin{align*}
		a_{ij} = \begin{cases}
			1 &\text{if } (v_i,v_j)\in E\\
			0 &\text{otherwise}
		\end{cases}.
	\end{align*}
\end{definition}

\begin{theorem}
	Let $G=(V,E)$ with $|V|=n$ and $|E|=m$. Then the following 
	complexities apply:
	\begin{center}
	\begin{tabular}{| l | c | c |}
		\hline
		 Measure & Matrix & List\\
		\hline
		Space & $\Theta(n^2)$ & $\Theta(n+m)$ \\
		\hline
		Time to check if $w$ adjacent to $v$ & $\Theta(1)$ & $\Theta(\text{out}(v))$\\
		\hline
		Time to visit all $w$ adjacent to $v$ & $\Theta(n)$ & $\Theta(\text{out}(v))$\\
		\hline
		Time to visit all edges & $\Theta(n^2)$ & $\Theta(n+m)$\\
		\hline
	\end{tabular}
\end{center}
\begin{definition}
	A graph $G=(V,E)$ with $|V|=n$ and $|E|=m$ is said to be
	\begin{itemize}
		\item \emph{dense} if $m$ is close to $n^2$,
		\item \emph{sparse} if $m$ is much smaller than $n^2$. 
	\end{itemize}
\end{definition}
\end{theorem}

\subsection{Graph traversal}

\begin{definition}
	A \emph{traversal} is a strategy for visiting all vertices of a graph.
\end{definition} 

\begin{lemma}
	For a directed graph $G=(V,E)$ with $m$ edges,
	\begin{align*}
		m=\sum_{v\in V} \textsf{out-degree}(v).
	\end{align*}
	Similarly, for an undirected graph $G=(V,E)$ with $m$ edges,
	\begin{align*}
		m=\frac{1}{2}\sum_{v\in V}\textsf{deg}(v).
	\end{align*}
\end{lemma}

\begin{pseudo}
\textbf{function} \textsf{dfs}$(G)$:											\\+
	$\textsf{visited}\leftarrow\text{empty set}$								\\
	\textbf{for all} $v$ in $V$:												\\+
		\textbf{if} $v\not\in\textsf{visited}$:									\\+
		$\textsf{dfsFromVertex}(G, v, $\textsf{visited}$)$						\\---
																				\\
\textbf{function} \textsf{dfsFromVertex}$(G,v,\textsf{visited})$: 				\\+
	$S\leftarrow \textsf{empty stack}$											\\
	$S.\textsf{push}(v)$														\\
	\textbf{while not} $S.\textsf{isEmpty}()$:									\\+
		$u\leftarrow S.\textsf{pop}()$											\\
		\textbf{if} $u\in \textsf{visited}$:									\\+
			\textbf{continue}													\\-
		$\textsf{visited}.\textsf{insert}(u)$									\\
		\textbf{for all} $w$ adjacent to $u$:									\\+
			$S.\textsf{push}(w)$
\end{pseudo}

\begin{pseudo}
\textbf{function} \textsf{dfsRec}$(G)$:											\\+
	$\textsf{visited}\leftarrow\text{empty set}$								\\
	\textbf{for all} $v$ in $V$:												\\+
		\textbf{if} $v\not\in\textsf{visited}$:									\\+
		$\textsf{dfsFromVertexRec}(G, v, $\textsf{visited}$)$					\\---
																				\\
\textbf{function} \textsf{dfsFromVertexRec}$(G, v, $\textsf{visited}$)$:		\\+
    $\textsf{visited}.\textsf{insert}(v)$										\\
    \textbf{for all} $w$ adjacent to $v$:										\\+
		\textbf{if} $w\not\in\textsf{visited}$:									\\+
			\textsf{dfsFromVertexRec}$(G, w, \textsf{visited})$
\end{pseudo}

\begin{definition}
	A vertex $w$ is a \emph{child} of a vertex $v$ in the DFS
	forest if \textsf{dfsFromVertex}$(G,w)$ is called from
	\textsf{dfsFromVertex}$(G,v)$.
\end{definition}

\subsection{Topological sorting}

\begin{definition}
	Let $G=(V,E)$ be a directed graph. A \emph{topological order of $G$} is a
	total order $\prec$ of the vertex set $V$ such that for all edges $(v,w)\in E$
	we have $v\prec w$.
\end{definition}

\begin{definition}
	A \emph{directed acyclic graph} (DAG) is a digraph without cycles.
\end{definition}

\begin{theorem}
	A digraph has a topological order iff it is a DAG.
\end{theorem}

\begin{lemma}
	Let $G$ be a graph and $v$ a vertex of $G$. Consider the moment
	during the execution of \textsf{dfs}$(G)$ when \textsf{dfsFromVertex}$(G,v)$
	is started. We call a vertex $w$ \emph{finished} iff \textsf{dfsFromVertex}$(G,w)$
	has been started and terminated. Then for all vertices $w$ we have:
	\begin{enumerate}
		\item If $w$ is not yet visited and reachable from $v$, then $w$ will be
		finished before $v$.
		\item If $w$ is visited, but not yet finished, then $v$ is reachable from $w$.
	\end{enumerate}
\end{lemma}

\begin{theorem}
	Let $G=(V,E)$ be a digraph. We define an order $\prec$ on $V$ as follows:
	\begin{align*}
		v\prec w \hs\text{ if and only if}\hs\text{$w$ becomes finished before $v$}.
	\end{align*}
	If $G$ is a DAG, then $\prec$ is a topological order.
\end{theorem}

\begin{pseudo}
\textbf{function} \textsf{topSort}$(G=(V,E))$:										\\+
	$\textsf{state}\leftarrow\text{array of $\abs{V}$ colours
		initialised to \textsf{white}}$												\\
	$L\leftarrow\text{empty linked list}$											\\
	\textbf{for all} $v\in V$:														\\+
		\textbf{if} $\textsf{state}[v] = \textsf{white}$:							\\+
			\textsf{softFromVertex}$(G,v,L)$										\\--
    \textbf{return} $L$																\\-
\end{pseudo}

\begin{pseudo}
\textbf{function} \textsf{softFromVertex}$(G,v,L)$:									\\+
	$\textsf{state}[v]\leftarrow\textsf{grey}$										\\
	\textbf{for all} $w$ adjacent to $v$:											\\+
		\textbf{if} $\textsf{state}[w] = \textsf{white}$:							\\+
			\textsf{sortFromVertex}$(G,w)$											\\-
		\textbf{else if} $\textsf{state}[w] = \textsf{grey}$:						\\+
			\textbf{fail} "Cycle detected" 											\\--
	$\textsf{state}[v]\leftarrow \textsf{black}$									\\
	$L.\textsf{cons}(v)$
\end{pseudo}

\subsection{Connected components}

\begin{definition}[L15]
	Let $G=(V,E)$ be an undriected graph. Then a subset $C\subseteq V$ is
	(\emph{strongly}) \emph{connected} if for all $v,w\in C$ there is a path 
	from $v$ to $w$. A \emph{connected component} of $G$ is a \emph{maximum
	connected subset} $C\subseteq V$, i.e. there is no connected subset
	$C'\subseteq V$ such that $C'\subset C$. $G$ is \emph{connected} if
	it only has one connected component.
\end{definition}

\begin{corollary}
	Let $G=(V,E)$ be undirected. Each vertex $v\in V$ is contained in exactly
	one connected component. Further, the connected component that contains $v$
	is precisely the set of all vertices that are reachable from $v$. 
\end{corollary}

\begin{pseudo}
\textbf{function} \textsf{connectedComponent}$(G)$:									\\+
	$\textsf{visited}\leftarrow\text{empty set}$									\\
	$\textsf{components}\leftarrow\text{empty set}$									\\
	\textbf{for all} $v$ in $V$:													\\+
		\textbf{if} $v\not\in\textsf{visited}$:										\\+
			$C\leftarrow\textsf{empty set}$											\\
			$\textsf{ccFromVertex}(G, v, $\textsf{visited}$, C)$					\\
			$\textsf{components}.\textsf{insert}(C)$								\\--
	\textbf{return} \textsf{components}												\\-
																					\\
\textbf{function} \textsf{ccFromVertex}$(G, v, $\textsf{visited}$, C)$:				\\+
    $\textsf{visited}.\textsf{insert}(v)$											\\
	$C.\textsf{insert}(v)$															\\
    \textbf{for all} $w$ adjacent to $v$:											\\+
		\textbf{if} $w\not\in\textsf{visited}$:										\\+
			\textsf{ccFromVertex}$(G, w, \textsf{visited}, C)$
\end{pseudo}
	
\section{Dynamic programming}

\subsection{Principles}
\begin{definition}[L16]
	The \emph{principles of dynamic programming} are:
	\begin{enumerate}[label=(dp\arabic*)]
		\item Need is to see that computing the optimum solution for our original 
			instance can be achieved by finding solutions to problems of the same 
			type, and combining them.
		\item Solution needs to be expressible in terms of a \emph{recurrence}, 
			where the RHS contains one or more recursive calls for smaller 
			instances of the same problem.
		\item We need to be able to organise storage for the results for all 
			possible subproblems.
		\item We need an algorithm to control the order in which subproblems 
			are solved.
	\end{enumerate}
\end{definition}

\subsection{The coin-changing problem}

\begin{definition}[L16]
	Given a value $v\in\N_0$, plus a sequence of coin values $c_1,c_2,...,c_k\in\N_0$,
	find a \emph{multiset} $S$ of coins whose values sum to $v$, whose cardinality
	of $S$ is the minimum possible for $v$ is this coin system.
\end{definition}

\begin{pseudo}
\textbf{function} \textsf{dpCoinChanging}$(v, c_1, ..., c_k)$:					\\+
	$c\leftarrow\text{array of length $k$ holding $c_1, ..., c_k$}$				\\
	$C,P\leftarrow\text{arrays of length $v+1$ containing $\infty$}$			\\
	$C[0]\leftarrow 0,C[1]\leftarrow 1$											\\
	\textbf{for} $w$ \textbf{from} $2$ \textbf{to} $v$:							\\+
		\textbf{for} $i$ \textbf{from} $1$ \textbf{to} $k$:						\\+
			\textbf{if} $c[i]\leq w$ and $C[w-c[i]]+1<C[w]$:					\\+
				$C[w]\leftarrow 1 + C[w-c[i]]$									\\
				$P[w]\leftarrow i$												\\---
	$S\leftarrow\text{array of length $k$ containing $0$}$						\\
	\textbf{while} $v>0$:														\\+
		$i\leftarrow P[v]$														\\
		$S[i]\leftarrow S[i]+1$													\\
		$v\leftarrow v - c[i]$													\\-
	\textbf{return} $S$
\end{pseudo}

\begin{corollary}
	The worst-case running time of \textsf{dpCoinChanging}$(v, c_1, ..., c_k)$
	is $O(vk)$.
\end{corollary}

\subsection{Seam Carving}

\begin{definition}[Seams; L17]
	Given an $m\times n$ matrix $A$, a \emph{vertical seam} is a finite sequence 
	$(s_k)_{k\leq m}$ of natural numbers such that
	\begin{align*}
		\forall i\leq m,\: 1 < s_i < n\hs\text{and}\hs\forall i<m,\: \abs{s_i - s_{i+1}} \leq 1.
	\end{align*}
	A \emph{horizontal seam} of $A$ is a finite sequence $(s_k)_{k\leq n}$ of 
	natural numbers such that
	\begin{align*}
		\forall j\leq n,\: 1 < s_j < m\hs\text{and}\hs\forall j<n,\: \abs{s_j - s_{j+1}} \leq 1.
	\end{align*}
\end{definition}

\begin{definition}[L17]
	Let $A$ be an $m\times n$ matrix and let $e_A:[m]\times[n]\to\R$ be an
	energy function. Then the \emph{energy} of a seam $s=(s_k)_{k\in\N}$ is defined as
	\begin{align*}
		e(s) := \begin{cases}
			\sum_{i=1}^m e_A(i,s_i) &\text{if } s \text{ is a vertical seam},\\
			\sum_{j=1}^m e_A(s_j,j) &\text{if } s \text{ is a horizontal seam}.
		\end{cases}
	\end{align*}
\end{definition}

\begin{definition}[L17]
	$L_1$ gradient scoring can be written as
	\begin{align*}
		e_A(i,j) := \abs{\frac{\p}{\p x} A}_{ij} + \abs{\frac{\p}{\p y} A}_{ij}
	\end{align*}
	where $\p/\p x$ and $\p/\p y$ are gradient functions defined in the image
	processing context, e.g. by using the \emph{Sobal operators}
	\begin{align*}
		\begin{bmatrix}
			-1 & 0 & +1 \\
			-2 & 0 & +2 \\
			-1 & 0 & +1
		\end{bmatrix}
		\hs \text{and} \hs
		\begin{bmatrix}
			-1 & -2 & -1 \\
			0  &  0 &  0 \\
			+1 & +2 & +1
		\end{bmatrix}
	\end{align*}
\end{definition}

\begin{theorem}
	Let $A$ be an $m\times n$ matrix and let $e_A$ be an energy function for
	$A$ such that
	\begin{align*}
		\forall i \leq m,\: e_A(i,1) = e_A(i,n) = \infty.
	\end{align*}
	Then, for every $\lra{i,j}\in[m]\times[n]$, the minimum cost $c_A(i,j)$ 
	of a vertical seam ending in $\lra{i,j}$ is given by
	\begin{align*}
		c_A(i,j) = e_A(i,j) + \begin{cases}
			0 &\text{if }i=1,\\
			m_{i-1,j} &\text{if }i>1
		\end{cases}
	\end{align*}
	where $m_{i,j}$ is
	\begin{align*}
		m_{i,j} = \min\{c_A(i,j-1), c_A(i,j), c_A(i,j+1)\}.
	\end{align*}
\end{theorem}

\begin{pseudo}
\textbf{function} \textsf{dpVerticalSeam}$(A, m, n)$:						\\+
	$\textsf{opt},p\leftarrow\text{arrays of size $m\times n$}$ (1-based)	\\
	\textbf{for $j$ from $1$ to $n$}:										\\+
		\textbf{for $i$ from $1$ to $m$}:									\\+
			$e[i,j]\leftarrow e_A(i,j)$										\\-
		$\textsf{opt}[1,j]\leftarrow e[1,j]$								\\
		$p[1,j]\leftarrow 0$												\\-
	\textbf{for $i$ from $1$ to $m$}:										\\+
		\textbf{for $j$ from $1$ to $n$}:									\\+
			$\textsf{opt}[i,j]\leftarrow\textsf{opt}[i-1,j]$				\\
			$p[i,j]\leftarrow 0$											\\
			\textbf{if} $\textsf{opt}[i-1,j-1]<\textsf{opt}[i,j]$:			\\+
				$\textsf{opt}[i,j]\leftarrow\textsf{opt}[i-1,j-1]$			\\
				$p[i,j]\leftarrow -1$										\\-
			\textbf{if} $\textsf{opt}[i-1,j+1]<\textsf{opt}[i,j]$:			\\+
				$\textsf{opt}[i,j]\leftarrow\textsf{opt}[i-1,j+1]$			\\
				$p[i,j]\leftarrow +1$										\\-
			$\textsf{opt}[i,j]\leftarrow\textsf{opt}[i,j]+e[i,j]$			\\--
	$j'\leftarrow 2$														\\
	\textbf{for $j$ from $1$ to $n$}:										\\+
		\textbf{if} $\textsf{opt}[m,j]<\textsf{opt}[m,j']$:					\\+
			$j'\leftarrow j$												\\--
	\textbf{return} $\lra{m,j'}, p$

\end{pseudo}

\begin{corollary}[L17]
	The worst-case running time of \textsf{dpVerticalSeam}$(A,m,n)$ is $O(mn)$.
\end{corollary}

\subsection{All-pairs shortest paths}

\begin{definition}[All-pairs shortest paths problem; L18]
	Given a graph or digraph $G=(V,E)$ together with a weight function
	$w:E\to\R$, compute an $n\times n$ matrix $D$ such that $D[i,j]$
	is the value of the shortest path w.r.t. $w$ from $i$ to $j$, for
	every $0\leq i,j\leq n-1$.	
\end{definition}

\begin{definition}[L18]
	Let $u,v\in V$ and suppose we have a path $p=e_1,...,e_{\abs p}$ from
	$u$ to $v$. Then the cost $d_p$ of this path is 
	\begin{align*}
		d_p = \sum_{h=1}^{\abs p} w(e_h).
	\end{align*}
\end{definition}

\begin{lemma}[L18]
	Let $i,j\in V$ and suppose that there is at least one path from $u$ to $v$
	in $G=(V,E)$. Assume there is no cycle of total negative weight in $G$, $w$.
	Then the mminimum-cost path from $u$ to $v$ has length at most $n-1$.
\end{lemma}

\begin{definition}[L18]
	Let $G=(V,E)$. Then let $V=\{v_1, ..., v_n\}$ and
	\begin{align*}
		V_k = \{v_i\in V : i\leq k\}.
	\end{align*}
	Further, let $P_k$ be the set of paths in $G$ with interior
	restricted to $V_k$. Then, for $0\leq i,j<n$ and $k\geq 0$, let
	\begin{align*}
		d_{ij}^{<k}=\begin{cases}
			0 & 
			i=j, \\
			\min\{d_p:p\in P_k, p\text{ is from $v_i$ to $v_j$}\} & 
			i\not=j, \text{path from $v_i$ to $v_j$ exists in $P_k$}, \\
			\infty &
			i\not=j, \text{path from $v_i$ to $v_j$ does not exist in $P_k$}. 
		\end{cases}
	\end{align*}
	Then we define $D^{<k}=(d_{ij}^{<k})_{0\leq i,j\leq n-1}$. We call $D^{<k}$
	the matrix of distances via $V_k$-restridcted paths.
\end{definition}

\begin{theorem}[L18]
	Let $0\leq i,j<n$. If $k>0$, then 
	\begin{align*}
		D^{<k+1}[i,j]=\begin{cases}
			0 & \text{if $i=j$}, \\
			\min\{D^{<k}[i,j],D^{<k}[i,k]+D^{<k}[k,j]\} & \text{otherwise}.
		\end{cases}
	\end{align*}
	Further,
	\begin{align*}
		D^{<0}[i,j] = \begin{cases}
			0 		& \text{if $i=j$}, \\
			w(i,j)  & \text{if $i\not=j$, $(i,j)\in E$}, \\
			\infty  & \text{if $i\not=j$, $(i,j)\not\in E$}.
		\end{cases}
	\end{align*}
	For all $x\in\R$ we define $x+\infty = \infty + x = \infty + \infty = \infty$,
	$\min\{\infty, \infty\}=\infty$ and $\min\{\infty, x\}=x$.
\end{theorem}

\begin{pseudo}
\textbf{function} \textsf{FloydWarshall}$(G=(V,E), w)$:\\+
	$D^{<0}, ..., D^{<n+1}\leftarrow \text{arrays of size $|V|\times |V|$}$	\\
	\textbf{for $k$ from $0$ to $n-1$}:										\\+
		\textbf{for $i$ from $0$ to $n-1$}:									\\+
			\textbf{for $j$ from $0$ to $n-1$}: 							\\+
				$D^{<k+1}[i,j]\leftarrow \min
					\{D^{<k}[i,j],D^{<k}[i,k]+D^{<k}[k,j]\}$				\\---
	\textbf{return} $D^{<n+1}$
\end{pseudo}

\begin{theorem}
	\textsf{FloydWarshall}$(G=(V,E), w)$ runs in $\Theta(\abs{V}^3)$ time.
\end{theorem}

\subsection{Probabilistic finite-state machines}

\begin{definition}[L19]
	A \emph{probabilistic FSM} is a finite-state machine of the form
	$M=\lra{Q, \Sigma, q_0, F, \delta}$ with $\Delta\subseteq Q\times\Sigma\times Q$,
	together with a \emph{probability label} $p_{q,a,q'}\in[0,1]$ for every
	$\lra{q,a,q'}\in\Delta$ such that for every $q\in Q$, $a\in\Sigma$, we have
	\begin{align*}
		\sum_{q'\in Q, \lra{q,a,q'}\in\Delta} p_{q,a,q'}=1.
	\end{align*}
\end{definition}

\begin{definition}[L19]
	A \emph{Hidden Markov Model} is a state-machine $M=\lra{Q, \Sigma, \Delta, P, 
	\{b_q:q\in Q\}, \pi}$ with $\Delta\subseteq Q\times Q$, together with
	\begin{itemize}
		\item A $\abs Q \times \abs Q$ \emph{transition matrix} $P$ dfining a
			probability label $p_{q,q'}\in[0,1]$ for every $\lra{q,q'}\in\Delta$
			such that for every $q\in Q$ we have \begin{align*}
				\sum_{q'\in Q}p_{q,q'} = 1.
			\end{align*}
        \item A probability distribution $b_q$ on $\Sigma$ for every $q\in Q$,
			$b_q$ defining the distribution of \emph{emission} from $\Sigma$
			associated with state $q$. For all $q\in Q$ we require \begin{align*}
				\sum_{a\in\Sigma} b_q(a) = 1.
			\end{align*}
        \item A distribution $\pi$ to describe the starting state.
	\end{itemize}
\end{definition}

\begin{definition}[Max likelihood path]
	Given a HMM defined by $M=\lra{Q,\Sigma,\Delta,P,\{b_q:q\in Q\},\pi}$, and
	a sequence $s\in\Sigma^*$, what is the \emph{most likely path} through $M$
	to have generated $s$?	
\end{definition}

\begin{theorem}[L19]
	The max likelihood path through a HMM given by $M=\lra{Q,\Sigma,\Delta,
	P,\{b_q:q\in Q\},\pi}$ to have generated a string $s$ can be calculated 
	using the algorithm below.
	\begin{pseudo}
		\textbf{function} \textsf{Viterbi}$(M=\lra{Q,\Sigma,\Delta, P,
			\{b_q:q\in Q\},\pi}, s=s_1...s_n)$:									\\+
			$\textsf{mlp}, \textsf{prev}
				\leftarrow \text{arrays of size $n\times\abs Q$}$				\\
			\textbf{for} $q\in Q$:												\\+
				$\textsf{mlp}[1,q]\leftarrow\pi_q\times b_{q,s_1}$				\\-
			\textbf{for $i$ from $2$ to $n$}:									\\+
				\textbf{for $q\in Q$}:											\\+
					$\textsf{mlp}[i,q]\leftarrow 0$								\\
					\textbf{for $q'\in Q$}:										\\+
						$\textsf{trans}\leftarrow p_{q',q}\times b_{q,s_i}$	\\
						\textbf{if} $\textsf{trans}\times\textsf{mlp}[i-1,q']>
							\textsf{mlp}[i,q]$:									\\+
							$\textsf{mlp}[i,q]\leftarrow
								\textsf{trans}\times\textsf{mlp}[i-1,q']$		\\
							$\textsf{prev}[i,q]\leftarrow q'$					\\----
			$\textsf{max}\leftarrow 0$											\\
			\textbf{for $q\in Q$}:												\\+
				\textbf{if} $\textsf{mlp}[n,q]>\textsf{max}$:					\\+
					$\textsf{max}\leftarrow\textsf{mlp}[n,q]$					\\--
			\textbf{return} \textsf{max}
	\end{pseudo}
	$\textsf{Viterbi}(M, s=s_1\cdots s_n)$ runs in $\Theta(n\cdot\abs{Q}^2)$ time
	and takes $\Theta(n\cdot\abs Q)$ space.
\end{theorem}

\subsection{Edit distance}

\begin{definition}
	An \emph{alignment} of two sequences $s\in\Sigma^m$, $t\in\Sigma^n$ is
	any padding (with some $-$ insertions) $s'$ of $s$, and $t'$ of $t$ such that
	\begin{align*}
		\abs{s'}&=\abs{t'},\\
		(s'_i\not=-)&\text{ or }(t'_i\not=-)\hs\text{for all }i\in[|s'|].
	\end{align*}
\end{definition}

\begin{definition}
	The score of an alignment is the total number of \emph{insertions},
	\emph{deletions} and \emph{substitutions}.
\end{definition}

\begin{definition}
	The \emph{edit distance} $d(s,t)$ between two strings $s,t\in\Sigma^*$ is the
	minimum number of operations possible for an alignment of those strings.
\end{definition}

\begin{theorem}
	The edit distance $d$ between two strings $s_m=(s_k)_{k\leq m}$ 
	and $t_n=(t_k)_{k\leq n}$ is given by 
	\begin{align*}
		d(s_m,t_n) = \begin{cases}
			m &\text{if }n=0,\\
			n &\text{if }m=0,\\
			d(s_{m-1}, t_{n-1}) &\text{if }s_m=t_n,\\
			1 + h_{m,n} &\text{if }s_m\not=t_n
		\end{cases}
	\end{align*}
	where $h$ is given as
	\begin{align*}
		h_{m,n}= 1 + \min\{d(s_{m-1},t_{n-1}), d(s_{m-1}, t_n), d(s_m, t_{n-1})\}.	
	\end{align*}
\end{theorem}

\subsection{Parsing for context free grammars}

\begin{definition}
	A context-free grammar is in \emph{Chomsky normal form} if for
	each production the right-hand side consists of
	\begin{itemize}
		\item \emph{either} just two non-terminals (e.g $X\to YZ$)
		\item \emph{or} just one terminal (e.g. $X\to +$).
	\end{itemize}
\end{definition}

\begin{lemma}
	Every context-free grammar can be transformed into an
	equivalent one in Chomsky normal form (disregarding
	the empty string).
\end{lemma}

\begin{definition}
	The \emph{CYK algorithm} allows parsing of a context-free
	grammar in Chomsky normal form.
	\begin{pseudo}
		\textbf{function} \textsf{CYK}$(s,G)$:\\+
			$n\leftarrow\text{length}(s)$\\
			$\text{table}\leftarrow\text{Matrix }A_{ij}\text{ for }(i,j)\in[0,n-1][n]$\\
			\textbf{for} $j\in\{1,..,n\}$:\\+
				\textbf{for} $(X\to t)\in G$:\\+
					\textbf{if} $t=s[j-1]$:\\+
						add $X$ to table$[j-1,j]$\\--
				\textbf{for} $i\in\{j-2,..,0\}$:\\+
					\textbf{for} $k\in\{i+1,..,,j-1\}$:\\+
						\textbf{for} $(X\to YZ)\in G$:\\+
							\textbf{if} $Y\in\text{table}[i,k]$ and $Z\in\text{table}[k,j]$:\\+
								add $X$ to table$[i,j]$\\-----
            \textbf{return} table
    \end{pseudo}
\end{definition}

\begin{theorem}
	Given a grammar $G$ in Chomsky normal form with $n$ 
	productions and an input of length $n$, the run time
	of the CYK algorithm is $O(mn^3)$.
\end{theorem}

\section{P and NP}

\subsection{Decision problems}

\begin{definition}
	A computational problem $Q$ is solvable in \emph{polynomial
	time} if there is some fixed $r\in\R$, and some deterministic
	algorithm $A$, which returns a correct solution for every
	instance $\mathcal{J}$ in time at most $O(\abs{\mathcal{J}}^r)$.
\end{definition}

\begin{definition}
	A computational problem $Q$ is a \emph{decision problem} if
	it can be described in terms of a collection of potential
	solutions $S$, where $Q(\mathcal{J})=1$ if there is a solution
	in $S$ which solves the instance $\mathcal{J}$ and $Q(\mathcal{J})=0$
	otherwise.
\end{definition}

\subsection{Complexity classes}

\begin{definition}
	The \emph{complexity class P} is the class of decision problems
	$Q$ for which there is a polynomial-time aglorithm to compute $Q$
	exactly on all input instances.
\end{definition}

\begin{definition}
	Consider a decision problem $Q$ with respect to its collection
	of potential solutions $S$. We say that a two-parameter algorithm 
	$A$ is a \emph{verifier} for $Q$ iff for all instances
	$\mathcal{J}$ of $Q$
	\begin{align*}
		\exists y\in S.\hs A(\mathcal{J},y)=1\hs\Leftrightarrow\hs Q(\mathcal{J})=1
	\end{align*}
\end{definition}

\begin{definition}
	The \emph{complexity class NP} is the class of decision problems $Q$
	for which there is a verifier $A=A(\mathcal{J}, y)$ which runs in time
	polynomial in the size $\abs{\mathcal{J}}$ of the instance.
\end{definition}

\subsection{Reductions between problems}

\begin{definition}
	A problem $R$ can be \emph{reduced} to the problem $Q$ if there
	is a polynomial-time computational function $f:\{0,1\}^*\to\{0,1\}^*$
	such that for all instances $\mathcal{J}$ of $R$
	\begin{align*}
		R(\mathcal{J})=1 \hs\Leftrightarrow\hs Q(f(\mathcal{J}))=1.
	\end{align*}
	We write $R\leq_P Q$.
\end{definition}

\begin{theorem}
	Let $Q,R$ be decision problems such that $R\leq_p Q$ and
	$Q\in P$. Then $R\in P$.
\end{theorem}

\begin{definition}
	A decision problem $Q$ is said to be \emph{NP-complete} if it
	belongs to the class NP, and it is also the case that for every
	problem $R$ in NP, $R\leq_P Q$.
\end{definition}

\subsection{SAT}

\begin{definition}
	We say a propositional logic formula $\Phi$ over the variables
	$\{x_1, ..., x_n\}$ is in \emph{Conjunctive Normal Form} (CNF)
	if it is written in the form
	\begin{align*}
		\Phi = C_1 \wedge C_2 \wedge \cdots \wedge C_m
	\end{align*}
	where each of the clauses $C_1, ..., C_m$ is a disjunction
	of literals over $\{x_1,...,x_m\}$.\\
	The corresponding decision problem SAT is:
	Given a CNF formula $\Phi=C_1\wedge\cdots\wedge C_m$ over
	variables $\{x_1,...,x_n\}$, determine whether there is some
	satisfying assignment for $\Phi$.	
\end{definition}

\begin{theorem}[Cook-Levin]
	SAT is NP-complete.	
\end{theorem}

\begin{definition}
	The CNF formula $\Phi$ is said to be \emph{3-CNF} if each of its clauses
	$C_j$ is a disjunction of exactly three literals.\\
	The corresponding decision problem 3-SAT is:
	Given a 3-CNF formula $\Phi$ over the variables $\{x_1,...,x_n\}$,
	determine whether there exists an assignment of binary values to
	$\{x_1,...,x_n\}$ that causes all clauses to be satisfied.
\end{definition}

\begin{theorem}
	3-SAT is NP-complete.
\end{theorem}

\subsection{Independent sets}

\begin{definition}
	Given an undirected graph $G=(V,E)$, an \emph{independent set} is
	a subset $I\subseteq V$ such that for every pair $u,v\in I$,
	$(u,v)\not\in E$. The size of such an independent set is the
	the cardinality $\abs I$.\\
	The related decision problem INDEPENDENT-SET is: Given an undirected graph $G=(V,E)$,
	and a natural number $k\in\N$, determine whether $G$ has an independent
	set of size $\geq k$.
\end{definition}

\begin{proposition}
	INDEPENDENT-SET is NP-complete.
\end{proposition}


\section{Dealing with NP-completeness}


\paragraph{General possibilities}
\begin{itemize}
	\item heuristics
	\item polynomial-time algorithm for approximation
	\item brute-force in exponential time
	\item recursive backtracking
\end{itemize}

\subsection{Polynomial-time approximation}

\begin{definition}[$\alpha$-approximation]
	Consider some optimisation problem \textsf{OPT} where for a given 
	instance $\mathcal{J}$, and the set of feasible solutions $y$,
	$\textsf{OPT}(\mathcal{J})$ is the cost/value of the optimum $y$.\\
	An algorithm $A$ is said to be an $\alpha$-approximation for \textsf{OPT}
	if for every instance $\mathcal{J}$, the algorithm returns a value
	$A(\mathcal{J})$ satisfying
	\begin{align*}
		A(\mathcal{J}) \begin{cases}
			\leq \alpha \cdot \textsf{OPT}(\mathcal{J}) &\text{if \textsf{OPT} is a minimisation problem},\\
			\geq 1/\alpha \cdot \textsf{OPT}(\mathcal{J}) &\text{if \textsf{OPT} is a maximisation problem}.
		\end{cases}
	\end{align*}
\end{definition}

\begin{definition}[Vertex cover]
	Given an undirected graph $G=(V,E)$, a subset $V'\subseteq V$ is a
	\emph{vertex cover (VC)} for $G$ if every edge $e\in E$ has at least one
	endpoint in $V'$.
\end{definition}

\begin{theorem}
	The following algorithm is a 2-approximation for the vertex cover
	problem.	
	\begin{pseudo}
	\textbf{function} \textsf{Approx-Vertex-Cover}$(G=(V,E))$:\\+
		$C\leftarrow \emptyset$\\
		$E'\leftarrow\emptyset$\\
		\textbf{while} $E'\not=\emptyset$:\\+
			$(u,v)\leftarrow\textsf{some edge in }E'$\\
			$C\leftarrow C\cup\{u,v\}$\\
			$E'\leftarrow E'\setminus\{(a,b)\in E : \{a,b\}\cap\{u,v\}\not=\emptyset\}$\\-
		\textbf{return} $C$
	\end{pseudo}
\end{theorem}

\begin{definition}[MAX 3-SAT]
	Given a 3-CNF formula $\Phi=C_1\wedge\cdots\wedge C_m$ over the variables
	$x_1, ..., x_n$, determine the maximum number $k$ of clauses $C_i$ such 
	that there exists an assignment of binary values to $x_1, ..., x_n$ that 
	makes $k$ clauses satisfied.
\end{definition}

\begin{theorem}
	The following algorithm is a $7/8$-approximation for \textsf{MAX 3-SAT}:
	\begin{pseudo}
	\textbf{function} \textsf{Greedy-3-SAT}$(\Phi, n, m)$:\\+
		\textbf{for} $i\in\{1,...,n\}$:\\+
			$E_0 \leftarrow E(Y|x_1=b_1, ...,x_{i-1}=b_{i-1},x_i=0)$\\
			$E_1 \leftarrow E(Y|x_1=b_1, ...,x_{i-1}=b_{i-1},x_i=1)$\\
			\textbf{if} $E_0\geq E_1$:\\+
				$b_i\leftarrow 0$\\-
			\textbf{else}:\\+
				$b_i\leftarrow 1$\\-
			Update $\Phi$ by fixing $x_i=b_i$\\-
		\textbf{return }$\vec b$
	\end{pseudo}
\end{theorem}

\begin{theorem}
	The \emph{maximum independent set} problem cannot be approximated to
	any constant approximation factor $\alpha\in\R$, assuming $P\not= NP$.
\end{theorem}

\subsection{Exhaustive search}

\subsubsection{Na\"ive backtracking}

\begin{pseudo}
\textbf{function} \textsf{SAT-backtrack}$(\Phi=C_1\wedge\cdots\wedge C_m, \mathcal{J}, b)$:\\+
	\textbf{if} $m=0$:\\+ \textbf{return} \textsf{true}\\-
	\textbf{if} $\exists$ empty clause in $\Phi$:\\+
		\textbf{return} \textsf{false}\\-
    $i\leftarrow\text{some index in $[n]\setminus\mathcal{J}$}$\\
	$\Phi'\leftarrow\Phi(x_i\leftarrow 0)$\\
	\textbf{if} \textsf{SAT-backtrack}$(\Phi', \mathcal{J}\cup\{i\}, b\cup\{x_i=0\})$:\\+
		\textbf{return} \textsf{true}\\-
	$\Phi'\leftarrow\Phi(x_i\leftarrow 1)$\\
	\textbf{return} \textsf{SAT-backtrack}$(\Phi',\mathcal{J}\cup\{i\}, b\cup\{x_i=1\})$
\end{pseudo}

\subsubsection{Davis, Putnam, Logemann, Loveland (DPLL)}

\begin{pseudo}
\textbf{function} \textsf{DPLL}$(\Phi=C_1\wedge\cdots\wedge C_m)$:\\+
	\textbf{if} every literal in $\Phi$ is pure:\\+\textbf{return} \textsf{true}\\-
	\textbf{if} some clause in $\Phi$ is empty:\\+\textbf{return} \textsf{false}\\-
	\textbf{while} $\exists$ unit clause $l$ in $\Phi$:\\+
		\textbf{if} $l$ is $x_i$:\\+$\Phi\leftarrow\Phi(x_i\leftarrow 1)$\\-
		\textbf{else if} $l$ is $\neg x_i$:\\+$\Phi\leftarrow\Phi(x_i\leftarrow 0)$\\--
	\textbf{while} $\exists$ pure literal $l$ in $\Phi$:\\+
		\textbf{if} $l$ is $x_i$:\\+$\Phi\leftarrow\Phi(x_i\leftarrow 1)$\\-
		\textbf{else if} $l$ is $\neg x_i$:\\+$\Phi\leftarrow\Phi(x_i\leftarrow 0)$\\--
    $x\leftarrow\text{undetermined variable in }\Phi$\\
	\textbf{return} \textsf{DPLL}$(\Phi(x\leftarrow 0))$ 
	\textbf{or}     \textsf{DPLL}$(\Phi(x\leftarrow 1))$
\end{pseudo}

\begin{theorem}
	The runtime of DPLL is bounded by $O(2^n\cdot\abs{\Phi})$. Depending on the
	heuristic for chosing the next variable to split on, the practical running
	time is much better. Some heuristics are
	\begin{itemize}
		\item most-constraining variable
		\item almost pure literals
		\item shortest clause
		\item maximising $\sum_{k=2}^n 2^{-k} \abs{\{C_j : x\in C_j, \abs{C_j}=k\}}$
	\end{itemize}
\end{theorem}

\section{Computability}

\subsection{Register machines}

\begin{definition}
	We say a register machine $M$ \emph{computes} a partial function
	$f:\N\times\N\partialto\N$ if, for any $m,n\in\N$, the following
	holds after setting the registers $A=m$, $B=n$, $C=D=\cdots=0$:
	\begin{itemize}
		\item $M$ terminates if and only if $f(m,n)$ is defined.
		\item If $M$ terminates, the final value in $A$ is $f(m,n)$.
	\end{itemize}
	We say $f:\N\times\N\partialto\N$ is \emph{RM-computable} if and
	only if there is some register machine that computes $f$.
\end{definition}

\begin{theorem}
	The following classes of functions coincide:
	\begin{itemize}
		\item functions definable in $\lambda$-calculus,
		\item functions computable by Turing and register machines,
		\item functions computable in any Turing complete programming language.
	\end{itemize}
	We call this class \emph{Church-Turing computable}.
\end{theorem}

\subsection{Undecidable questions}

\begin{theorem}[Halting problem]
	An RM computation is said to \emph{halt} if we eventually emerge at
	an exit. Let $m,n\in\N$. Then the function $h$ given by
	\begin{align*}
		h(m,n)=\begin{cases}
			0 &\text{if machine $m$ halts on input $n$,}\\
			1 &\text{otherwise}
		\end{cases}	
	\end{align*}
	is not RM-computable.
\end{theorem}

\begin{theorem}
	A \emph{Diophantine equation} is a multi-variable polynomial
	equation with integer coefficients for which we require integer
	solutions. Whether or not any such equation has a solution is
	undecidable.
\end{theorem}

\begin{theorem}
	Let $S,T$ be finite sets of strings. Whether or not there exists
	a string that can be formed both as a concatenation of strings in
	$S$ as well as a concatenation of strings in $T$ is undecidable.
\end{theorem}

\end{document}
