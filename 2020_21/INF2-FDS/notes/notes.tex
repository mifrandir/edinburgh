\documentclass{article}
\usepackage{notes-preamble}
\mkthms
\begin{document}
\title{Foundations of Data Science (YEAR2)}
\author{Franz Miltz}
\maketitle
\tableofcontents
\pagebreak
\section{Statistical preliminaries}
\begin{definition}
	The \B{sample mean} $\bar{x}$ for sample $x_1, x_2, ..., x_n$
	is defined as
	\begin{align*}
		\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{sample median} $\tilde x$ for a sample $x_1, x_2, ..., x_n$ is defined as 
	\begin{align*}
		\tilde x = \begin{cases}
			x_{(n+1)/2} 				   &\text{if $n$ is odd},\\
			\frac{1}{2}(x_{n/2}+x_{n/2+1}) &\text{otherwise}.
		\end{cases}.
	\end{align*}
\end{definition}
\begin{theorem}
	The relation of the mean $\bar x$ and the median $\tilde x$ can be inferred as follows
	\begin{enumerate}
		\item If a distribution is \B{symmetric}, then $\bar x = \tilde x$
		\item If a distribution is \B{positively skewed}, then $\bar x > \tilde x$
		\item If a distribution is \B{negatively skewed}, then $\bar x < \tilde x$.
	\end{enumerate}
\end{theorem}
\begin{definition}
	The \B{sample variance} $s_x^2$ of a sample $x_1, x_2, ..., x_n$ is defined as
	\begin{align*}
		s_x^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar x)^2.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{sample standard deviation} $s_x$ of a sample $x_1, x_2, ..., x_n$ is defined as
	\begin{align*}
		s_x = \sqrt{s_x^2}.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{population variance} $\sigma_x^2$ of a population $x_1, x_2, ..., x_N$ is defined as
	\begin{align*}
		\sigma_x^2 =\frac{1}{N}\sum_{i=1}^N (x_i-\bar x)^2.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{population standard deviation} $\sigma_x$ of a population $x_1, x_2, ..., x_N$ is defined as
	\begin{align*}
		\sigma_x = \sqrt{\sigma_x^2}.
	\end{align*}
\end{definition}


\section{Linear Models}


\begin{definition}
	The \B{sample covariance} $s_{xy}$ of a sample $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$
	is defined as
	\begin{align*}
		s_{xy} = \frac{1}{n}\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y).
	\end{align*}
\end{definition}
Notes about the sample covariance:
\begin{enumerate}
	\item The units are $[x][y]$.
	\item Scaling the units linearly scales $s_{xy}$ linearly.
\end{enumerate}
\begin{definition}
	The \B{correlation coefficient} $r$ of a sample $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ is defined as
	\begin{align*}
		r = \frac{s_{xy}}{s_x s_y}.
	\end{align*}
\end{definition}
Properties about the correlation coefficient:
\begin{enumerate}
	\item $-1 \leq r \leq 1$
	\item $y=cx \Rightarrow |r|=1$
\end{enumerate}
\begin{definition}
	The standardised variable $z$ of the quantity $x$ is defined as
	\begin{align*}
		z_i = \frac{x_i-\bar x}{s_x}.
	\end{align*}
\end{definition}
Properties of the standardised variable:
\begin{enumerate}
	\item $\bar z = 0$,
	\item $s_z = 1$,
	\item dimensionless,
	\item covariance of two standardised variables is the same as correlation.
\end{enumerate}
\begin{definition}
	Let $y$ be a variable dependent on $x$ with $y=\beta_0 + \beta_1 x$. Then
	$\beta_0$ is the intercept and $\beta_1$ is the slope.
\end{definition}
\begin{definition}
	The distance of the line $(\beta_0,\beta_1)$ to a sample $(x,y)$ is definend as
	\begin{align*}
		f(\beta_0, \beta_1) = \sum_{i=1}^n (y_i -(\beta_0 + \beta_1 x_i))^2.
	\end{align*}
	The line $(\hat\beta_0, \hat\beta_1)$ is the one that minimises $f$.
\end{definition}
\begin{theorem}
	The closest line $(\hat\beta_0,\hat\beta_1)$ to a sample $(x,y)$ can be calculated with
	\begin{align*}
		\hat\beta_1&=\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x_i-\bar x)^2},\\
		\hat\beta_0&=\bar y - \hat\beta_1\bar x.
	\end{align*}
\end{theorem}
Properties of the linear regression line:
\begin{enumerate}
	\item $(\bar x, \bar y)\in(\hat\beta_0, \hat\beta_1)$,
	\item $\hat\beta_1 = \frac{s_y}{s_x}r$.
\end{enumerate}
Properties of the regression line of standardised variables:
\begin{enumerate}
	\item $(0,0)\in(\hat\beta_0,\hat\beta_1)$,
	\item $\hat\beta_0=0,\hat\beta_1=r$,
	\item the predicted standardised $y$ is always closer to the mean than the standardised $x$.
\end{enumerate}
\begin{definition}
	Let $(\hat\beta_0,\hat\beta_1)$ be a regression line of $(x,y)$. Then the
	\B{predicted value} $\hat y_i$ for each $x_i$ is defined as
	\begin{align*}
		\hat y_i = \hat\beta_0 + \hat\beta_1 x_i.
	\end{align*}
\end{definition}
\begin{definition}
	The differences $y_i - \hat y_i$ for each $i$ are called \B{residuals}.
\end{definition}
Note on residuals from linear regression:
\begin{enumerate}
	\item Zero mean
	\item Zero correlation with independent variable $x$
	\item Zero correlation with the predicted values $\hat y$
\end{enumerate}
\begin{definition}
	The \B{mean square error} is defined by
	\begin{align*}
		\mse = \frac{1}{n}\sum_{i=1}^n (y-\hat y)^2
	\end{align*}
	and the \B{root mean square error} is given by $\rmse=\sqrt{\mse}$.
\end{definition}
\begin{definition}
	A measure of the total variance in $y$ before we know anything
	about $x$ is the \B{total sum of squares (SST)}, which we define
	as the sum of squared deviations from the mean of $y$:
	\begin{align*}
		\sst = S_{yy} = \sum (y_i - \bar y)^2 = (n-1)s^2_y.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{sum of squared errors (SSE)} is defined
	\begin{align*}
		\sse = \sum (y_i -\hat y_i)^2.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{coeficient of determination} is defined
	\begin{align*}
		R^2 = 1-\frac{\sse}{\sst}.
	\end{align*}
\end{definition}
Notes on $R^2$:
\begin{itemize}
	\item $R^2$ is a measure of "goodness of fit"
	\item $R^2=1$ indicates that the model predicts the data perfectly
	\item $R^2=0$ indicates no predictive value
\end{itemize}


\section{Multiple regression}


\begin{definition}
	The process of predicting a dependent variable from multiple independent variables is called \B{multiple regression}.
\end{definition}
\begin{definition}
	Suppose we have two independent variables $x^{(1)}$ and $x^{(2)}$ and one dependent variable $y$.
	The multiple regression model is then expressed as
	\begin{align*}
		y = \beta_0 + \beta_1 x^{(1)} + \beta_2 x^{(2)}.
	\end{align*}
\end{definition}
\begin{definition}
	Let $x^*_{ik}=x^{(k)}_i - \bar x^{(k)}$. Then the \B{regressor matrix} $T$ is
	the $n\times K$ matrix defined as
	\begin{align*}
		T = \begin{pmatrix}
			x^*_{11} & x^*_{12} & \cdots & c^*_{1K}\\
			x^*_{12} & x^*_{22} & \cdots & c^*_{2K}\\
			\vdots & \vdots & \ddots & \vdots\\
			x^*_{1n} & x^*_{2n} &\cdots & c^*_{nK}
		\end{pmatrix}.
	\end{align*}
\end{definition}
\begin{definition}
	Let $y^*_i=y_i-\bar y$. Then the \B{moment matrix} is defined as
	\begin{align*}
		X^T y = \begin{pmatrix}
			\sum x^*_{i1} y^*_i\\
			\vdots\\
			\sum x^*_{iK} y^*_i
		\end{pmatrix}
	\end{align*}
	where $y$ is the column vector containing all $y^*_i$.	
\end{definition}
\begin{definition}
	The \B{covariance matrix} is defined as
	\begin{align*}
		S =\frac{1}{n-1}X^T X.
	\end{align*}
\end{definition}
\begin{theorem}
	To minimise
	\begin{align*}
		f(\beta_0, \beta_1, \beta_2) = \sum (y_i - (\beta_1 x_{i1}+\beta_2 x_{i2}))^2
	\end{align*}
	we require
	\begin{align*}
		\hat\beta_0 = \bar y - \beta_1\bar x^{(1)} - \beta_2\bar x^{(2)}
	\end{align*}
	and
	\begin{align*}
		\hat\beta = \begin{pmatrix}
			\hat\beta_1\\
			\hat\beta_2
		\end{pmatrix}
		= (X^T X)^{-1}X^T y.
	\end{align*}
\end{theorem}
\begin{definition}
	The \B{interaction term model} for two independent variables $x^{(1)}$ and $x^{(2)}$ and one dependent variable $y$
	is defined as
	\begin{align*}
		y = \beta_0 + \beta_1 x^{(1)} + \beta_2 x^{(2)} + \beta_3 x^{(1)}x^{(2)}.
	\end{align*}
\end{definition}
Notes:
\begin{itemize}
	\item for $x^{(2)}\in (0,1)$ this is equivalent to two separate linear regressions
	\item we avoid having to create separate datasets
	\item makes regression on two continuous variables more precise
\end{itemize}
\begin{definition}
	If the number of variables is $k$ and the number of instances is $n$, the
	\B{adjusted coefficient of determination} is given
	\begin{align*}
		R^2_a = 1 - \frac{n-1}{n-k-1}\frac{\sse}{\sst}.
	\end{align*}
\end{definition}
Note: Adjustment is useful because finding a good fit becomes easier with more variables. We don't want this to affect $R^2$.



\section{Principle Component Analysis}



\begin{definition}
	The \B{variance along the $i$th axis} is given by $S_{ii}$ where $S$ is the
	covariance matrix.
\end{definition}
\begin{definition}
	Given a unit vector $\vec p$ the \B{component score} $t_i$ of a data point
	$\vec x_i$ is defined as
	\begin{align*}
		t_i = {\vec p}^{T} \vec x_i.
	\end{align*}
\end{definition}
\begin{definition}
	The \B{variance of a component score} $t^{(i)}$ is
	\begin{align*}
		s_t^2 = \frac{1}{n-1}\sum_{i=1}^n t_i^2.
	\end{align*}
	This can be rewritten as
	\begin{align*}
		s^2_t = \vec p^T S\vec p.
	\end{align*}
\end{definition}
\begin{theorem}
	The $i$th principal component of a dataset with the $D\times D$ covariance
	matrix $S$ is the eigenvector corresponding to the $i$th largest eigenvalue 
	of $S$.
\end{theorem}
\begin{proposition}
	The fraction of the total variance explained by the $i$th principal component
	is 
	\begin{align*}
		\frac{\lambda_i}{\sum_{j=1}^D \lambda_j}
	\end{align*}
	where $\lambda_j$ is the eigenvalue corresponding to the $j$th prinicipal
	component.
\end{proposition}

\begin{theorem}
	Given component scores $\vec t^{(i)}$ and a set of components $\vec p_j$,
	the original data point $\vec x_i$ may be reconstructed as
	\begin{align*}
		\vec x_i = \sum_j t^{(i)}_j \vec p_j.
	\end{align*}
	Note: If only a subset of the principle components is chosen, this
	reconstruction is likely to be in accurate.
\end{theorem}


\section{Clustering, unsupervised and supervised learning}


\begin{definition}
	\B{Cluster analysis} aims to partition a data set into meaningful or useful
	groups, based on distances between data points.
\end{definition}
\begin{definition}
	Classification is a \B{supervised learining} process: there is a training
	set in which each dat a item has a label.
\end{definition}
\begin{definition}
	Clustering is an \B{unsupervised learning} process in which the training
	set does not contain any labels. The aim of a clustering algorithm is to group
	such a data set into clusters, based on the unlabelled data alone.
\end{definition}
\begin{definition}
	Clustering may be used to compress data by representing each data itme
	in a cluster by a single cluster \B{prototype}, typically at the centre
	of the cluster.
\end{definition}

\subsection{Types of clustering}

\begin{definition}
	\B{Hierarchical clustering} forms a tree of nested clusters in which, at
	each level in the tree, a cluster is the union of its children.
\end{definition}
\begin{definition}
	\B{Partitional clustering} divides the data into a fixed number of
	non-overlapping clusters, with each data point assigned to exactly one
	cluster.
\end{definition}

\subsection{K-means}

\subsubsection{Aim}

Dividing a set of $D$-dimensional data points into $K$ clusters.

\subsubsection{Algorithm}

\begin{enumerate}
	\item Initialise $K$ cluster centres, $\{\vec m_k\}^K_1$
	\item While not converged: \begin{enumerate}
		\item Assign each data vector $\vec x_i$ ($i\in[1,n]$) to the closest cluster centre
		\item Recompute each cluster mean as the mean of the vectors assigned to that cluster
	\end{enumerate}
\end{enumerate}
\begin{theorem}
	Let $C_k$ be the set of points in cluster $k$. Then the mean of cluster $k$
	is 
	\begin{align*}
		\vec m_k=\frac{1}{\abs{C_k}}\sum_{i\in C_k}\vec x_i.
	\end{align*}
\end{theorem}
\begin{definition}
	The distance between two points $\vec x$ and $\vec y$ in Euclidean space
	is given by
	\begin{align*}
		d(\vec x, \vec y) = \vabs{\vec x - \vec y}=\sqrt{\sum_{j=1}^D(x_j-y_j)^2}.
	\end{align*}
\end{definition}
There are several possible ways to \B{initialise} the cluster centres, including:
\begin{itemize}
	\item random data points
	\item means of random partitions of the input
	\item data points with extreme values
	\item perturb mean of data set 
\end{itemize}
\begin{definition}
	\B{Convergence} is reached when the assignment of points to clusters
	does not change after an iteration.
	\begin{itemize}
		\item convergence is guranteed
		\item number of iterations up to convergence is not guranteed
		\item there may be \B{multiple converging solutions}
	\end{itemize}
\end{definition}

\subsubsection{Evaluation}

To find best solution: Repeat with different initialisations and pick solution
with the lowest error.
\begin{definition}
	The \B{mean squared error function} may be used to compare two clusterings
	each with $K$ clusters. Let $C_k$ be the set of points in the $k$th cluster.
	Then the MSE is
	\begin{align*}
		E = \frac{1}{n}\sum_{k=1}^K \sum_{i\in C_k} \vabs{\vec x_i - \vec m_k}^2
	\end{align*}
	where $\vec m_k$ is the centre of cluster $k$ and $n$ is the number of data
	points in total.
\end{definition}

\subsubsection{Shortcomings}

\begin{itemize}
	\item dependent on the scale of data
	\item large clouds can pull small clusters off-centre
\end{itemize}

\subsubsection{Variants}

\begin{itemize}
	\item Batch: update centres after assigning all the values
	\item Online: update centres after assigning individual values 
\end{itemize}



\section{Introduction to supervised learning}



\begin{definition}
	A \B{classifier} is a function that takes a feature vector $\vec x$
	and returns a class $c$ where $c$ is a member of a set $C$.
\end{definition}
\begin{definition}
	A \B{decision boundary} partitions the input space between two
	classifications.
\end{definition}


\subsection{Construction classifiers using unsupervised learning}


To construct the classifier automatically we need
\begin{enumerate}
	\item a set of training data containing feature vectors and their labels
	\item an algorithm that we train using the training data
	\item \B{hyperparameters}, number that control how the algorithm learns and predicts
\end{enumerate}
This is referred to as \B{supervised learning}, since the label for a training
vector acts as supervision for the classifier when it is learning from
training data.


\subsection{Regression as supervised learning}


\begin{itemize}
	\item labels are not categories but independent variable
	\item tries to predict label for unknown input
\end{itemize}


\subsection{Nearest neighbour classification}


\subsubsection{Procedure}

\begin{itemize}
	\item requires a distance measure
	\item for each new input we return the label of the nearest point in the training data
\end{itemize}

\subsubsection{Decision boundaries}
\begin{itemize}
	\item \B{Voronoi diagram}
	\item piecewise linear
\end{itemize}

\subsubsection{Evaluation}

\begin{definition}
	A suitable error function for classification is the number of items
	that are misclassified. The classification error is often expressed 
	as the percentage of the total number of items that is misclassified,
	the \B{error rate}.
\end{definition}
\begin{theorem}
	For one-nearest neighbour classification the error rate when we consider
	members of the training sset is $0$.
\end{theorem}
\begin{definition}
	To evaluate our algorithm, we split the available data into the
	\B{training set} (70\%) and the \B{testing set} (30\%). No data points
	from the testing set are used in training.
\end{definition}
\begin{definition}
	The \B{training set error rate} is the percentage of misclassifications
	that the classifier makes on the training set after the learning 
	algorithm has been applied.\\
	The \B{testing set error rate} refers to errors made by the classifier
	on the testing set.
\end{definition}


\subsection{$k$-Nearest neighbour classification}


\subsubsection{Principle}

\begin{itemize}
	\item We consider the $k$ closest points.
	\item If the classification is tied, we choose randomly.
\end{itemize}

\subsubsection{Decision boundaries}

\begin{itemize}
	\item susceptible to noise for small $k$
	\item smoother with large $k$
\end{itemize}

\subsubsection{Algorithm}

For an unseen example $\vec x$:
\begin{enumerate}
	\item Compute the $n$ distances $d_i=d(\vec x, \vec x_i)$ and the features
		  of each training example $x_i$, $i\in[0,n]$.
    \item Sort the distances from lowest to highest and find the indices $i_1,...,i_k$ of the $k$ lowest values of $d_i$.
    \item Find the classes that belong to the closest points.
    \item Count the votes for each class and return the one with the largest number.
    \item If there is a tie, choose randomly, or look at the $k+1$th neighbour to resolve it.
\end{enumerate}

\subsubsection{Choosing $k$}

To find the hyperparameter $k$, we need a validation set. So we split
the input data into three parts:
\begin{enumerate}
	\item training data (50\%)
	\item validation data (25\%)
	\item testing data (25\%)
\end{enumerate}

\subsubsection{Efficiency}
\begin{itemize}
	\item training is very efficient because no processing is done
	\item testing is very inefficient and impractical for large data sets
\end{itemize}

Optimisation: Managing data better, e.g. $k$-d trees.
\end{document}
