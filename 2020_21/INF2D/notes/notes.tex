\documentclass{article}
\usepackage{notes-preamble}
\mkfpmthms
\usepackage{pseudo}

\title{INF2D: Reasoning and Agents (SEM4)}
\author{Franz Miltz}
\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Intelligent agents and their environments}

\begin{definition}
    An \emph{agent} is an entity that perceives and
    acts based on a sequence of observations in a system.
\end{definition}

\begin{definition}
    A \emph{rational agent} is an agent that acts to achieve
    the best possible outcome or the best possible expected
    outcome, if uncertainty is involved.
\end{definition}

\begin{definition}
    The computing device an agent runs on combined with all sensors and actuators
    is called the agents \emph{architecture}.
\end{definition}

\subsection{Types of intelligent agents}

\begin{definition}
    \emph{Simple reflex agents} are agents that only react to the immediate percepts
    and ignore the entirety of their percept history.
\end{definition}

\begin{definition}
    \emph{Model-based reflex agents} are agents whose actions may depend on the
    percept history as well as unperceived aspects of the world. They need to
    maintain an \emph{internal world model}.
\end{definition}

\begin{definition}
    \emph{Goal-based agents} are agents that have variable goals depending on the
    state of their environment.
\end{definition}

\begin{definition}
    \emph{Utility-based agents} are agents that can prioritise between multiple, 
    potentially conflicting goals at the same time.
\end{definition}

\subsection{Types of environments}

\begin{definition}
    If the agents sensors descirbe an environment fully, it is called
    \emph{fully observable}. Otherwise, it is \emph{partially observable}.
\end{definition}

\begin{definition}
    If the next state of an environment is fully determined by the current state
    and the agents actions, it is called \emph{deterministic}. Otherwise, it is
    \emph{stochastic}.
\end{definition}

\begin{definition}
    If the next action within an environment does not depend on the previous actions
    it is called \emph{episodic}. Otherwise, it is \emph{sequential}.
\end{definition}

\begin{definition}
    If the an environment does not change while the agent determines his next action
    it is called \emph{static}. Otherwise, it is \emph{dynamic}.
\end{definition}

\begin{definition}
    If the percepts, actions and episodes within an environment are discrete, it is called
    \emph{discrete}. Otherwise, it is \emph{continuous}.
\end{definition}


\section{Problem solving and search}


\subsection{Types of problems}

\begin{definition}
    If the environment is deterministic and fully observable
    the problem is called a \emph{single-state problem}.
    The agent knows exactly which state it will be in and the
    solution is a \emph{sequence}.
\end{definition}

\begin{definition}
    If the environment is non-observable the problem is called
    \emph{sensorless}. The agent may have no information about
    the state of the environment and the solution is a
    \emph{sequence}.
\end{definition}

\begin{definition}
    If the environment is non-deterministic and/or partially 
    observable the problem is called a \emph{contingency problem}.
\end{definition}

\begin{definition}
    If the state space is unknown to the agent, the problem is
    called an \emph{exploration problem}.
\end{definition}

\subsection{Problem formulation}

\begin{definition}[Problem formulation]
    A single-state problem in state space $S$
    and action space $A$ is defined by
    \begin{enumerate}
        \item \emph{initial state}
        \item \emph{successor function}, $f:S\to A\times S$
        \item \emph{goal test}, explicit or implicit
        \item additive \emph{path cost}, $c:S\times A\times S\to\R_{\geq 0}$
    \end{enumerate}
\end{definition}

\subsection{Selecting a state space}

Complexity of real world needs to be abstracted away.
\begin{itemize}
    \item \emph{(abstract) state}: set of real states
    \item \emph{(abstract) action}: complex combination of real actions
    \item \emph{(abstract) solution}: set of real paths, solutions in the real world
\end{itemize}

\subsection{Tree search algorithms}

\emph{Idea}: offline, simulated exploration of state space by generating successors of already
explored states.

\begin{pseudo}
\textbf{function} TREE-SEARCH(problem) \textbf{returns} a solution or failure   \\+
    initialise the frontier using the initial state of the problem              \\
    \textbf{loop do}                                                            \\+
        \textbf{if} the frontier is empy \textbf{then}                          \\+
            \textbf{return} failure                                             \\-
        choose a leaf node and remove it from the frontier                      \\
        \textbf{if} the node contains a goal state \textbf{then}                \\+
            \textbf{return} the corresponding solution                          \\-
        expand the chosen node, adding the resulting nodes to the frontier
\end{pseudo}


\section{Uninformed search strategies}


\begin{definition}
    A \emph{search strategy} defines the order of expansion in a tree search algorithm.
\end{definition}

\begin{definition}
    Search strategies are \emph{evaluated} along the following
    dimensions
    \begin{itemize}
        \item \emph{completeness}
        \item \emph{optimality}
        \item \emph{time complexity}
        \item \emph{space complexity}
    \end{itemize}
\end{definition}

\begin{pseudo}
\textbf{function} GRAPH-SEARCH(problem) \textbf{returns} a solution or failure  \\+
    initialise the frontier using the initial state of the problem              \\
    \textbf{loop do}                                                            \\+
        \textbf{if} the frontier is empy \textbf{then}                          \\+
            \textbf{return} failure                                             \\-
        choose a leaf node and remove it from the frontier                      \\
        \textbf{if} the node contains a goal state \textbf{then}                \\+
            \textbf{return} the corresponding solution                          \\-
        \emph{add the node to the explored set}                                 \\
        expand the chosen node, adding the resulting nodes to the frontier      \\+
            \emph{\textbf{only if} not in the frontier or explored set}
\end{pseudo}

\subsection{Breadth-first search}

\begin{definition}
    \emph{Breadth-first search} is a search strategy where
    the frontier is a FIFO queue.
    \begin{pseudo}
    \textbf{function} BREADTH-FIRST-SEARCH(problem) \textbf{returns} a solution or failure  \\+
        \emph{node} $\leftarrow$ a node with STATE=\emph{problem}.INITIAL-STATE, PATH-COST=0\\
        \textbf{if} \emph{problem}.GOAL-TEST(\emph{node}.STATE) \textbf{then}               \\+
            \textbf{return} SOLUTION(\emph{node})                                           \\-
        \emph{frontier} $\leftarrow$ FIFO queue only containing \emph{node}                 \\
        \emph{explored} $\leftarrow \emptyset$                                              \\
        \textbf{loop do}                                                                    \\+
            \textbf{if} EMPTY?(\emph{frontier}) \textbf{then}                               \\+
                \textbf{return} failure                                                     \\-
            \emph{node} $\leftarrow$ POP(\emph{frontier})                                   \\
            add \emph{node}.STATE to \emph{explored}                                        \\
            \textbf{for each} \emph{action} $\in$ \emph{problem}.ACTIONS(\emph{node}.STATE) \textbf{do}             \\+
                \emph{child} $\leftarrow$ CHILD-NODE(\emph{problem}, \emph{node}, \emph{action})                    \\
                \textbf{if} \emph{child}.STATE $\not\in$ \emph{explored} $\cup$ \emph{frontier} \textbf{then}       \\+
                \textbf{if} \emph{problem}.GOAL-TEST(\emph{child}.STATE) \textbf{then return} SOLUTION(\emph{child})\\
                \emph{frontier} $\leftarrow$ INSERT(\emph{child}, \emph{frontier})
    \end{pseudo}
\end{definition}

\begin{theorem}
    The breadth-first search strategy on a problem with branching factor $b$,
    depth $d$ and maximum depth $m$ may be evaluated as follows:
    \begin{itemize}
        \item complete: yes, if $b$ is finite
        \item optimal: yes, if each step has equal cost
        \item time complexity: $O(b^d)$
        \item space complexity: $O(b^d)$
    \end{itemize}
\end{theorem}

\subsection{Depth-first search}

\begin{definition}
    \emph{Depth-first search} is a search strategy where the frontier is
    a LIFO queue.
\end{definition}

\begin{theorem}
    The depth-first search strategy on a problem with branching factor $b$,
    depth $d$ and maximum depth $m$ may be evaulated as follows:
    \begin{itemize}
        \item complete: no, fails for loops and infinite-depth spaces
        \item optimal: no
        \item time complexity: $O(b^m)$ 
        \item space complexity: $O(bm)$
    \end{itemize}
\end{theorem}

\subsection{Depth-limited search}

\begin{definition}
    \emph{Depth-limited search} is a search strategy where the frontier is
    a LIFO queue and nodes at a given depth $l$ have no successors.
\end{definition}

\begin{theorem}
    The depth-limited search strategy on a problem with branching factor $b$,
    depth $d$, maximum depth $m$ and depth limit $l$ may be evaluated as follows:
    \begin{itemize}
        \item complete: yes, if $l\geq d$
        \item optimal: no
        \item time complexity: $O(b^l)$
        \item space complexity: $O(bl)$
    \end{itemize}
\end{theorem}

\subsection{Iterative deepening search}

\begin{definition}
    \emph{Iterative deepening search} is a search strategy where
    depth-limited search is performed with increasing depth limit until
    a solution is found.
\end{definition}

\begin{theorem}
    The iterative deepening search strategy on a problem with branching factor
    $b$, depth $d$ and maximum depth $m$ may be evaulated as follows:
    \begin{itemize}
        \item complete: yes
        \item optimal: yes, if each step has equal cost
        \item time complexity: $O(b^d)$
        \item space complexity: $O(bd)$
    \end{itemize}
\end{theorem}


\section{Informed search strategies}

\subsection{Heuristics}

\begin{definition}
    A \emph{heuristic} $h:S\to \R_{\geq 0}$ is a problem-specific function
    that evaluates every element of a search space $S$ to a nonnegative value.
    For all goal states $s_0\in S$ we have $h(s_0)=0$.
\end{definition}

\begin{definition}
    A heuristic $h$ is \emph{admissible} if for every node $n$, $h(n)\leq h^*(n)$,
    where $h^*(n)$ is the true cost to reach the goal node from $n$.
\end{definition}

\begin{definition}
    A heuristic $h$ is \emph{consistent} if for every node $n$, every
    successor $n'$ of $n$ generated by any action $a$,
    \begin{align*}
        h(n) \leq c(n, a, n') + h(n').
    \end{align*}
\end{definition}

\begin{definition}
    Let $h_1,h_2:S\to \R_{\geq 0}$ be admissible heuristics for a state
    space $S$. If
    \begin{align*}
        \forall s\in S,\: h_1(s) \leq h_2(s) 
    \end{align*}
    \emph{$h_2$ dominates $h_1$}.
\end{definition}

\begin{theorem}
    Let $h_1,h_2$ be admissible heuristics. If $h_2$ dominates $h_1$ it is
    better for tree and graph search.
\end{theorem}

\subsection{Best-first search}

\begin{definition}
    \emph{Best-first search} is a search strategy where an evaluation function
    $f$ assigns a desirability to each node in the frontier and the next node
    to expand is chosen based on that value.
\end{definition}

\subsection{Greedy best-first search}

\begin{definition}
    \emph{Greedy (best-first) search} is a best-first search strategy where the
    evaluation function is chosen to be a heuristic $h$ and the next node to
    expand is the one in the frontier that minimises $h(n)$.
\end{definition}

\begin{theorem}
    The greedy search strategy on a problem with branching factor $b$, depth
    $d$ and maximum depth $m$ may be evaluated as follows:
    \begin{itemize}
        \item complete: no, can get stuck in loops
        \item optimal: no
        \item time complexity: $O(b^m)$
        \item space complexity: $O(b^m)$
    \end{itemize}
\end{theorem}

\subsection{A* search}

\begin{definition}
    \emph{A* search} is a best-first search strategy where the evaluation
    function $f=g+h$ is chosen to be the sum of a heuristic $h$, representing the
    path cost from a node $n$ to a goal node, and a cummulative 
    cost function $g$ that calculates the cost of reaching $n$
    in from the start node.
\end{definition}


\begin{theorem}
    If $h$ is admissible, A* using TREE-SEARCH is optimal.
\end{theorem}

\begin{theorem}
    If $h$ is consistent, A* using GRAPH-SEARCH is optimal.
\end{theorem}

\begin{theorem}
    The A* search strategy on a problem with branching factor $b$, depth $d$
    and maximum depth $m$ may be evaluated as follows:
    \begin{itemize}
        \item complete: yes, unless there are infinitely many nodes $n$ with $f(n) \leq f(G)$.
        \item optimal: yes
        \item time complexity: $O(b^d)$
        \item space complexity: $O(b^d)$
    \end{itemize}
\end{theorem}

\subsection{Relaxed problems}

\begin{definition}
    Given a problem $P$, a \emph{relaxed problem of $P$} is a problem with
    fewer restrictions.
\end{definition}

\begin{theorem}
    Let $P$ be a problem and let $P'$ be a relaxed problem of $P$. Then
    the cost of an optimal solution of $P'$ is a admissible heuristic
    for $P$.
\end{theorem}


\section{Adversarial search}


\begin{definition}
    \emph{Adversarial serach} is required when multiple agents work to achieve 
    opposing goals. Its purpose is to ensure the best possible outcome for the
    agent applying it, taking into account all possible actions by other agents.
\end{definition}

\subsection{Minimax}

\begin{definition}
    \emph{Minimax} is an adversarial search strategy in deterministic, perfect
    information games, where the agent chooses the action that guarantees the
    highest lower bound for the utility value of the final state.

    \begin{pseudo}
        \textbf{function} MINIMAX-DECISION(\emph{state}) \textbf{returns} \emph{an action}\\+
            \textbf{return} $\text{argmax}_{a\in\text{ACTIONS}(s)}$(MIN-VALUE(RESULT(\emph{state}, \emph{a})))\\-
        \\
        \textbf{function} MAX-VALUE(\emph{state}) \textbf{return} \emph{a utility value}\\+
            \textbf{if} TERMINAL-TEST(\emph{state}) \textbf{then}\\+
                \textbf{return} UTILITY(\emph{state})\\-
            $v\leftarrow-\infty$\\
            \textbf{for each} $a\in\text{ACTIONS(\emph{state})}$ \textbf{do}\\+
                $v\leftarrow \text{MAX}(v, \text{MIN-VALUE}(\text{RESULT}(s, a)))$\\-
            \textbf{return} $v$\\-
        \\
        \textbf{function} MIN-VALUE(\emph{state}) \textbf{return} \emph{a utility value}\\+
            \textbf{if} TERMINAL-TEST(\emph{state}) \textbf{then}\\+
                \textbf{return} UTILITY(\emph{state})\\-
            $v\leftarrow\infty$\\
            \textbf{for each} $a\in\text{ACTIONS(\emph{state})}$ \textbf{do}\\+
                $v\leftarrow \text{MIN}(v, \text{MAX-VALUE}(\text{RESULT}(s, a)))$\\-
            \textbf{return} $v$
    \end{pseudo}
\end{definition}

\begin{theorem}
    The minimax search strategy on a problem with branching
    factor $b$, depth $d$ and maximum depth $m$ may be evaluated
    as follows:
    \begin{itemize}
        \item complete: yes, if tree is finite
        \item optimal: yes, against optimal opponent
        \item time complexity: $O(b^m)$
        \item space complexity: $O(bm)$
    \end{itemize}
\end{theorem}

\subsection{Alpha-beta pruning}

\begin{definition}
    \emph{\alpha-\beta pruning} is a minimax-like adversarial search
    strategy, where upper and lower bounds are used to identify
    sub-trees that do not affect the solution.
    \begin{pseudo}
        \textbf{function} ALPHA-BETA-SEARCH(\emph{state}) \textbf{returns} \emph{an action}\\+
            $v\leftarrow$MAX-VALUE(\emph{state}, $-\infty$, $+\infty$)\\
            \textbf{return} the \emph{action} in ACTIONS(\emph{state}) with value $v$\\-
        \\
        \textbf{function} MAX-VALUE(\emph{state}, \alpha, \beta) \textbf{return} \emph{a utility value}\\+
            \textbf{if} TERMINAL-TEST(\emph{state}) \textbf{then}\\+
                \textbf{return} UTILITY(\emph{state})\\-
            $v\leftarrow-\infty$\\
            \textbf{for each} $a\in\text{ACTIONS(\emph{state})}$ \textbf{do}\\+
                $v\leftarrow \text{MAX}(v, \text{MIN-VALUE}(\text{RESULT}(\text{\emph{state}}, a), \alpha, \beta))$\\
                \textbf{if} $v\geq \beta$ \textbf{then}\\+
                    \textbf{return} $v$\\-
                $\alpha\leftarrow\text{MAX}(\alpha, v)$\\-
            \textbf{return} $v$\\-
        \\
        \textbf{function} MIN-VALUE(\emph{state}, \alpha, \beta) \textbf{return} \emph{a utility value}\\+
            \textbf{if} TERMINAL-TEST(\emph{state}) \textbf{then}\\+
                \textbf{return} UTILITY(\emph{state})\\-
            $v\leftarrow\infty$\\
            \textbf{for each} $a\in\text{ACTIONS(\emph{state})}$ \textbf{do}\\+
                $v\leftarrow \text{MIN}(v, \text{MAX-VALUE}(\text{RESULT}(\text{\emph{state}}, a), \alpha, \beta))$\\
                \textbf{if} $v\leq \alpha$ \textbf{then}\\+
                    \textbf{return} $v$\\-
                $\beta\leftarrow\text{MIN}(\beta, v)$\\-
            \textbf{return} $v$\\-
    \end{pseudo}
\end{definition}

\section{Logical Agents}

\subsection{Knowledge-based agents}

\begin{definition}
    An agents \emph{knowledge base} is a set of \emph{sentences} representing assertions
    about the world.
\end{definition}

\begin{definition}
    A sentence is called an \emph{axiom} if it is not derived from other sentences.
\end{definition}

\begin{pseudo}
    \textbf{function} KB-AGENT(\emph{percept}) \textbf{returns} an \emph{action} \\+
        \textbf{persistent:} $KB$, $t$\\
        TELL($KB$,MAKE-PERCEPT-SENTENCE(\emph{percept}, $t$))\\
        $\text{\emph{action}}\leftarrow\text{ASK}(KB,\text{MAKE-ACTION-QUERY}(t))$\\
        $\text{TELL}(KB, \text{MAKE-ACTION-SENTENCE}(\text{\emph{action}},t))$\\
        $t\leftarrow t+1$\\
        \textbf{return} \emph{action}
\end{pseudo}

\subsection{Logic}

\begin{definition}
    A \emph{representation language} must have a \emph{syntax} that defines which
    sentences are well formed and \emph{semantics}, i.e. the meaning of sentences.
\end{definition}

\begin{definition}
    Let $m$ be a model and $\alpha$ be a sentence. If $\alpha$ is true in $m$, then
    $m$ \emph{satisfies} $\alpha$. The set of all models that satisfy $\alpha$ is
    denoted $M(\alpha)$.
\end{definition}

\begin{definition}
    Let $\alpha,\beta$ be sentences. Then $a\vDash b$ if and only if $M(\alpha)\subseteq M(\beta)$.
\end{definition}

\begin{definition}
    Let $i$ be an inference algorithm, $\alpha$ be a sentence and $KB$ be a knowlege base.
    If $i$ can derive $\alpha$ from $KB$ then we write $KB\vdash_i \alpha$.
\end{definition}

\begin{definition}
    An inference alogrithm that only derives entailed sentences is called \emph{sound}.
\end{definition}

\begin{definition}
    An inference algorithm is complete if it can derive any sentence that is entailed.
\end{definition}

\begin{definition}
    Let $\alpha,\beta$ be sentences. Then $a\equiv b$ if and only if $M(\alpha)=M(\beta)$.
\end{definition}

\begin{definition}
    Let $\alpha$ be a sentence. If $M(\alpha)\not=\emptyset$ then $\alpha$ is \emph{satisfiable}.
    If $M(\neg\alpha)=\emptyset$ then $\alpha$ is \emph{valid} or a \emph{tautologie}.
\end{definition}

\begin{lemma}
    Let $\alpha,\beta$ be sentences. Then
    \begin{itemize}
        \item $\alpha\vDash\beta$ if and only if $\alpha\Rightarrow\beta$ is valid.
        \item $\alpha\vDash\beta$ if and only if $(\alpha\wedge\neg\beta)$ is satisfiable.
    \end{itemize}
\end{lemma}

\begin{definition}
    A knowledge base is called \emph{monotonous} if the set of entailed sentences can
    only increase as information is added, i.e.
    \begin{align*}
        \text{if}\hs KB \vDash \alpha \hs\text{then}\hs KB\wedge \beta \vDash \alpha
    \end{align*}
    for all sentences $\alpha,\beta$.
\end{definition}

\subsection{SAT}

\begin{theorem}[Resolution rule]
    Let $l_1,...,l_k,m_1,...,m_n$ be literals and let $l_i\Leftrightarrow\neg m_j$. Then
    \begin{align*}
        \infer{l_1\vee \cdots\vee l_{i-1}\vee l_{i+1}\vee\cdots\vee l_k\vee m_1\vee\cdots\vee m_{j-1}\vee m_{j+1}\vee \cdots\vee m_n}{
            l_1\vee\cdots \vee l_k\hs m_1\vee\cdots\vee m_n
        }.
    \end{align*}
\end{theorem}

\begin{pseudo}
    \textbf{function} PL-RESOLUTION$(KB,\alpha)$ \textbf{returns} $true$ or $false$\\+
        $\text{\emph{clauses}}\leftarrow \text{the set of clauses in the CNF representation of }KB\wedge\neg\alpha$\\
        $\text{\emph{new}}\leftarrow\emptyset$\\
        \textbf{loop do}\\+
            \textbf{for each} pair of clauses $(C_i,C_j)$ \textbf{in} \emph{clauses} \textbf{do}\\+
                $\text{\emph{resolvents}}\leftarrow\text{PL-RESOLVE}(C_i,C_j)$\\
                \textbf{if} \emph{resolvents} contains an empty clause \textbf{then return} $true$\\
                $\text{\emph{new}}\leftarrow \text{\emph{new}}\cup \text{\emph{resolvents}}$\\-
            \textbf{if} $\text{\emph{new}} \subseteq \text{\emph{clauses}}$ \textbf{then return} $false$\\
            $\text{\emph{clauses}}\leftarrow\text{\emph{clauses}}\cup\text{\emph{new}}$
\end{pseudo}

\begin{theorem}[Ground resolution theorem]
    If a set of clauses is unsatisfiable, then the resoltuion closure of
    those clauses contains the empty clause. 
\end{theorem}

\begin{definition}
    A \emph{definite clause} is a disjunction of literals of which
    exactly one is positive.
\end{definition}

\begin{definition}
    A \emph{Horn clause} is a disjunction of literals of which at most
    one is positive.
\end{definition}

\begin{pseudo}
    \textbf{function} DPPL-SATISFIABLE?$(s)$ \textbf{returns} $true$ or $false$\\+
        $\text{\emph{clauses}}\leftarrow\text{the set of clauses in the CNF representation of }s$\\
        $\text{\emph{symbols}}\leftarrow\text{a list of the proposition symbols in }s$\\
        \textbf{return} DPLL$(\text{\emph{clauses}}, \text{\emph{symbols}}, \emptyset)$
    \\\\-
    \textbf{function} DPLL$(\text{\emph{clauses}}, \text{\emph{symbols}}, \text{\emph{model}})$ \textbf{returns} $true$ or $false$\\+
        \textbf{if} every clause in \emph{clauses} is true in \emph{model} \textbf{then return} $true$\\
        \textbf{if} some clause in \emph{clauses} is false in \emph{model} \textbf{then return} $false$\\
        $P,\text{\emph{value}}\leftarrow\text{FIND-PURE-SYMBOL}(\text{\emph{symbols}},\text{\emph{clauses}},\text{\emph{model}})$\\
        \textbf{if} $P$ is non-null \textbf{then return} DPLL$(\text{\emph{clauses}}, \text{\emph{symbols}}-P,\text{\emph{model}}\cup\{P=\text{\emph{value}}\})$\\
        $P,\text{\emph{value}}\leftarrow\text{FIND-UNIT-CLAUSE}(\text{\emph{clauses}}, \text{\emph{model}})$\\
        \textbf{if} $P$ is non-null \textbf{then return} DPLL$(\text{\emph{clauses}}, \text{\emph{symbols}}-P,\text{\emph{model}}\cup\{P=\text{\emph{value}}\})$\\
        $P\leftarrow\text{FIRST}(\text{\emph{symbols}})$; $\text{\emph{rest}}\leftarrow\text{REST}(\text{\emph{symbols}})$\\
        \textbf{return} $\text{DPLL}(\text{\emph{clauses}}, \text{\emph{rest}}, \text{\emph{model}}\cup\{P=true\})$ \textbf{or}
                        $\text{DPLL}(\text{\emph{clauses}}, \text{\emph{rest}}, \text{\emph{model}}\cup\{P=false\})$
\end{pseudo}

\paragraph{Optimisations in SAT solvers}
\begin{enumerate}
    \item \emph{component analysis}: Find disjoint components and solve separately
    \item \emph{variable and value ordering}: Find most occuring literal and set to true
    \item \emph{intelligent backtracking}: record conflicts to not repeat them later
    \item \emph{random restarts}: restarting with learned information may improve progress
    \item \emph{clever indexing}: optimise queries like "the set of clauses in which variable $X_i$ appears as a positive literal"
\end{enumerate}

\subsection{SAT-based planning}

\begin{theorem}
    To create a plan using a SAT solver, we need to provide the following
    \begin{itemize}
        \item \emph{initial state}
        \item \emph{goal}
        \item \emph{successor-state axioms}
        \item \emph{precondition axioms}: avoid illegal actions by imposing constraints
        \item \emph{action-exclusion axioms}: avoid illegal combinations of actions
    \end{itemize}
\end{theorem}

\end{document}