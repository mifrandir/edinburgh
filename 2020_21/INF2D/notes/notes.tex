\documentclass{article}
\usepackage{notes-preamble}
\mkthms
\usepackage{pseudo}

\title{INF2D: Reasoning and Agents (SEM4)}
\author{Franz Miltz}
\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Intelligent agents and their environments}

\begin{definition}
    An \emph{agent} is an entity that perceives and
    acts based on a sequence of observations in a system.
\end{definition}

\begin{definition}
    A \emph{rational agent} is an agent that acts to achieve
    the best possible outcome or the best possible expected
    outcome, if uncertainty is involved.
\end{definition}

\begin{definition}
    The computing device an agent runs on combined with all sensors and actuators
    is called the agents \emph{architecture}.
\end{definition}

\subsection{Types of intelligent agents}

\begin{definition}
    \emph{Simple reflex agents} are agents that only react to the immediate percepts
    and ignore the entirety of their percept history.
\end{definition}

\begin{definition}
    \emph{Model-based reflex agents} are agents whose actions may depend on the
    percept history as well as unperceived aspects of the world. They need to
    maintain an \emph{internal world model}.
\end{definition}

\begin{definition}
    \emph{Goal-based agents} are agents that have variable goals depending on the
    state of their environment.
\end{definition}

\begin{definition}
    \emph{Utitlity-based agents} are agents that can prioritise between multiple, 
    potentially conflicting goals at the same time.
\end{definition}

\subsection{Types of environments}

\begin{definition}
    If the agents sensors descirbe an environment fully, it is called
    \emph{fully observable}. Otherwise, it is \emph{partially observable}.
\end{definition}

\begin{definition}
    If the next state of an environment is fully determined by the current state
    and the agents actions, it is called \emph{deterministic}. Otherwise, it is
    \emph{stochastic}.
\end{definition}

\begin{definition}
    If the next action within an environment does not depend on the previous actions
    it is called \emph{episodic}. Otherwise, it is \emph{sequential}.
\end{definition}

\begin{definition}
    If the an environment does not change while the agent determines his next action
    it is called \emph{static}. Otherwise, it is \emph{dynamic}.
\end{definition}

\begin{definition}
    If the percepts, actions and episodes within an environment are discrete, it is called
    \emph{discrete}. Otherwise, it is \emph{continuous}.
\end{definition}


\section{Problem solving and search}


\subsection{Types of problems}

\begin{definition}
    If the environment is deterministic and fully observable
    the problem is called a \emph{single-state problem}.
    The agent knows exactly which state it will be in and the
    solution is a \emph{sequence}.
\end{definition}

\begin{definition}
    If the environment is non-observable the problem is called
    \emph{sensorless}. The agent may have no information about
    the state of the environment and the solution is a
    \emph{sequence}.
\end{definition}

\begin{definition}
    If the environment is non-deterministic and/or partially 
    observable the problem is called a \emph{contingency problem}.
\end{definition}

\begin{definition}
    If the state space is unknown to the agent, the problem is
    called an \emph{exploration problem}.
\end{definition}

\subsection{Problem formulation}

\begin{definition}[Problem formulation]
    A single-state problem in state space $S$
    and action space $A$ is defined by
    \begin{enumerate}
        \item \emph{initial state}
        \item \emph{successor function}, $f:S\to A\times S$
        \item \emph{goal test}, explicit or implicit
        \item additive \emph{path cost}, $c:S\times A\times S\to\R_{\geq 0}$
    \end{enumerate}
\end{definition}

\subsection{Selecting a state space}

Complexity of real world needs to be abstracted away.
\begin{itemize}
    \item \emph{(abstract) state}: set of real states
    \item \emph{(abstract) action}: complex combination of real actions
    \item \emph{(abstract) solution}: set of real paths, solutions in the real world
\end{itemize}

\subsection{Tree search algorithms}

\emph{Idea}: offline, simulated exploration of state space by generating successors of already
explored states.

\begin{pseudo}
\textbf{function} TREE-SEARCH(problem) \textbf{returns} a solution or failure   \\+
    initialise the frontier using the initial state of the problem              \\
    \textbf{loop do}                                                            \\+
        \textbf{if} the frontier is empy \textbf{then}                          \\+
            \textbf{return} failure                                             \\-
        choose a leaf node and remove it from the frontier                      \\
        \textbf{if} the node contains a goal state \textbf{then}                \\+
            \textbf{return} the corresponding solution                          \\-
        expand the chosen node, adding the resulting nodes to the frontier
\end{pseudo}
\end{document}