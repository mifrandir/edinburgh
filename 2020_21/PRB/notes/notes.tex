\documentclass{article}
\usepackage[a4paper]{geometry}
\geometry{tmargin=3cm, bmargin=3cm, lmargin=2cm, rmargin=2cm}
\usepackage[british]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{nicefrac}
\usepackage{siunitx}
\usepackage{mathtools}
\usepackage{fontspec}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\setmainfont{arial}
\newtheoremstyle{sltheorem} {}                % Space above
{}                % Space below
{\upshape}        % Theorem body font % (default is "\upshape")
{}                % Indent amount
{\bfseries}       % Theorem head font % (default is \mdseries)
{.}               % Punctuation after theorem head % default: no punctuation
{ }               % Space after theorem head
{}                % Theorem head spec
\theoremstyle{sltheorem}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\st}{\text{ s.t }}
\newcommand{\ih}{\widehat i}
\newcommand{\jh}{\widehat j}
\newcommand{\kh}{\widehat k}
\DeclareMathOperator{\lub}{LUB}
\DeclareMathOperator{\glb}{GLB}
\DeclareMathOperator{\hcf}{hcf}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Binom}{Binom}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\Exp}{Exponential}
\DeclareMathOperator{\GD}{Gamma}
\newcommand{\wbigcup}{\mathop{\bigcup}\displaylimits}
\newcommand{\wbigcap}{\mathop{\bigcap}\displaylimits}
%\DeclareMathOperator{\mod}{mod}
\newcommand*\lneg[1]{\overline{#1}}
\newcommand*\B[1]{\textbf{#1}}
\newcommand*\binco[2]{\begin{pmatrix}
    #1\\#2
\end{pmatrix}}
\begin{document}
\title{Probability (SEM3)}
\author{Franz Miltz}
\maketitle
\tableofcontents
\section{Basic theory}
\subsection{Assumptions}
\begin{definition}
    An \B{$n$-sided die}, denoted $Dn$, is an object with $n$ faces numbered $\{1,..,n\}$.
    When rolled, each side is equally likely do be shown.
\end{definition}
\begin{definition}
    A \B{coin} is a $D2$ with the faces labeled "heads" (H) and "tails" (T).
\end{definition}
\subsection{Experiments}
\begin{definition}
    Performing an \B{experiment} may produce one of a number of possible \B{outcomes}.
    Let $S$ be the set of possible outcomes of an experiment $E$. Then let $x\in S$ be one specific outcome and
    $p=\P(x)$ be its probability.\\
    If we perform $E$ a number of times $N\in\N$, we expect $x$ to occur approximately $pN$ times.
\end{definition}
\subsection{Sample spaces}
\begin{definition}
    The \textbf{sample space $S$} of an experiment $E$ is the set of all possible outcomes for $E$.
\end{definition}
\subsection{Events}
\begin{definition}
    An \B{event} is a subset $A\subseteq S$ of the sample space. If the experiment is performed and the
    outcome is $x\in S$, then we say the even $A$ has \B{occured} if $x\in A$ and that it \B{has not occurred}
    otherwise.
\end{definition}
\begin{theorem}
    Let $A,B\subseteq S$ be events.
    \begin{itemize}
        \item The union $A\cup B$ is the event that $A$ or $B$ occurs.
        \item The intersection $A\cap B$ is the event that both $A$ and $B$ occur.
        \item The theoretic difference $A\setminus B$ is the even that $A$ occurs but $B$ does not.
        \item The complement of $A$ is the event $A^c=S\setminus A$ that occurs when $A$ does not.
        \item $A\subseteq B$ mean that every outcome in $A$ is also in $B$.
    \end{itemize}
\end{theorem}
\begin{definition}
    Let $X=\{A_1, ..., A_n\}$ be a set of events with $|X|\leq |\N|$. Then 
    \begin{itemize}
        \item $\wbigcup_i A_i$ is the event that one $A\in X$ happens, and
        \item $\wbigcap_i A_i$ is the event that all $A\in X$ happen.
    \end{itemize}
\end{definition}
\begin{definition}
    Two events $A,B\subseteq S$ are \B{disjoint} if $A\cap B=\emptyset$. 
    A collection of $A_1, A_2,...$ of events is \B{pairwise disjoint} if every pair of
    events within it is disjoint.
\end{definition}
\subsection{Probability of outcomes and events}
\begin{definition}
    A \B{probability mass function} (pmf) on $S$ is a function $\P$ on $S$ such that writing $p_j=\P(x_j)$ we have
    \begin{align*}
        \forall j,\:0 \leq p_j \leq 1 \text{ and } \sum_j p_j = 1.
    \end{align*}
    We say that $p_j=\P(x_j)$ is the \B{probability} of the outcome $x_j$.
\end{definition}
\begin{definition}
    For an event $A\subseteq S$ we define the probability of $A$ to be the sum of the probabilities of the outcomes contained in 
    $A$: 
    \begin{align*}
        \P(A) = \sum_{\{j | x_j \in A\}} p_j.
    \end{align*}
\end{definition}
\begin{theorem}
    The following properties may be observed:
    \begin{enumerate}
        \item For all events $A$ we have $0\leq \P(A) \leq 1$.
        \item $\P(S)=1$.
        \item If the finite or countably infinite collection of events $A_1, A_2, ...$ is pairwise disjoint then
        \begin{align*}
            \P(A_1 \cup A_2 \cup \cdots) = \P\left(\wbigcup_i A_i\right) = \sum_i \P(A_i).
        \end{align*}
    \end{enumerate}
\end{theorem}
\begin{theorem}
    Let $A,B\subseteq S$ be events. Then 
    \begin{itemize}
        \item $A \subseteq B \Rightarrow \P(A) \leq \P(B)$, and
        \item $\P(A^c) = 1 - \P(A)$.
    \end{itemize}
\end{theorem}
\subsection{Inclusion-Exclusion}
\begin{theorem}[Inclusion-Exclusion]
    Let $A_1, A_2, ...$ be events. Then
    \begin{align*}
        \P(A\cup B)=\P(A) + \P(B) - \P(A\cap B).
    \end{align*}
\end{theorem}
\section{Trees, Binomial random variables, independence}
\subsection{The binomial distribution}
\begin{definition}
    Let $E$ be an experiment with a sample space $S$.
    Let $P\subseteq S$ be the set of successful outcomes.
    Then the event
    \begin{align*}
        A:\:\text{$E$ is performed $n$ times with exactly $k$ successes.}
    \end{align*}
    has the probability
    \begin{align*}
        \P(A) = \binom{n}{k}p^k (1-p)^{n-k}
    \end{align*}
    where $p$ is the probability of a successful outcome.
\end{definition}
\subsection{Independence}
\begin{definition}
    Two events $A,B\subseteq S$ are \B{independent} if
    \begin{align*}
        \P(A\cap B) = \P(A)\P(B).
    \end{align*}
    We say two events are \B{dependent} if they are not independent.
\end{definition}
\subsection{Random variables}
\begin{definition}
    A \B{random variable} is a function on the sample space.
\end{definition}
\begin{definition}
    The \B{standard uniform random variable} on $\{1,2,...,n\}$ is a random variable $X$ such that
    \begin{align*}
        \P(X=k)=\frac{1}{k}\text{ for }k\in\N, k\leq n.
    \end{align*}
\end{definition}
\begin{definition}
    A \B{Bernoulli random variable} with parameter $p$ (where $0\leq p\leq 1$) is a random variable
    taking just the two values $0$ and $1$ such that $\P(X=1)=p$ and $\P(X=0)=q$ where $q=1-p$.
\end{definition}
\begin{definition}
    A \B{Binomial random variable} with parameters $n\in\N$ and $p$ (where $0\leq p\leq 1$) is a
    random variable taking values $\{0,1,...,n\}$ with
    \begin{align*}
        \P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}.
    \end{align*}
\end{definition}
\begin{definition}
    Let $X,Y$ be random variables defined on the same discrete sample space. 
    Then $X$ and $Y$ are \B{independent} if for all values of $x,y$ we have
    \begin{align*}
        \P(X=x \text{ and } Y=y) = \P(X=x)\P(Y = y).
    \end{align*}
\end{definition}
\subsection{Expected value}
\begin{definition}
    Let $X$ be a random variable that takes a countable number of different
    numerical values $x_i$. Let the probability that $X$ takes the value $x_i$
    be denoted $p_i$.\\
    The \B{expected value $\E(X)$} of $X$ is defined to be
    \begin{align*}
        \E(X) = \sum_i p_i x_i.
    \end{align*}
\end{definition}
\begin{definition}
    Let $X,Y,Z$ be random variables on the sample space $S$.
    Then $Z$ is a linear combination of $X$ and $Y$ if
    \begin{align*}
        Z = sX + tY
    \end{align*}
    for some $s,t\in\R$.
\end{definition}
\begin{theorem}
    Let $X,Y$ be discrete random variables and $r\in\R$. Then
    \begin{itemize}
        \item $\E(rX)=r\E(X)$.
        \item $\E(X + Y) = \E(X) + \E(Y)$.
    \end{itemize}
\end{theorem}
\begin{proposition}
    Let $X\sim\text{Binom}(n,p)$. Then $\E(X)=np$.
\end{proposition}
\section{Conditional probabilities and results about natural numbers}
\subsection{Conditional probability}
\begin{definition}
    Let $A,B\subseteq S$ be events with $\P(A)\not=0$. Then the 
    \B{conditional probability of $B$ given $A$} is defined by
    \begin{align*}
        \P(B|A) = \frac{\P(B\cap A)}{\P(A)}
    \end{align*}
\end{definition}
\begin{theorem}
    Suppose $\P(A)\not= 0$. Then $\P(B|A)=\P(B)$ if, and only if, $A$ and $B$ are independent.
\end{theorem}
\begin{theorem}[The law of total probability]
    Suppose we partition a sample space $S$ into $n$ pairwise disjoint events $A_1, ..., A_n$.
    Let $B\subseteq S$ be an event. Then
    \begin{align*}
        \P(B)=\P(B|A_1)\P(A_1)+ \cdots + \P(B|A_n)\P(A_n).
    \end{align*} 
    In the case of a partition of two sets $A$ and $A^c$, we get that for events $A,B$ we have
    \begin{align*}
        \P(B) = \P(B|A)\P(A)+\P(B|A^c)\P(A^c).
    \end{align*}
\end{theorem}
\subsection{Conditional expectation}
\begin{definition}
    If $X$ is a random variable taking values $x_1, x_2, ...$ and $A$ is an event, we can
    compute the \B{conditional expectation} $\E(X|A)$. The formula is
    \begin{align*}
        \E(X|A) = \sum_k \P(X=x_k|A)x_k.
    \end{align*}
\end{definition}
\begin{proposition}[The law of total probability for expectations]
    The law of total probability translates immediately also into expectations, for a random
    variable and a partition of the samle space into disjoint events.
    \begin{align*}
        \E(X) = \E(X|A_1)\P(A_1)+\cdots\E(X|A_n)\P(A_n).
    \end{align*} 
\end{proposition}
\section{Four new distributions}
\subsection{Geometric random variables}
\begin{definition}
    A random variable $X$ is \B{geometric} with parameter $p$ and we write $X\sim \text{Geom}(p)$ if
    \begin{align*}
        \P(X=k)=q^{k-1}p
    \end{align*}
    where $p\in[0,1]$ and $q=1-p$.
\end{definition}
\begin{proposition}
    The expected value of $X\sim\Geom(p)$ is $\E(X)=1/p$.
\end{proposition}
\subsection{Continuous random variables}
\begin{definition}
    A \B{probability density function} for a random variable $X$ on an
    interval $I\subseteq\R$ is a piecewise continuous function $f_X:I\to\R$ satisfying
    \begin{enumerate}
        \item $f_X(x) \geq 0$ for all $x\in I$;
        \item $\int_I f_X(x) dx =1$.
    \end{enumerate}
    The interpretation of $f_X$ is that for $a,b\in I$ with $a\leq b$ we have
    \begin{align*}
        \P(a\leq X\leq b)=\int_a^b f_X(x)dx.
    \end{align*}
\end{definition}
\begin{definition}
    A continuous random variable $X\sim\Unif(a,b)$ if $X$ has a constant pdf
    \begin{align*}
        f_X(x) = \begin{cases}
            \frac{1}{b-a}, &\text{for }a\leq x\leq b\\
            0, &\text{otherwise.}
        \end{cases}
    \end{align*}
\end{definition}
\begin{definition}
    A random variable $X$ on $[0,\infty]$ with pdf of the form
    \begin{align*}
        f_X(x) = \lambda e^{-\lambda x}
    \end{align*}
    is called an \B{exponential random variable}.
    We write $X\sim\Exp(\lambda)$.
\end{definition}
\subsection{Properties of continuous random variables}
\begin{definition}
    Let $f_X$ be a pdf for a random variable. The \B{cumulative distribution
    function} is defined by
    \begin{align*}
        F_X(x) = \P(X\leq x) = \int_{-\infty}^x f_X(u)du.
    \end{align*}
\end{definition}
\begin{proposition}
    Let $f_X$ be a pdf and $F_X$ the corresponding cdf. Then
    \begin{itemize}
        \item $F_X$ is differentiable and $F_X'(x) = f_X(x)$.
        \item The cdf $F_X(x)$ is non-decreasing.
        \item If $X$ takes values only in an interval $[a,b]$, so that
              $f_X(x)=0$ outside that range, we have
              $F_X(x)=0$ for $x\leq a$ and $F_X(x)=1$ for $x\geq b$.
    \end{itemize}
\end{proposition}
\begin{definition}
    The \B{expected value} of a random variable $X$ with pdf $f_X(x)$ 
    is given by
    \begin{align*}
        \E(X) = \int_{-\infty}^\infty xf_X(x) dx.
    \end{align*}
    More generally, we can take the expected value of a function 
    $g(X)$ of our random variable $X$:
    \begin{align*}
        \E(g(X)) = \int_{-\infty}^\infty g(x) f_X(x)dx.
    \end{align*}
    If the random variable $X$ takes values only in an interval
    $[a,b]$ then the limits of the integral can and should be taken
    to be $a$ and $b$.
\end{definition}
\begin{definition}
    The \B{Gamma function} is a way of defining factorials for all
    positive real numbers rather than just integers. We define
    \begin{align*}
        \Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx,
    \end{align*}
    where $z>0$.
\end{definition}
\begin{theorem}
    For $\alpha,\lambda>0$ we have
    \begin{align*}
        \int_0^\infty x^{\alpha -1}e^{-\lambda x} dx = \frac{1}{\lambda^a}\Gamma(z).
    \end{align*}
\end{theorem}
\begin{definition}
    Let $\alpha$ and $\lambda$ be positive constants. The random variable
    $X$ taking values in $[0,\infty)$ has a \B{gamma distribution} and we 
    write $X\sim\GD(\alpha, \lambda)$ if its pdf is
    \begin{align*}
        f_X(x) = \frac{\lambda^\alpha}{\Gamma(a)}x^{\alpha-1}e^{-\lambda x}.
    \end{align*}
\end{definition}
\end{document}
