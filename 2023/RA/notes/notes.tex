\documentclass{article}
\usepackage{notes-preamble}
\usepackage{enumitem}
\mkthmstwounified

\title{Randomised Algorithms (SEM7)}
\author{Franz Miltz}
\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Fundamentals}

\begin{lemma}
  For all $n\geq 1$,
  \begin{align*}
    \rr{1-1/n}^n \leq 1/e.
  \end{align*}
\end{lemma}

\subsection{Probabilities and events}

\begin{definition}
  A probability space is a measure space $(\Omega,\Sigma,\text{Pr})$ where
  \begin{enumerate}
    \item for $E\in\Sigma$, $0\leq\pr{E}\leq 1$; and
    \item $\pr{\Omega} = 1$.
  \end{enumerate}
\end{definition}

\begin{definition}
  Events $\cc{E_i}_{i\in I}$ are mutually independent iff, for any subset
  $J\subseteq I$,
  \begin{align*}
    \pr{\bigcap_{j\in J}E_j} = \prod_{j\in J} \pr{E_j}.
  \end{align*}
\end{definition}

\begin{definition}
  Let $E,F$ be events such that $\pr{F}>0$. The conditional probability of $E$ given $F$ is
  \begin{align*}
    \prc{E}{F} = \frac{\pr{E\cap F}}{\pr{F}}.
  \end{align*}
\end{definition}

\subsection{Random variables}

\begin{definition}
  A random variable is a real-valued measurable function $X:\Omega\to\R$.
\end{definition}

\begin{definition}
  Random variables $\cc{X_i}_{i\in I}$ are mutually independent iff, for any subset
  $J\subseteq I$,
  \begin{align*}
    \pr{\bigcap_{j\in J}(X_j = x_j)} = \prod_{j\in J} \pr{X_j = x_j}
  \end{align*}
  for all choices of the $x_j$.
\end{definition}

\begin{definition}
  \label{def:expectation}
  Let $X$ be a discrete random variable. Then $X$ has finite expectation iff
  \begin{align*}
    \sum_{x} \abs{x} \pr{X=x} < \infty
  \end{align*}
  and we write
  \begin{align*}
    \ex{X} = \sum_{x} x \pr{X=x}.
  \end{align*}
\end{definition}

\begin{theorem}
  For any finite collection of discrete random variables $X_1,...,X_k$
  with finite expectation
  \begin{align*}
    \ex{\sum_{i=1}^{n} X_i}=\sum_{i=1}^{n} \ex{X_i}.
  \end{align*}
  Further, for any discrete random variable $X$ and any constant $\lambda\in\R$,
  \begin{align*}
    \ex{\lambda X}=\lambda\ex{X}.
  \end{align*}
\end{theorem}

\begin{definition}
  \label{def:moment}
  The $k$-th moment of a random variable $X$ is $\ex{X^k}$.
\end{definition}

\begin{definition}
  \label{def:variance}
  Let $X$ be a random variable. Then the variance is given by
  \begin{align*}
    \var{X}=\ex{\rr{X-\ex{X}}^2} = \ex{X^2}-\rr{\ex{X}}^2
  \end{align*}
\end{definition}

\begin{definition}
  \label{def:covariance}
  The covariance of two random variables $X$ and $Y$ is
  \begin{align*}
    \cov{X,Y}=\ex{\rr{X-\ex{X}}\rr{Y-\ex{Y}}}.
  \end{align*}
\end{definition}

\begin{theorem}
  For any two random variables $X,Y$,
  \begin{align*}
    \var{X+Y}=\var{X}+\var{Y}+2\cov{X,Y}
  \end{align*}
\end{theorem}

\begin{theorem}
  If $X,Y$ are independent random variables, then
  \begin{align*}
    \ex{XY}=\ex{X}\ex{Y}.
  \end{align*}
\end{theorem}

\begin{corollary}
  If $X,Y$ are independent random variables, then
  \begin{align*}
    \cov{X,Y}=0
  \end{align*}
  and
  \begin{align*}
    \var{X+Y}=\var{X}+\var{Y}.
  \end{align*}
\end{corollary}

\begin{theorem}
  Let $X_1,...,X_n$ be mutually independent random variables. Then
  \begin{align*}
    \var{\sum_{i=1}^{n} X_i}=\sum_{i=1}^{n} \var{X_i}.
  \end{align*}
\end{theorem}

\begin{theorem}
  Let $X$ be a random variable with finite expectation and finite median $m$. Then
  \begin{enumerate}
    \item the expectation is the value of $c$ that minimises the expression $\ex{\rr{X-c}^2}$;
    \item the median is the value of $c$ that minimises the expression $\ex{\abs{X-c}}$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  If $X$ is a random variable with finite standard deviation $\sigma$, expectation $\mu$ and
  median $m$, then
  \begin{align*}
    \abs{\mu-m}\leq\sigma.
  \end{align*}
\end{theorem}

\subsection{Moment generating function}\label{sec:moment-generating-function}

\begin{definition}\label{def:moment-generating-function}
  The moment generating function of a random variable $X$ is
  \begin{align*}
    M_X\rr{t}=\ex{\exp\rr{tX}}.
  \end{align*}
\end{definition}

\begin{theorem}
  Let $X$ be a random variable and $n>1$. Then
  \begin{align*}
    \ex{X^n}=\frac{d^n}{dt^n}\rr{M_X\rr{t}}.
  \end{align*}
\end{theorem}

\begin{theorem}
  Let $X,Y$ be random variables. If there exists $\delta>0$ such that, for all $t\in\rr{-\delta,\delta}$,
  $M_X\rr{t}=M_Y\rr{t}$, then $X$ and $Y$ have the same distribution.
\end{theorem}

\begin{theorem}
  Let $X,Y$ be independent random variables. Then
  \begin{align*}
    M_{X+Y}\rr{t}=M_X\rr{t}M_Y\rr{t}.
  \end{align*}
\end{theorem}

\subsection{Special random variables}

\begin{definition}
  \label{def:binomial}
  A random variable $X$ is binomial with parameters $n$ and $p$ iff,
  for all $0\leq j\leq n$,
  \begin{align*}
    \pr{X=j}=\binom{n}{j}p^j \rr{1-p}^{n-j}.
  \end{align*}
\end{definition}

\begin{lemma}
  The variance of a geometric random variable with parameter $p$ is $\rr{1-p}/p^2$.
\end{lemma}

\begin{definition}
  \label{def:geometric}
  A random variable $X$ is geometric with parameter $P$ iff, for all $n\in\N$,
  \begin{align*}
    \pr{X=n}=\rr{1-p}^{n-1} p
  \end{align*}
\end{definition}

\begin{lemma}
  For a geometric random variable $X$ with parameter $p$ and for $n>0$,
  \begin{align*}
    \pr{X=n+k}{X>k} = \pr{X=n}.
  \end{align*}
\end{lemma}

\begin{lemma}
  Let $X$ be a discrete random variable that takes on only nonnegative integer values. Then
  \begin{align*}
    \ex{X} = \sum_{i=1}^{\infty} \pr{X\geq i}
  \end{align*}
\end{lemma}

\begin{lemma}
  The harmonic number $H\rr{n}=\sum_{i=1}^{n} 1/i$ staisfies $H\rr{n}=\ln n+\Theta\rr{1}$.
\end{lemma}

\section{Inequalities}

\subsection{Basics}

\begin{lemma}[Union bound]
  For any countable family of events $\cc{E_i}_{i\in I}$,
  \begin{align*}
    \pr{\bigcup_{i\in I} E_i} \leq \sum_{i\in I} \pr{E_i}.
  \end{align*}
\end{lemma}

\begin{definition}
  \label{def:convex}
  A function $f:\R\to\R$ is said to be convex if, for any $x_1,x_2\in\R$
  and $\lambda\in\bb{0,1}$,
  \begin{align*}
    f\rr{\lambda x_1 + \rr{1-\lambda}x_2} \leq \lambda f\rr{x_1} + \rr{1-\lambda}f \rr{x_2}.
  \end{align*}
\end{definition}

\begin{theorem}[Jensen's inequality]
  \label{thm:jensens}
  Let $X$ be a random variable and let $f:\R\to\R$ be a convex function. Then
  \begin{align*}
    \ex{f\rr{X}} \geq f\rr{\ex{X}}.
  \end{align*}
\end{theorem}

\begin{theorem}[Chebyshev's inequality]
  Let $X$ be a random variable and $a>0$. Then
  \begin{align*}
    \pr{\abs{X-\ex{X}}\geq a}\leq \frac{\var{X}}{a^2}.
  \end{align*}
\end{theorem}

\begin{corollary}
  For any $t>1$,
  \begin{align*}
    \pr{\abs{X-\ex{X}}\geq t\sqrt{\var{X}}}\leq \frac{1}{t^2}, \hs
    \pr{\abs{X-\ex{X}}\geq t\ex{X}}\leq \frac{\var{X}}{t^2\rr{\ex{X}}^2}.
  \end{align*}
\end{corollary}

\begin{lemma}[Stirling]
  For $m>0$,
  \begin{align*}
    \sqrt{2\pi m}\rr{\frac{m}{e}}^m \leq m! \leq 2\sqrt{2\pi m}\rr{\frac{m}{e}}^m.
  \end{align*}
\end{lemma}

\subsection{Chernoff and Hoeffding}

\begin{theorem}[Chernoff bounds]
  Let $X_1,...,X_n$ be independent Poisson trials such that $\pr{X_i=1}=p_i$.
  Let $X=\sum_{i=1}^{n} X_i$ and $\mu=\ex{X}$. Then the following hold:
  \begin{enumerate}
    \item For any $\delta>0$, \begin{align*}
        \pr{X\geq \rr{1+\delta}\mu} \leq \rr{\frac{\exp\rr{\delta}}{\rr{1+\delta}^{1+\delta}}}^\mu
      \end{align*}
    \item For any $\delta\in\rr{0,1}$, \begin{align*}
        \pr{X\leq \rr{1-\delta}\mu} \leq \rr{\frac{\exp\rr{-\delta}}{\rr{1-\delta}^{1-\delta}}}^\mu
      \end{align*}
    \item For any $\delta\in\rb{0,1}$, \begin{align*}
        \pr{X\geq \rr{1+\delta}\mu}\leq \exp\rr{-\mu\delta^2/3}.
      \end{align*}
    \item For any $\delta\in\rr{0,1}$, \begin{align*}
        \pr{X\leq \rr{1-\delta}\mu}\leq \exp\rr{-\mu\delta^2/2}.
      \end{align*}
    \item For any $\delta\in\rr{0,1}$, \begin{align*}
        \pr{\abs{X-\mu}\geq \delta\mu}\leq 2\exp\rr{-\mu\delta^2/3}.
      \end{align*}
    \item For any $R\geq 6\mu$, \begin{align*}
        \pr{X\geq R}\leq 2^{-R}.
      \end{align*}
  \end{enumerate}
\end{theorem}

\begin{theorem}[Chernoff bounds]
  Let $X_1,...,X_n$ be independent random variables with
  \begin{align*}
    \pr{X_i=1}=\pr{X_i=-1}=1/2.
  \end{align*}
  Let $X=\sum_{i=1}^{n} X_i$. For any $a>0$,
  \begin{align*}
    \pr{X\geq a}\leq \exp\rr{-a^2/2n},
  \end{align*}
  and
  \begin{align*}
    \pr{\abs X\geq a}\leq \exp\rr{-a^2/2n}
  \end{align*}
\end{theorem}

\begin{corollary}
  Let $Y_1,...,Y_n$ be independent random variables with
  \begin{align*}
    \pr{Y_i=1}=\pr{Y_i=0}=1/2.
  \end{align*}
  Let $Y=\sum_{i=1}^{n} Y_i$ and $\mu=\ex{Y}=n/2$.
  \begin{enumerate}
    \item For any $a>0$, \begin{align*}
        \pr{Y\geq \mu+a}\leq \exp\rr{-2a^2/n}.
      \end{align*}
    \item For any $a\in\rr{0,\mu}$, \begin{align*}
        \pr{Y\leq \mu-a}\leq \exp\rr{-2a^2/n}.
      \end{align*}
    \item For any $\delta>0$, \begin{align*}
        \pr{Y\geq \rr{1+\delta}\mu}\leq \exp\rr{-\delta^2\mu}.
      \end{align*}
    \item For any $\delta\in\rr{0,1}$, \begin{align*}
        \pr{Y\leq \rr{1-\delta}\mu}\leq \exp\rr{-\delta^2\mu}.
      \end{align*}
  \end{enumerate}
\end{corollary}

\begin{theorem}[Hoeffding bound]
  Let $X_1,...,X_n$ be independent random variables such that for all $1\leq i\leq n$,
  $\ex{X_i}=\mu$ and $\pr{a\leq X_i\leq b}=1$. Then
  \begin{align*}
    \pr{\abs{\frac{1}{n}\sum_{i=1}^{n} X_i-\mu}\geq \e}\leq 2\exp\rr{-\frac{2n\e^2}{\rr{b-a}^2}}
  \end{align*}
\end{theorem}

\begin{theorem}
  Let $X_1,...,X_n$ be independent random variables with $\ex{X_i}=\mu_i$ and $\pr{a_i\leq X_i\leq b_i}=1$ for
  constants $a_i,b_i$. Then
  \begin{align*}
    \pr{\abs{\sum_{i=1}^{n} X_i - \sum_{i=1}^{n} \mu_i}\geq\e} \leq 2\exp\rr{-\frac{2\e^2}{\sum_{i=1}^{n} \rr{b_i-a_i}^2}}.
  \end{align*}
\end{theorem}

\begin{definition}
  A random graph $G=\rr{V,E}$ sampled from $G_{n,p}$ is obtained as follows:
  \begin{itemize}
    \item $G$ has $n=\abs{V}$ nodes.
    \item For each of the $\binom{n}{2}$ possible pairs, $\cc{u,v}$ with $u,v\in V$ and
      $u\neq v$, assign $\cc{u,v}\in E$ with independent probability $p$.
  \end{itemize}
\end{definition}

\subsection{Lovasz local lemma}

\begin{definition}
  A dependency graph for a set of events $E_1,...,E_n$ is a directed graph $G=\rr{V,E}$
  such that $V=\cc{1,...,n}$ and for each $i=1,...,n$ the event $E_i$ is mutually
  independent of all events $E\setminus N\rr{i}$.
\end{definition}

\begin{theorem}[Lovasz Local Lemma]
  Let $E_1,..,E_n$ be a set of events. Suppose that for some $p\in \rr{0,1}$ and some
  $d\in\N$ the following hold:
  \begin{enumerate}
    \item For all $1\leq i\leq n$, $\pr{E_i}\leq p$.
    \item A dependency graph on $\cc{E_1,...,E_n}$ has degree $\leq d$.
    \item $4dp\leq 1$.
  \end{enumerate}
  Then
  \begin{align*}
    \pr{\bigcup_{i=1}^N \overline{E_i}} > 0.
  \end{align*}
\end{theorem}

\section{Markov chains}

\subsection{Chains and states}

\begin{definition}
  A discrete time stochastic process $\rr{X_t}_{t\geq 0}$ is a Markov chain when
  \begin{align*}
    \prc{X_t=a_t}{X_{t-1}=a_{t-1},X_{t-2}=a_{t-2},\ldots,X_0=a_0}.
  \end{align*}
\end{definition}

\begin{definition}
  Let $\rr{X_t}_{t\geq 0}$ be a Markov chain and let $i,j\in\im\rr{X_t}$ be states.
  Then $j$ is accessible from state $i$ if, for some $n\geq 0$,
  \begin{align*}
    \prc{X_{t+n}=j}{X_t=i} > 0.
  \end{align*}
  If two states are accessible from each other, we say that they communicate and write
  $i\leftrightarrow j$.
\end{definition}

\begin{lemma}
  The relation $\leftrightarrow$ on the set of states of a Markov chain is an equivalence
  relation.
\end{lemma}

\begin{definition}[Irreducible]
  Let $\rr{X_t}_{t\geq 0}$ be a Markov chain with states $S$. Then it is
  irreducible iff $\vv{S/\leftrightarrow}=1$.
\end{definition}

\begin{lemma}
  A finite Markov chain is irreducible iff its graph representation is a strongly
  connected graph.
\end{lemma}

\begin{definition}
  Let $\rr{X_t}_{t\geq 0}$ be a Markov chain with states $S$. For $i,j\in S$ and $t\geq 0$,
  denote the probability of reaching $j$ when starting in $i$ at time $t$ by
  \begin{align*}
    r_t\rr{i,j}=\prc{(X_t = j)\cap\rr{\bigcap_{0\leq s<t} X_s \neq j}}{X_0 = i}.
  \end{align*}
  A state $i\in S$ is called recurrent if
  \begin{align*}
    \sum_{t\geq 1} r^t\rr{i,i} = 1
  \end{align*}
  and it is transient if
  \begin{align*}
    \sum_{t\geq 1} r^t\rr{i,i} < 1.
  \end{align*}
  A chain is recurrent if every state is recurrent.
\end{definition}

\begin{definition}
  Let $\rr{X_t}_{t\geq 0}$ be a Markov chain with states $S$. For $i\in S$, denote
  the expected time to return to state $j$ when starting at $i$ by
  \begin{align*}
    h\rr{i,j}=\sum_{t\geq 1} tr^t\rr{i,j}.
  \end{align*}
  A recurrent state $i$ is positive recurrent if $h\rr{i}<\infty$. Otherwise, it is
  null recurrent.
\end{definition}

\begin{lemma}
  In a finite Markov chain
  \begin{enumerate}
    \item at least one state is recurrent; and
    \item all recurrent states are positive recurrent.
  \end{enumerate}
\end{lemma}

\begin{definition}
  A state $i$ in a discrete time Markov chain is periodic if there exists an integer
  $\Delta>1$ such that, for all $\Delta\nmid s$, $\prc{X{t+s}=i}{X_t = i}=0$ .

  A discrete time Markov chain is periodic if any state is periodic.
\end{definition}

\begin{definition}
  An aperiodic, positive recurrent state is an ergodic state. A Markov chain is ergodic
  if all its states are ergodic.
\end{definition}

\begin{lemma}
  Any finite, irreducible, and aperiodic Markov chain is an ergodic chain.
\end{lemma}

\subsection{Stationary distributions}

\begin{definition}
  A stationary distribution of a Markov chain with transition matrix $P$ is
  a probability distribution $\pi$ such that
  \begin{align*}
    \pi = \pi P.
  \end{align*}
\end{definition}

\begin{theorem}
  Any finite, irreducible, and ergodic Markov chain has the following properties:
  \begin{enumerate}
    \item the chain has a unique stationary distribution $\pi$;
    \item for all $i,j$, the limit $\lim_{t\to\infty} \prc{X_t=j}{X_0=i}$
      exists and is independent of $i$;
    \item $\pi\cc{i}=\lim_{t\to\infty} \prc{X_t=j}{X_0=i}=1/h\rr{i}$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  Let $S$ be a set of states of a finite, irreducible, aperiodic Markov chain.
  Then
  \begin{align*}
    \prc{X_{t+1}\in S}{X_t\not\in S} = \prc{X_{t+1}\not\in S}{X_t\in S}.
  \end{align*}
\end{theorem}

\begin{theorem}
  Consider a finite, irreducible, and ergodic Markov chain with transition matrix
  $P$ and states ${1,...,n}$. If there is a state distribution $\pi$ such that, for all
  $1\leq i,j\leq n$,
  \begin{align*}
    \pi\cc{i}P_{ij} = \pi\cc{j}P_{ji}.
  \end{align*}
\end{theorem}

\begin{theorem}
  Any irreducible aperiodic Markov chain belongs to one of the following two classes:
  \begin{enumerate}
    \item the chain is ergodic;
    \item no state is positive recurrent.
  \end{enumerate}
\end{theorem}

\subsection{Random walks}

Let $G=(V,E)$ be a finite, undirected, and connected graph.

\begin{definition}
  A random walk on $G$ is a Markov chain $\rr{X_t}_{t\geq 0}$ with states $V$
  such that, for all $u,v\in V$ and $t\geq 0$,
  \begin{align*}
    \prc{X_{t+1}=v}{X_t=u} =
    \begin{cases}
      1/N\rr{u} & \text{if } v\in N\rr{u}, \\
      0         & \text{if }v\not\in N\rr{u}.
    \end{cases}
  \end{align*}
\end{definition}

\begin{lemma}
  A random walk on an undirected graph $G$ is aperiodic if and only if $G$ is not bipartite.
\end{lemma}

\begin{theorem}A random walk on $G$ converges to a stationary distribution $\pi$ where
  \begin{align*}
    \pi\cc{v}=\frac{\vv{N\rr{v}}}{2\vv{E}}.
  \end{align*}
\end{theorem}

\begin{definition}
  The cover time $C\rr{G}$ of a graph $G=\rr{V,E}$ is defined to be the maximu over all
  vertices of $v\in V$ of the expected time to visit all of the nodes in the graph by a
  random walk starting from $v$.
\end{definition}

\begin{lemma}
  If $\rr{u,v}\in E$,
  \begin{align*}
    h\rr{u,v}+h\rr{v,u}\leq 2\vv{E}.
  \end{align*}
\end{lemma}

\begin{lemma}
  For all undirected graphs $G=(V,E)$,
  \begin{align*}
    C\rr{G}\leq 2\vv{E}\rr{\vv{V}-1}.
  \end{align*}
\end{lemma}

\subsection{Markov Chain Monte Carlo}

\begin{definition}
  A randomised algorithm gives an $(\e,\delta)$-approximation for the value $V$
  if the output $X$ of the algorithm satisfies
  \begin{align*}
    \pr{\vv{X-V}\leq\e V} \geq 1-\delta.
  \end{align*}
\end{definition}

\begin{definition}
  A fully polynomial randomised approximation scheme (FPRAS) for a problem
  is a randomised algorithm for which, given an input $x$, $\e>0$, and $\delta < 1$,
  the algorithm outputs an $(\e,\delta)$-approximation to $V\rr{x}$ in time
  that is polynomial in $1/\e$, $\ln\rr{1/\delta}$, and the size of the input $x$.
\end{definition}

\begin{definition}
  Let $W$ be the random variable representing the output of a smapling algorithm for a
  finite sample space $\Omega$. The sampling algorithm is said to generate an $\e$-uniform
  sample of $\Omega$ if, for any $S\subseteq\Omega$,
  \begin{align*}
    \vv{\pr{W\in S}-\frac{\vv{S}}{\vv{\Omega}}}\leq\e.
  \end{align*}
\end{definition}

\begin{definition}
  A sampling algorithm is a fully polynomial almost uniform sampler (FPAUS) for a
  problem if, given an input $x$ and a parameter $\e>0$, it generates an $\e$-uniform
  sample of $\Omega\rr{x}$ and runs in time that is polynomial in $\ln\rr{1/\e}$ and the
  size of the input $x$.
\end{definition}

\begin{lemma}
  For a finite state space $\Omega$ and neighbourhood structure $\cc{N\rr{x} : x\in\Omega}$,
  let $M\geq\max_{x\in\Omega}\vv{N\rr{x}}$. Consider a Markov chain $\rr{X_t}_{t\geq 0}$
  where, for all $x,y\in\Omega$,
  \begin{align*}
    \prc{X_{t+1}=y}{X_t=x} =
    \begin{cases}
      1/M &\text{if $x\neq y$ and $y\in N\rr{x}$}, \\
      0 &\text{if $x\neq y$ and $y\not\in N\rr{x}$}, \\
      1-N\rr{x}/M &\text{if $x=y$}.
    \end{cases}
  \end{align*}
  If this chain is irreducible and aperiodic, then the stationary distribution is
  the uniform distribution.
\end{lemma}

\begin{lemma}[Metropolis]
  For a finite state space $\Omega$ and neighbourhood structure $\cc{N\rr{x} : x\in\Omega}$,
  let $M\geq\max_{x\in\Omega}\vv{N\rr{x}}$, and let $\pi$ be a state distribution.
  Consider a Markov chain $\rr{X_t}_{t\geq 0}$ where, for all $x,y\in\Omega$,
  \begin{align*}
    \prc{X_{t+1}=y}{X_t=x} =
    \begin{cases}
      \rr{1/M}\min\rr{1,\pi\cc{y}/\pi\cc{x}} &\text{if $x\neq y$ and $y\in N\rr{x}$}, \\
      0                                      &\text{if $x\neq y$ and $y\not\in N\rr{x}$}, \\
      1-\sum_{x\neq y}\prc{X_{t+1}=y}{X_t=x} &\text{if $x=y$}.
    \end{cases}
  \end{align*}
  If this chain is irreducible and aperiodic, then the stationary distribution is $\pi$.
\end{lemma}

\begin{lemma}[Glauber dynamics]
  Let $S,V$ be finite sets, let $\Omega\subseteq S^V$ be a finite state space,
  let $\cc{N\rr{x} : x\in\Omega}$ be a neighbourhood structure, and let $\pi$ be a
  state distribution. Let $\rr{V_t}_{t\geq 0}$ be uniformly sampled from $V$.

  Consider a Markov chain $\rr{X_t}_{t\geq 0}$ where, for all $x,y\in\Omega$ and $v\in V$,
  \begin{align*}
    \prc{X_{t+1}=y}{X_t=x, V_t=v} =
    \begin{cases}
      \frac{\pi\cc{y}}{\pi\rr{\Omega\rr{x,v}}} &\text{if $y\in\Omega\rr{x,v}$}, \\
      0 &\text{otherwise}.
    \end{cases}
  \end{align*}
\end{lemma}

\subsection{Variation distance}

\begin{definition}
  The variation distance between two distributions $P$ and $Q$ on a countable
  on a state space $\Omega$ is given by
  \begin{align*}
    \vabs{D_1-D_2}=\sup_{A\subseteq\Omega} \abs{P\rr{A}-Q\rr{A}}.
  \end{align*}
\end{definition}

\begin{definition}
  Let $\pi$ be the stationary distribution of an ergodic Markov chain with state space $S$.
  Define the distribution $p_t^x$ such that, for all $y\in S$,
  \begin{align*}
    p^t_x\cc{y} = \prc{X_t=y}{X_0=x}.
  \end{align*}
  Further, for all $x\in S$ and $t\geq 0$, define
  \begin{align*}
    \Delta_x \rr{t} = \vabs{p_x^t - \pi};\hs
    \Delta\rr{t} = \max_{x\in S}\Delta_x\rr{t}.
  \end{align*}
  Finally, for all $x\in S$ and $\e>0$, define
  \begin{align*}
    \tau_x\rr{\e}=\min\cc{t : \Delta_x\rr{t}\leq\e}; \hs
    \tau\rr{\e} = \max_{x\in S}\tau_x\rr{\e}.
  \end{align*}
\end{definition}

\begin{lemma}
  For any ergodic Markov chain and for all $t\geq 0$, $\Delta\rr{t+1}\leq\Delta\rr{t}$.
\end{lemma}

\subsection{Coupling}

\begin{definition}
  A coupling of a Markov chain $\rr{M_t}_{t\geq 0}$ with state space $S$ is a Markov
  chain $\rr{Z_t}_{t\geq 0}=\rr{\rr{X_t,Y_t}}_{t\geq 0}$ on the state space $S\times S$
  such that
  \begin{align*}
    \prc{X_{t+1}-x'}{Z_t=\rr{x,y}} = \prc{M_{t+1}=x'}{M_t=x}; \\
    \prc{Y_{t+1}-y'}{Y_t=\rr{x,y}} = \prc{M_{t+1}=y'}{M_t=y}.
  \end{align*}
\end{definition}

\begin{lemma}[Coupling lemma]
  Let $Z_t=\rr{X_t,Y_t}$ be a coupling for a Markov chain $M$ on a state space $S$.
  Suppose that there exists a $T$ such that, for every $x,y\in S$,
  \begin{align*}
    \prc{X_T\neq Y_T}{X_0=x,Y_0=y}\leq \e.
  \end{align*}
  Then
  \begin{align*}
    \tau\rr{\e}\leq T.
  \end{align*}
\end{lemma}

\end{document}
